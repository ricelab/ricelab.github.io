<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-07-04T08:21:46+08:00</updated><id>/feed.xml</id><title type="html">RICELab</title><subtitle>RICELab Homepage</subtitle><entry><title type="html">Singapore CHI Club 2023</title><link href="/blog/2023/chiclub/" rel="alternate" type="text/html" title="Singapore CHI Club 2023" /><published>2023-07-04T00:00:00+08:00</published><updated>2023-07-04T00:00:00+08:00</updated><id>/blog/2023/chiclub</id><content type="html" xml:base="/blog/2023/chiclub/"><![CDATA[<blockquote>
  <p>CHI Club is designed as an opportunity for CHI authors to get feedback from other members of the community, and for reviewers to gain experience in reading and providing useful critique for others who are writing.</p>
</blockquote>

<p>Your writing can be better.</p>

<p>It can be better because you are not the person who is ultimately going to read your writing, and so you do not know what others’ perspectives on your work are – namely, what the blindspots in your writing are. The purpose of a review process is to help show you: (1) what those blindspots are in your own writing so you can improve the writing to cover them, and (2) how obviously glaring blindspots are to you as a reviewer when you are reading someone else’s work. The final “learning objective” of this process is to show you that your writing is never great on the first go, and really great writing happens after iterating on it several times.</p>

<h1 id="singapore-chi-club-2023">Singapore CHI Club 2023</h1>

<p>Singapore CHI Club 2023 is here to help improve your CHI submission. As an author, you will also write two reviews of others’ work, and then provide feedback to principal authors in a small meeting. Thus, as an author, you will get feedback from others, alongside of getting the chance to help others. If you are not an author, you are also more than welcome to join in the fun by reviewing others’ work and helping to improve the lab’s output.</p>

<p>We have an aggressive timeline to help you get on track. But, this is not intended to be a “make it or break it” for folks that cannot hit the deadlines – if you can’t make the deadline, you can still contribute reviews (to help in that way); however, you will need to solicit reviews outside of this little “club.”</p>

<h1 id="timeline">Timeline</h1>

<ul>
  <li>Jul 24 (week of): Tony gives a presentation on “Elements of a First Page”</li>
  <li>Jul 28 - Submit the <strong>first page (plus stubs)</strong> of your submission to the submission site by noon. You will receive your review assignments by 5pm.</li>
  <li>Aug 2 - Prepare your feedback on the first pages into the system</li>
  <li>Aug 3 - Meet at SMU to hang out with your reviewers, and to provide feedback to others.</li>
  <li>Aug 11 - Tony provides a presentation on “How to provide constructive feedback on a paper submission”</li>
  <li>Aug 21 - Submit a <strong>complete draft</strong> of your submission by noon. You will receive your review assignments by 5pm.</li>
  <li>Aug 24 (evening) - Submit both of <strong>your reviews</strong> before the morning of Aug 25. This way your authors can review the reviews.</li>
  <li>Aug 25 - Meet at SMU to hang out with your reviewers, and to provide feedback to others</li>
  <li>Sep 7 - Abstract Submission</li>
  <li>Sep 14 - Paper Submission</li>
  <li>Sep 15 - Dinner somewhere</li>
</ul>

<h1 id="submission-site">Submission Site</h1>

<p>This is a real submission site that is used for many conferences. You will need to create an account on the site if you do not have one, and register to author and review submissions.</p>

<ul>
  <li><a href="https://easychair.org/conferences/?conf=chiclubsg2023">Submission site</a></li>
</ul>

<h1 id="rules">Rules</h1>

<p>The first rule of CHI Club is: you do not talk about CHI Club. The second rule of CHI Club is: you do not talk about CHI Club. (<a href="http://www.diggingforfire.net/fightclub/">Kidding!</a>)</p>

<p>Okay, seriously, here are the rules:</p>

<ol>
  <li>You need to complete at least <strong>2</strong> reviews for every paper you submit as a first author. If your co-authors can contribute additional reviews, or you want to pass one of those reviews off to a co-author, excellent. But, you should do at least one personally.</li>
  <li>You adhere to the timeline – particularly for submitting your full draft, and your reviews.</li>
  <li>You commit as a reviewer to reading drafts of others’ submissions, and working with (and meeting with) the author to improve the work.</li>
  <li>As an author, your responsibility is to arrange the meeting with your reviewer(s) when we meet to go over the feedback to improve your work.</li>
</ol>

<h1 id="faq">FAQ</h1>

<ul>
  <li><strong>I am not a first author. Can I still review submissions?</strong></li>
</ul>

<p>Absolutely! Just register onto the submission site, and you will be provided with two reviews (or more, if you request) to complete.</p>

<ul>
  <li><strong>I do not know how to provide feedback. What do I do?</strong></li>
</ul>

<p>We will develop a presentation that helps talk you through this issue. Additionally, we will provide a rubric that you can work with you develop your reviewer. Remember that ultimately, you want to provide the author with feedback to help improve their work – the purpose here is not to write an excellent prose review.</p>

<ul>
  <li><strong>I am a post doc.</strong></li>
</ul>

<p>That’s not a question, but great. You’ll be assigned 4-6 reviews. Thanks for your service.</p>

<ul>
  <li><strong>Do I have to do this?</strong></li>
</ul>

<p>Absolutely not. It is voluntary. That is, unless you’re Tony’s student, in which case, it is mandatory.</p>]]></content><author><name></name></author><category term="internal" /><summary type="html"><![CDATA[CHI Club is designed as an opportunity for CHI authors to get feedback from other members of the community, and for reviewers to gain experience in reading and providing useful critique for others who are writing.]]></summary></entry><entry><title type="html">Human-AI Collaboration</title><link href="/projects/hai-collaboration/" rel="alternate" type="text/html" title="Human-AI Collaboration" /><published>2023-06-29T00:00:00+08:00</published><updated>2023-06-29T00:00:00+08:00</updated><id>/projects/hai-collaboration</id><content type="html" xml:base="/projects/hai-collaboration/"><![CDATA[<!-- 160x144 -->

<p>A major focus of our current work is to explore how to design effective interfaces for human-AI collaboration. This framing of interaction draws focus away from the idea of computation (as in human-<em>computer</em> interaction), and rather on the idea of computation that can assist and amplify human activity through interactive collaboration. Collaboration, as we understand it, involves multiple turns for interacting, as well as a commitment to help and assist one another.</p>

<h2 id="coordination-and-communication-based-on-human-human-interaction">Coordination and Communication based on Human-Human Interaction</h2>

<p>Recently, we have been exploring how our understanding of human-human communication and coordination can be used to smooth human-AI interaction. One design resource for this is in studying and understanding how humans work with other humans, and using ideas here to apply to human-AI communication and coordination.</p>

<p>For instance, we have been interested in studying how we can use proxemics to mediate how we interact with robots <a class="citation" href="#mahadevan2021hri">(Mahadevan, Sousa, Tang, &amp; Grossman, 2021)</a>. We have found that when robots can understand how people are using space to coordinate their interactions with one another, they can more straightforwardly anticipate how human collaborators will work. This smooths collaborative activity between humans and robots.</p>

<p>We applied similar ideas in Stargazer <a class="citation" href="#li2023stargazer">(Li, Sousa, Mahadevan, Wang, Aoyagui, Yu, Yang, Balakrishnan, Tang, &amp; Grossman, 2023)</a>, where we designed a robotic camera that could understand both gestures and speech from a howto video tutorial producer. The robotic camera could, based on what the producer was saying, make decisions about how to reposition itself, or to zoom in/out.</p>

<p>Broadly, we expect that if we can design AI that understands proxemic behaviours and language, this will make it easier for humans to interact with AI that understands and has embodiment in our physical world. This is not the <em>only</em> way to design for human-AI collaboration, but one tact that is likely to be successful.</p>

<h2 id="generative-tools-for-authors">Generative Tools for Authors</h2>

<p>We have recently begun exploring how generative tools can be designed to support authors. An important aspect of this is understand what kinds of challenges authors have. In one project, we are exploring the kinds of challenges that fiction authors have in crafting their works <a class="citation" href="#stark2023fictionworkshop">(Stark, Tang, Kim, Park, &amp; Wigdor, 2023)</a>. After having identified some of these challenges, we have begun the process of designing and studying the impacts of tools that support the creative process.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/stark2023fictionworkshop"><span id="stark2023fictionworkshop">Jessi Stark, Anthony Tang, Young-Ho Kim, Joonsuk Park, and Daniel Wigdor. (2023). <span style="text-decoration: underline"> Can AI Support Fiction Writers Without Writing For Them? </span>. In <i>CHI23 Workshop: In2Writing - The Second Workshop on Intelligent and Interactive Writing Assistants</i>.</span>

	(Gero, Katy Ilonka and Chang, Minsuk and Huang, Ting-Hao ’Kenneth’ and Chung, John Joon Young and Raheja, Vipul and Wambsganss, Thiemo and Kang, Dongyeop and Lee, Mina, Eds.)

(workshop).</a><br />




	<a href=" 2023-chi2023workshop-stark-fiction-writers.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://in2writing.glitch.me/"><i class="fa fa-sticky-note-o"></i></a>



</section><a class="details" href="/papers/stark2023fictionworkshop/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/li2023stargazer"><span id="li2023stargazer">Jiannan Li, Mauricio Sousa, Karthik Mahadevan, Bryan Wang, Paula Akemi Aoyagui, Nicole Yu, Angela Yang, Ravin Balakrishnan, Anthony Tang, and Tovi Grossman. (2023). <span style="text-decoration: underline">Stargazer: An Interactive Camera Robot for Capturing How-To Videos Based on Subtle Instructor Cues</span>. In <i>CHI 2023: Proceedings of the 2023 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2023-chi2023-li-stargazer.mp4"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2023-chi2023-li-stargazer.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3544548.3580896"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/li2023stargazer/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/mahadevan2021hri"><span id="mahadevan2021hri">Karthik Mahadevan, Mauricio Sousa, Anthony Tang, and Tovi Grossman. (2021). <span style="text-decoration: underline">"Grip-that-there": An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration</span>. In <i>CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 26.3% - 749/2844.</small>


	<small>Notes: Best Paper Nominee (top 5% of submissions).</small>


	<br />


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=Fvn_qjWJqwk"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3411764.3445355"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/mahadevan2021hri/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Proxemic Interaction Design</title><link href="/projects/proxemics/" rel="alternate" type="text/html" title="Proxemic Interaction Design" /><published>2023-06-28T00:00:00+08:00</published><updated>2023-06-28T00:00:00+08:00</updated><id>/projects/proxemic-interaction-design</id><content type="html" xml:base="/projects/proxemics/"><![CDATA[<!-- 160x144 -->

<p>Proxemic interaction design explores how systems can use an understanding of people’s relative position, orientation and identity with one another and objects can be used to support rich interaction techniques. For example, we can design rooms that understand to turn on the lights when someone enters, and to turn them off when people leave. Similarly, we can design interactive displays to respond appropriately when someone is looking at them from a distance or up close. This is useful since we use proxemics to mediate our interactions with other people in everyday life – for instance, by moving physically closer to people we want to interact with, and by turning away from people we don’t. Proxemic interaction design explores how these basic cues can be used to improve interaction with computing technology.</p>

<p>In <a class="citation" href="#kudo2021balancing">(Kudo, Tang, Fujita, Endo, Takashima, &amp; Kitamura, 2021)</a>, we explored how VR headsets and reveal bystanders who may be potential interaction partners based on their proximity and orientation to the VR headset user. This is important when we consider that people wearing headsets are completely cut off from the world.</p>

<p>We can also use proxemics to mediate how we interact with robots <a class="citation" href="#mahadevan2021hri">(Mahadevan, Sousa, Tang, &amp; Grossman, 2021)</a>. We have found that when robots can understand how people are using space, they can more straightforwardly anticipate how human collaborators will work. This smooths collaborative activity between humans and robots.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/kudo2021balancing"><span id="kudo2021balancing">Yoshiki Kudo, Anthony Tang, Kazuyuki Fujita, Isamu Endo, Kazuki Takashima, and Yoshifumi Kitamura. (2021). <span style="text-decoration: underline">Towards Balancing VR Immersion and Bystander Awareness</span>. <i>Proceedings of the ACM on Human-Computer Interaction (PACMHCI)</i> 5, ISS, Article 484.</span>

(journal).</a><br />


	<small>Notes: Best Paper award.</small>


	<br />


	<a href="http://hcitang.org/papers/2021-pacmiss-balance-immersion-awareness.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2021-pacmiss-balance-immersion-awareness.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3486950"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/kudo2021balancing/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/mahadevan2021hri"><span id="mahadevan2021hri">Karthik Mahadevan, Mauricio Sousa, Anthony Tang, and Tovi Grossman. (2021). <span style="text-decoration: underline">"Grip-that-there": An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration</span>. In <i>CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 26.3% - 749/2844.</small>


	<small>Notes: Best Paper Nominee (top 5% of submissions).</small>


	<br />


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=Fvn_qjWJqwk"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3411764.3445355"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/mahadevan2021hri/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Games and Leisure</title><link href="/projects/games/" rel="alternate" type="text/html" title="Games and Leisure" /><published>2023-06-27T00:00:00+08:00</published><updated>2023-06-27T00:00:00+08:00</updated><id>/projects/games</id><content type="html" xml:base="/projects/games/"><![CDATA[<p>Games are an important piece of the social fabric in our everyday lives. They help us build relationships with one another, and us with a fantasy space to practice working with and competing against others.</p>

<figure>

	<img src="/assets/images/project-images/sphero.jpg" alt="A physical Sphero can be controlled by tilting a corresponding &lt;em&gt;control sphero&lt;/em&gt; in the desired direction of movement." width="100%" />


<figcaption>
    A physical Sphero can be controlled by tilting a corresponding <em>control sphero</em> in the desired direction of movement.
</figcaption>

</figure>

<p>We have a continuing interest in games – and I don’t just mean in terms of playing them! – both in terms of studying how people interact with one another in games <a class="citation" href="#tang2012verbal">(Tang, Massey, Wong, Reilly, &amp; Edwards, 2012; Wong, Tang, Livingston, Gutwin, &amp; Mandryk, 2009; Neustaedter, Tang, &amp; Tejinder, 2010)</a>, and in terms of designing engaging experiences <a class="citation" href="#jones2014spherosumo">(Jones, Dillman, Manesh, Sharlin, &amp; Tang, 2014; Finke, Tang, Leung, &amp; Blackstock, 2008; Neustaedter, Tang, &amp; Judge, 2013)</a>. Our explorations have included physical games <a class="citation" href="#jones2014spherosumo">(Jones, Dillman, Manesh, Sharlin, &amp; Tang, 2014)</a>, MMORPGs <a class="citation" href="#wong2009character">(Wong, Tang, Livingston, Gutwin, &amp; Mandryk, 2009)</a>, first-person shooters <a class="citation" href="#tang2012verbal">(Tang, Massey, Wong, Reilly, &amp; Edwards, 2012)</a>, and pervasive games <a class="citation" href="#neustaedter2010role">(Neustaedter, Tang, &amp; Tejinder, 2010; Neustaedter, Tang, &amp; Judge, 2013; Jeffrey, Blackstock, Finke, Tang, Lea, Deutscher, &amp; Miyaoku, 2006)</a>.</p>

<p>Our most recent efforts have gone into exploring how to describe and articulate the kinds of design strategies game designers use in their games. By identifying these strategies, other designers can learn from this, and improve their own games – perhaps by replicating these approaches, or by improving them. For instance, we categorized how game designers promote learnability in their games <a class="citation" href="#poretski2022learnability">(Poretski &amp; Tang, 2022)</a>. We also explored how game designers visually cue players within the context of video games <a class="citation" href="#dillman2018visualinteractioncue">(Dillman, Mok, Tang, Oehlberg, &amp; Mitchell, 2018)</a>. Finally, we considered how game designers provide players with an awareness of other players within the game space <a class="citation" href="#wuertz2018awareness">(Wuertz, Alharthi, Hamilton, Bateman, Gutwin, Tang, Toups, &amp; Hammer, 2018)</a></p>

<h2 id="livestreaming">Livestreaming</h2>

<p>We have also taken a keen interest in livestreaming culture. We explored the nature of mukbang culture – that is, eating massive quantities of food on video – specifically to understand why people watch these videos <a class="citation" href="#anjani2020mukbang">(Anjani, Mok, Tang, Oehlberg, &amp; Boon, 2020)</a>. We learned that people watch these videos for a variety of reasons, including missing their home culture or having an interest in learning about new food cultures. We have also recently found that it is possible to realize new taste sensations through such videos <a class="citation" href="#james2022flavorvideos">(James, Ranasinghe, Tang, &amp; Oehlberg, 2022)</a>.</p>

<p>Yet, Livestreaming has created entirely new spaces for people to live, exist and be together with one another. Our recent work has explored how autistic livestreamers have taken over a sizable corner of the livestreaming market, creating a space for them to be who they are, to share their stories, and to be visible to one another and others <a class="citation" href="#mok2022livestreamers">(Mok, Tang, McCrimmon, &amp; Oehlberg, 2022)</a>.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/poretski2022learnability"><span id="poretski2022learnability">Lev Poretski and Anthony Tang. (2022). <span style="text-decoration: underline">Press A to Jump: Design Strategies for Video Game Learnability</span>. In <i>CHI 2022: Proceedings of the 2022 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 24.7% - 638/2579.</small>



	<br />


	<a href="http://hcitang.org/papers/2022-chi2022-poretski-video-game-learnability.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2022-chi2022-poretski-video-game-learnability-talk.mov"><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/poretski2022learnability/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/mok2022livestreamers"><span id="mok2022livestreamers">Terrance Mok, Anthony Tang, Adam McCrimmon, and Lora Oehlberg. (2022). <span style="text-decoration: underline"> Social Access and Representation for Autistic Adult Livestreamers </span>. In <i>ASSETS ’22: The 24th International ACM SIGACCESS Conference on Computers and Accessibility</i>.</span>

(poster).</a><br />




	<a href=" http://hcitang.org/papers/2022-assets2022poster-mok-livestreamers.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href=" http://hcitang.org/papers/2022-assets2022poster-mok-livestreamers.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3517428.3550400"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/mok2022livestreamers/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/james2022flavorvideos"><span id="james2022flavorvideos">Meetha Nesam James, Nimesha Ranasinghe, Anthony Tang, and Lora Oehlberg. (2022). <span style="text-decoration: underline"> Flavor-Videos: Enhancing the Flavor Perception of Food while Eating with Videos </span>. In <i>IMX 2022: ACM International Conference on Interactive Media Experiences</i>.</span>

(conference).</a><br />

	<small>Acceptance: 40% - 19/47.</small>



	<br />


	<a href="https://hcitang.org/papers/2022-imx2022-james-flavor-videos.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://imx.acm.org/2022/"><i class="fa fa-sticky-note-o"></i></a>



	<a href="http://doi.acm.org?doi=3505284.3529967"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/james2022flavorvideos/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/anjani2020mukbang"><span id="anjani2020mukbang">Laurensia Anjani, Terrance Mok, Anthony Tang, Lora Oehlberg, and Goh Wooi Boon. (2020). <span style="text-decoration: underline">Why do people watch others eat? An empirical study on the motivations and practices of mukbang viewers</span>. In <i>CHI 2020: Proceedings of the 2020 SIGCHI Conference on Human Factors in Computing Systems</i>, 1–12.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2020-chi2020-mukbang.pdf "><i class="fa fa-file-pdf-o"></i></a>




</section><a class="details" href="/papers/anjani2020mukbang/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/wuertz2018awareness"><span id="wuertz2018awareness">Jason Wuertz, Sultan A. Alharthi, William A. Hamilton, Scott Bateman, Carl Gutwin, Anthony Tang, Zachary O. Toups, and Jessica Hammer. (2018). <span style="text-decoration: underline">A Design Framework for Awareness Cues in Distributed Multiplayer Games</span>. In <i>CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems</i>, Paper 243.</span>

(conference).</a><br />

	<small>Acceptance: 25.7% - 667/2595.</small>


	<small>Notes: 10 pages.</small>


	<br />


	<a href="http://hcitang.org/papers/2018-chi2018-awarenses-cues.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://chi2018.acm.org"><i class="fa fa-sticky-note-o"></i></a>



	<a href="http://doi.acm.org/10.1145/3173574.3173817"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/wuertz2018awareness/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/dillman2018visualinteractioncue"><span id="dillman2018visualinteractioncue">Kody Dillman, Terrance Mok, Anthony Tang, Lora Oehlberg, and Alex Mitchell. (2018). <span style="text-decoration: underline">A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality</span>. In <i>CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems</i>, Paper 140.</span>

(conference).</a><br />

	<small>Acceptance: 25.7% - 667/2595.</small>


	<small>Notes: 10 pages; Includes raw supplemental material of the game examples described in the paper.</small>


	<br />


	<a href="http://hcitang.org/papers/2018-chi2018-visual-interaction-cues.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://hcitang.org/papers/2018-chi2018-visual-interaction-cues-game-examples-supplement.zip"><i class="fa fa-sticky-note-o"></i></a>


	<a href="https://www.youtube.com/watch?v=3FoZStToALQ"><i class="fa fa-film"></i></a>


	<a href="http://doi.acm.org/10.1145/3173574.3173714"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/dillman2018visualinteractioncue/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/jones2014spherosumo"><span id="jones2014spherosumo">Brennan Jones, Kody Dillman, Setareh Aghel Manesh, Ehud Sharlin, and Anthony Tang. (2014). <span style="text-decoration: underline">Designing an Immersive and Entertaining Pervasive Gameplay Experience with Spheros as Game and Interface Elements</span>. In <i>EA CHI PLAY ’14: ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play</i>.</span>

(poster).</a><br />


	<small>Notes: 2-page abstract + poster.</small>


	<br />


	<a href="http://hcitang.org/papers/2014-chiplay2014-spherosumo.pdf"><i class="fa fa-file-pdf-o"></i></a>




	<a href="http://dx.doi.org/10.1145/2658537.2661301"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/jones2014spherosumo/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/neustaedter2013creating"><span id="neustaedter2013creating">Carman Neustaedter, Anthony Tang, and Tejinder K. Judge. (2013). <span style="text-decoration: underline">Creating scalable location-based games: lessons from Geocaching</span>. <i>Personal Ubiquitous Comput.</i> 17, 2: <span style="text-decoration: none">335–349</span>.</span>

(journal).</a><br />




	<a href="http://hcitang.org/papers/2013-puc-scalable-location-based-games.pdf"><i class="fa fa-file-pdf-o"></i></a>




	<a href="http://dx.doi.org/10.1007/s00779-011-0497-7"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/neustaedter2013creating/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2012verbal"><span id="tang2012verbal">Anthony Tang, Jonathan Massey, Nelson Wong, Derek Reilly, and W. Keith Edwards. (2012). <span style="text-decoration: underline">Verbal coordination in first person shooter games</span>. In <i>CSCW ’12: Proceedings of the ACM 2012 conference on Computer Supported
    Cooperative Work</i>, ACM, 579–582.</span>

(conference).</a><br />

	<small>Acceptance: 29.6% - 37/125 for notes.</small>



	<br />


	<a href="http://hcitang.org/papers/2012-cscw2012-fps.pdf"><i class="fa fa-file-pdf-o"></i></a>




	<a href="http://doi.acm.org/10.1145/2145204.2145292"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2012verbal/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/neustaedter2010role"><span id="neustaedter2010role">Carman Neustaedter, Anthony Tang, and Judge K. Tejinder. (2010). <span style="text-decoration: underline">The role of community and groupware in geocache creation and maintenance</span>. In <i>CHI ’10: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems</i>, ACM, 1757–1766.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2010-chi2010-geocaching.pdf"><i class="fa fa-file-pdf-o"></i></a>




	<a href="http://doi.acm.org/10.1145/1753326.1753590"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/neustaedter2010role/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/wong2009character"><span id="wong2009character">Nelson Wong, Anthony Tang, Ian Livingston, Carl Gutwin, and Regan Mandryk. (2009). <span style="text-decoration: underline">Character sharing in World of Warcraft</span>. In <i>ECSCW 2009: Proceedings of the European Conference on Computer Supported
    Cooperative Work (ECSCW)</i>, Springer London, 343–362.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2009-ecscw2009-character-sharing.pdf"><i class="fa fa-file-pdf-o"></i></a>




</section><a class="details" href="/papers/wong2009character/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/finke2008lessons"><span id="finke2008lessons">Matthias Finke, Anthony Tang, Rock Leung, and Michael Blackstock. (2008). <span style="text-decoration: underline">Lessons learned: game design for large public displays</span>. In <i>DIMEA ’08: Proceedings of the 3rd international conference on Digital
    Interactive Media in Entertainment and Arts</i>, ACM, 26–33.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2008-dimea2008-lessons-learned.pdf"><i class="fa fa-file-pdf-o"></i></a>




	<a href="http://doi.acm.org/10.1145/1413634.1413644"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/finke2008lessons/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/jeffrey2006chasing"><span id="jeffrey2006chasing">Phillip Jeffrey, Mike Blackstock, Matthias Finke, Anthony Tang, Rodger Lea, Meghan Deutscher, and Kento Miyaoku. (2006). <span style="text-decoration: underline">Chasing the Fugitive on Campus: Designing a Location-based Game for Collaborative
    Play</span>. <i>Loading.. Journal</i> 1, 1.</span>

(journal).</a><br />




	<a href="http://hcitang.org/papers/2006-cgsa2006-chasing-the-fugitive-full.pdf"><i class="fa fa-file-pdf-o"></i></a>




</section><a class="details" href="/papers/jeffrey2006chasing/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[Games are an important piece of the social fabric in our everyday lives. They help us build relationships with one another, and us with a fantasy space to practice working with and competing against others.]]></summary></entry><entry><title type="html">Electronic Fashion</title><link href="/projects/efashion/" rel="alternate" type="text/html" title="Electronic Fashion" /><published>2023-06-27T00:00:00+08:00</published><updated>2023-06-27T00:00:00+08:00</updated><id>/projects/electronic-fashion</id><content type="html" xml:base="/projects/efashion/"><![CDATA[<figure>

	<img src="/assets/images/project-images/mannequette.png" alt="The mannequette is a prototyping platform for electronic fashion garments. It is used in different ways: (a) in-situ, iterating and deciding upon fabrics at a market; (b) for prototyping light (LED) patterns and sensors on a miniature dress form with fabrics by using the mixer; (c) for integrating a previously prototyped and now completed pattern and sensor interactions into an assembled garment; (d) in a completed garment created in a runway show." width="100%" />


<figcaption>
    The mannequette is a prototyping platform for electronic fashion garments. It is used in different ways: (a) in-situ, iterating and deciding upon fabrics at a market; (b) for prototyping light (LED) patterns and sensors on a miniature dress form with fabrics by using the mixer; (c) for integrating a previously prototyped and now completed pattern and sensor interactions into an assembled garment; (d) in a completed garment created in a runway show.
</figcaption>

</figure>

<p>Digital tools enable rich new forms of self-expression, and we explore this in the context of electronic fashion.</p>

<p>To enable our work, we design new toolkits for building electronic fashion. This gives designers the ability to access electronic sensors and actuatuators without needing to write computer code <a class="citation" href="#seyed2019mannequette">(Seyed &amp; Tang, 2019)</a>, all while enabling their creativity <a class="citation" href="#pratte2022nlitenworkshop">(Pratte, Hoover, Tang, &amp; Oehlberg, 2022)</a>.</p>

<p>We see electronic fashion as a way to engage people in storytelling, be it to promote new forms of empathy <a class="citation" href="#pratte2021empathy">(Pratte, Tang, &amp; Oehlberg, 2021)</a>, or to express a story one has to tell <a class="citation" href="#pratte2023hacklesworkshop">(Pratte, Tang, &amp; Oehlberg, 2023)</a>.</p>

<p>We help push the space of avante garde electronic fashion, and have explored how the runway space shapes and enables rich storytelling <a class="citation" href="#pratte2023designspace">(Pratte, Tang, Hoover, Hoover, Laprairie, Larose, &amp; Oehlberg, 2023)</a></p>

<p>We are also interested in how these electronic garments can be used for communicating challenging concepts and ideas.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/pratte2023designspace"><span id="pratte2023designspace">Sydney Pratte, Anthony Tang, Shannon Hoover, Maria Elana Hoover, Matt Laprairie, Catherine Larose, and Lora Oehlberg. (2023). <span style="text-decoration: underline"> Towards a Design Space for Storytelling on the Fashion Technology Runway </span>. In <i>TEI ’23: Seventeeth International Conference on Tangible, Embedded and Embodied Interaction</i>.</span>

(conference).</a><br />




	<a href=" http://hcitang.org/papers/2023-tei2023-pratte-fashiontech-design-space.pdf "><i class="fa fa-file-pdf-o"></i></a>


	<a href=" http://hcitang.org/papers/2023-tei2023-pratte-fashiontech-design-space-artifact-set.pdf"><i class="fa fa-sticky-note-o"></i></a>



	<a href="https://doi.org/10.1145/3569009.3573899"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/pratte2023designspace/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/pratte2023hacklesworkshop"><span id="pratte2023hacklesworkshop">Sydney Pratte, Anthony Tang, and Lora Oehlberg. (2023). <span style="text-decoration: underline"> HACKLES: A Case Study on Using Data to Create Experiences with Wearables </span>. In <i>CHI22 Workshop: Toolkits &amp; Wearables - Developing Toolkits for Exploring Wearable Designs</i>.</span>

	(Lee-Smith, Matthew and Benjamin, Jesse Josua and Desjardins, Audrey and Funk, Mathias and Odom, William and Oogjes, Doenja and Park, Young-Woo and Pierce, James and Sanches, Pedro and Tsaknaki, Vasiliki, Eds.)

(workshop).</a><br />




	<a href=" 2023-chi2023workshop-pratte-hackles.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://materialfordesign.net/chi2023_workshop/"><i class="fa fa-sticky-note-o"></i></a>



</section><a class="details" href="/papers/pratte2023hacklesworkshop/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/pratte2022nlitenworkshop"><span id="pratte2022nlitenworkshop">Sydney Pratte, Shannon Hoover, Anthony Tang, and Lora Oehlberg. (2022). <span style="text-decoration: underline"> nLITEn: A Wearables Toolkit for Enabling Creativity in Fashion Technology Design </span>. In <i>CHI22 Workshop: Toolkits &amp; Wearables - Developing Toolkits for Exploring Wearable Designs</i>.</span>

	(Genç, Çaglar and Buruk, Oguz and Jabari, Shiva and Jones, Lee and Ragozin, Kirill and Hartman, Kate and Virkki, Johanna and Juhlin, Oskar and Kunze, Kai and Häkkilä, Jonna, Eds.)

(workshop).</a><br />




	<a href=" http://hcitang.org/papers/2022-chi2022workshop-pratte-nliten.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://techfashion.design/wearable-toolkits/"><i class="fa fa-sticky-note-o"></i></a>


	<a href=" http://hcitang.org/papers/2022-chi2022workshop-pratte-nliten-poster.pdf"><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/pratte2022nlitenworkshop/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/pratte2021empathy"><span id="pratte2021empathy">Sydney Pratte, Anthony Tang, and Lora Oehlberg. (2021). <span style="text-decoration: underline">Evoking Empathy: A Framework for Describing Empathy Tools</span>. In <i>TEI 2021: International Conference on Tangible, Embedded and Embodied Interaction</i>.</span>

(conference).</a><br />

	<small>Acceptance: 29.9% - 30/134.</small>



	<br />


	<a href="http://hcitang.org/papers/2021-tei2021-pratte-empathy.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=JBCzPt5ILxo"><i class="fa fa-sticky-note-o"></i></a>



	<a href="https://doi.org/10.1145/3430524.3440644"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/pratte2021empathy/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/seyed2019mannequette"><span id="seyed2019mannequette">Teddy Seyed and Anthony Tang. (2019). <span style="text-decoration: underline">Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways</span>. In <i>DIS 2019: Conference on Designing Interactive Systems 2019</i>, 10 pages.</span>

(conference).</a><br />

	<small>Acceptance: 25% - 100/400.</small>


	<small>Notes: Honourable mention (top 2% of all submissions).</small>


	<br />


	<a href="http://hcitang.org/papers/2019-dis2019-mannequette.pdf "><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2019-dis2019-mannequette.mp4 "><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/seyed2019mannequette/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[The mannequette is a prototyping platform for electronic fashion garments. It is used in different ways: (a) in-situ, iterating and deciding upon fabrics at a market; (b) for prototyping light (LED) patterns and sensors on a miniature dress form with fabrics by using the mixer; (c) for integrating a previously prototyped and now completed pattern and sensor interactions into an assembled garment; (d) in a completed garment created in a runway show.]]></summary></entry><entry><title type="html">360 Video Interaction</title><link href="/projects/360video/" rel="alternate" type="text/html" title="360 Video Interaction" /><published>2023-06-27T00:00:00+08:00</published><updated>2023-06-27T00:00:00+08:00</updated><id>/projects/360-video</id><content type="html" xml:base="/projects/360video/"><![CDATA[<!-- 160x144 -->

<p>360 video cameras provide an omnidirectional view on the world around us, and new opportunities to explore those views. Videos from these devices, in particular, afford us new ways to see and understand the world, as well as certain kinds of freedoms to explore these recorded experiences. Our work in this space has explored new ways to take advantage of the media – both in “single user” kinds of scenarios, where we are exploring spaces on our own, as well as in “multi user” kinds of experiences, where we are exploring the spaces with others.</p>

<p>One of the neat challenges and opportunities with 360 video is that they allow us to decide what <em>we</em> want to look at as viewers. Most videos have an “intended viewing angle” determined by the producer/camera person. In this sense, they restrict your view, and your ability to explore the space. What makes 360 videos even more interesting is that we have an additional dimension of flexibilit (time), which we must also navigate in addition to viewing direction. Our work has explored ways to smoothing this interaction to make it more straightforward for viewers.</p>

<h2 id="exploring-with-others--navigating-time-and-space-together">Exploring with Others – Navigating Time and Space Together</h2>

<p>Most interfaces for exploring 360 video are made for single user exploration. This presents real challenges when we are trying to explore 360 video scenes with other people. Some of our earliest work in this space showed that viewing 360 videos with others is somewhat cumbersome – mainly because our goals as viewers may differ <a class="citation" href="#tang2017watching360together">(Tang &amp; Fakourfar, 2017)</a>. We observe similar kinds of challenges when people try to communicate over live mobile video chat <a class="citation" href="#jones2015mobilecamerawork">(Jones, Witcraft, Tang, Bateman, &amp; Neustaedter, 2015)</a>. We designed two prototypes to explore how live video interaction over distance could be made richer through 360 video cameras: in one where studied how a simple prototype might introduce new complexities to the interaction <a class="citation" href="#tang2017360videochat">(Tang, Fakourfar, Neustaedter, &amp; Bateman, 2017)</a>, and in another where we attached a 360 camera to a telepresence robot to support play <a class="citation" href="#heshmat2018beam">(Heshmat, Jones, Xiong, Neustaedter, Tang, Riecke, &amp; Yang, 2018)</a>. These experiences revealed the complexities of interacting with remote collaborators, and paved the way for new kinds of interaction techniques to smooth those interactions.</p>

<p>Our most recent explorations of this research space have involved the design and evaluation of a prototype called Tourgether360 <a class="citation" href="#kumar2022tourgether">(Kumar, Poretski, Li, &amp; Tang, 2022; Kumar, Poretski, Li, &amp; Tang, 2022)</a>. This prototype allows people to experience virtual 360 tours together, and focuses on the design of communication and awareness tools to smooth the interactive dialogue between partners. We learned valuable lessons in this work – especially in how much communication is implicit and subtle.</p>

<h2 id="single-user-exploration-of-360-videos">Single-User Exploration of 360 Videos</h2>

<p>One challenge of exploring 360 videos is that one can really only manipulate one dimension at a time—either changing the view, or changing the time. Controlling both at the same time is hard. This means exploring 360 videos quickly is difficult. To this end, we created the Route Tapestries approach <a class="citation" href="#li2021routetapestries">(Li, Lyu, Sousa, Balakrishnan, Tang, &amp; Grossman, 2021)</a>, which borrows from the slit-tear approach <a class="citation" href="#tang2008exploring">(Tang, Greenberg, &amp; Fels, 2008)</a>. This technique creates continuous orthographic-perspective projection of scenes along camera routes, which allows users to quickly scan the entire video rapidly without having to watch the video in time.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/kumar2022tourgetherlbw"><span id="kumar2022tourgetherlbw">Kartikaeya Kumar, Lev Poretski, Jiannan Li, and Anthony Tang. (2022). <span style="text-decoration: underline">Tourgether360: Exploring 360° Tour Videos with Others</span>. In <i>EA CHI ’22: Extended Abstracts of the 2022 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(poster).</a><br />

	<small>Acceptance: 36.1% (261/722).</small>



	<br />


	<a href="http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.mp4"><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/kumar2022tourgetherlbw/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/kumar2022tourgether"><span id="kumar2022tourgether">Kartikaeya Kumar, Lev Poretski, Jiannan Li, and Anthony Tang. (2022). <span style="text-decoration: underline"> Tourgether360: Collaborative Exploration of 360 Tour Videos using Pseudo-Spatial Navigation</span>. <i>Proceedings of the ACM on Human-Computer Interaction (PACMHCI)</i> 6, CSCW2.</span>

(conference).</a><br />




	<a href="https://hcitang.org/papers/2022-cscw2022-kumar-tourgether360.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://cscw.acm.org/2022/"><i class="fa fa-sticky-note-o"></i></a>


	<a href="https://hcitang.org/papers/2022-cscw2022-kumar-tourgether360.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3555604"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/kumar2022tourgether/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/li2021routetapestries"><span id="li2021routetapestries">Jiannan Li, Jiahe Lyu, Mauricio Sousa, Ravin Balakrishnan, Anthony Tang, and Tovi Grossman. (2021). <span style="text-decoration: underline">Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations</span>. In <i>UIST 2021: Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology</i>.</span>

(conference).</a><br />

	<small>Acceptance: 95/367 - 25.9%.</small>



	<br />


	<a href="http://hcitang.org/papers/2021-uist2021-li-routetapestries.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=0uAhUxJXgpc"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2021-uist2021-li-routetapestries.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3472749.3474746"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/li2021routetapestries/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/heshmat2018beam"><span id="heshmat2018beam">Yasamin Heshmat, Brennan Jones, Xiaoxuan Xiong, Carman Neustaedter, Anthony Tang, Bernhard E. Riecke, and Lillian Yang. (2018). <span style="text-decoration: underline">Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</span>. In <i>CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems</i>, Paper 359.</span>

(conference).</a><br />

	<small>Acceptance: 25.7% - 667/2595.</small>


	<small>Notes: 10 pages.</small>


	<br />


	<a href="http://hcitang.org/papers/2018-chi2018-geocaching-with-a-beam.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://chi2018.acm.org"><i class="fa fa-sticky-note-o"></i></a>



	<a href="http://doi.acm.org/10.1145/3173574.3173933"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/heshmat2018beam/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2017360videochat"><span id="tang2017360videochat">Anthony Tang, Omid Fakourfar, Carman Neustaedter, and Scott Bateman. (2017). <span style="text-decoration: underline">Collaboration in 360° Videochat: Challenges and Opportunities</span>. In <i>DIS 2017: Conference on Designing Interactive Systems 2017 </i>, 1327–1339.</span>

(conference).</a><br />

	<small>Acceptance: 24% - 110/458.</small>


	<small>Notes: Appendix material: http://dspace.ucalgary.ca/handle/1880/51950.</small>


	<br />


	<a href="http://hcitang.org/papers/2017-dis2017-360videochat.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://dis2017.org/"><i class="fa fa-sticky-note-o"></i></a>


	<a href=""><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/3064663.3064707"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2017360videochat/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2017watching360together"><span id="tang2017watching360together">Anthony Tang and Omid Fakourfar. (2017). <span style="text-decoration: underline">Watching 360° Videos Together</span>. In <i>CHI 2017: Proceedings of the 2017 SIGCHI Conference on Human Factors in Computing Systems </i>, 4501–4506.</span>

(conference).</a><br />

	<small>Acceptance: 25% - 600/2400.</small>


	<small>Notes: Presentation - https://www.youtube.com/watch?v=OPc4mBD7pgw.</small>


	<br />


	<a href="http://hcitang.org/papers/2017-chi2017-watching360video.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://chi2017.acm.org"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2017-chi2017-watching360video.mp4"><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/3025453.3025519"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2017watching360together/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/jones2015mobilecamerawork"><span id="jones2015mobilecamerawork">Brennan Jones, Anna Witcraft, Anthony Tang, Scott Bateman, and Carman Neustaedter. (2015). <span style="text-decoration: underline">Mechanics of Camera Work in Mobile Video Collaboration</span>. In <i>CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems</i>, ACM, 957–966.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.mp4"><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/2702123.2702345"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/jones2015mobilecamerawork/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2008exploring"><span id="tang2008exploring">Anthony Tang, Saul Greenberg, and Sidney Fels. (2008). <span style="text-decoration: underline">Exploring video streams using slit-tear visualizations</span>. In <i>AVI ’08: Proceedings of the working conference on Advanced visual
    interfaces</i>, ACM, 191–198.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2008-avi2008-slit-tear.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://www.youtube.com/watch?v=-kvMth6IpNw"><i class="fa fa-film"></i></a>


	<a href="http://doi.acm.org/10.1145/1385569.1385601"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2008exploring/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Haptics in VR</title><link href="/projects/haptics/" rel="alternate" type="text/html" title="Haptics in VR" /><published>2023-06-27T00:00:00+08:00</published><updated>2023-06-27T00:00:00+08:00</updated><id>/projects/haptics-in-vr</id><content type="html" xml:base="/projects/haptics/"><![CDATA[<!-- 160x144 -->

<p>Virtual Reality experiences typically focus on the visual aspects of immersion. This project has explored the challenges of creating haptic experiences for such immersive experiences – first through the use of tangible devices, and then through the use of visio-haptic illusions.</p>

<p>One way to create haptic experiences is through the use of tangible devices – that is, things that you can physically hold and feel. Yet, the challenge with this is that we may have fantastical items in VR that are difficult to re-create in real life. To this end, we developed the Tangi toolkit <a class="citation" href="#feick2020tangi">(Feick, Bateman, Tang, Miede, &amp; Marquardt, 2020)</a>, which provided a means for designers to create physical objects that approximate what we see in the VR space. To some extent, these devices also had the capacity to be moved, and this added an element of additional realism.</p>

<p>The experience with Tangi made us realize that we were relying on a type of visuo-haptic illusion – that is, the idea that the visual aspects of the experience override our experience of the haptic sensation. This allowed us, for instance, to use blocky building blocks to represent what would otherwise be a smoothly-shaped bunny rabbit. To this end, we wanted to explore the limits of these illusions: to what extent can you approximate the physical/motor aspects of interacting with these objects without creating a “break” in the illusion? We explored these with linear physical proxies <a class="citation" href="#feick2021illusions">(Feick, Kleer, Zenner, Tang, &amp; Krüger, 2021)</a>, and then later more generally with multiple variables at play <a class="citation" href="#feick2022illusions">(Feick, Regitz, Tang, &amp; Krüger, 2022)</a>.</p>

<p>The work has resulted in interesting side projects with the idea of drones creating these illusions <a class="citation" href="#feick2022hapticpuppet">(Feick, Tang, &amp; Krug̈er, 2022)</a>, as well as toolkits for measuring immersion within VR <a class="citation" href="#feick2020vrqt">(Feick, Kleer, Tang, &amp; Krüger, 2020)</a>. Further, we have now begun exploring physiological metrics to determine when the illusion breaks <a class="citation" href="#feick2023handredirection">(Feick, Regitz, Tang, Jungbluth, Rekrut, &amp; Krüger, 2023)</a>, which should allow us to one day create rich experiences without subjective reports on immersion.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/feick2023handredirection"><span id="feick2023handredirection">Martin Feick, Kora Persephone Regitz, Anthony Tang, Tobias Jungbluth, Maurice Rekrut, and Antonio Krüger. (2023). <span style="text-decoration: underline">Investigating Noticeable Hand Redirection in Virtual Reality using Physiological and Interaction Data</span>. In <i>IEEE VR ’23: IEEE Conference on Virtual Reality and 3D User Interfaces</i>.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2023-vr2023-feick-handredirection.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="https://www.youtube.com/watch?v=HBqPnrxU0ek"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1109/VR55154.2023.00035"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/feick2023handredirection/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/feick2022illusions"><span id="feick2022illusions">Martin Feick, Kora Persephone Regitz, Anthony Tang, and Antonio Krüger. (2022). <span style="text-decoration: underline">Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass</span>. In <i>CHI 2022: Proceedings of the 2022 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 24.7% - 638/2579.</small>



	<br />


	<a href="http://hcitang.org/papers/2022-chi2022-feick-illusions.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://hcitang.org/papers/2022-chi2022-feick-illusions-talk.mp4"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2022-chi2022-feick-illusions.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3491102.3517671"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/feick2022illusions/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/feick2022hapticpuppet"><span id="feick2022hapticpuppet">Martin Feick, Anthony Tang, and Krug̈er. (2022). <span style="text-decoration: underline"> HapticPuppet: A Kinesthetic Mid-air Multidirectional Force-Feedback Drone-based Interface </span>. In <i>UIST ’22: The Adjunct Publication of the 35th Annual ACM Symposium on User Interface Software and Technology</i>.</span>

(poster).</a><br />




	<a href=" http://hcitang.org/papers/2022-uist2022poster-feick-hapticpuppet.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href=" http://hcitang.org/papers/2022-uist2022poster-feick-hapticpuppet.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3526114.3558694"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/feick2022hapticpuppet/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/feick2021illusions"><span id="feick2021illusions">Martin Feick, Niko Kleer, André Zenner, Anthony Tang, and Antonio Krüger. (2021). <span style="text-decoration: underline">Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality</span>. In <i>CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 26.3% - 749/2844.</small>



	<br />


	<a href="http://hcitang.org/papers/2021-chi2021-feick-visio-haptic-illusions.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2021-chi2021-feick-visio-haptic-illusions.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3411764.3445456"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/feick2021illusions/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/feick2020vrqt"><span id="feick2020vrqt">Martin Feick, Niko Kleer, Anthony Tang, and Antonio Krüger. (2020). <span style="text-decoration: underline">The Virtual Reality Questionnaire Toolkit</span>. In <i>AP UIST 2020: Adjunct Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, UIST 2019</i>.</span>

(poster).</a><br />




	<a href="http://hcitang.org/papers/2020-uist2020wip-feick-vrqt.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://github.com/MartinFk/VRQuestionnaireToolkit"><i class="fa fa-sticky-note-o"></i></a>



</section><a class="details" href="/papers/feick2020vrqt/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/feick2020tangi"><span id="feick2020tangi">Martin Feick, Scott Bateman, Anthony Tang, André Miede, and Nicolai Marquardt. (2020). <span style="text-decoration: underline">TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality</span>. In <i>ISMAR 2020: 2020 IEEE International Symposium on Mixed and Augmented Reality</i>.</span>

(conference).</a><br />

	<small>Acceptance: 28.8% - 87/302.</small>



	<br />


	<a href="http://hcitang.org/papers/2020-ismar2020-feick-tangi.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2020-ismar2020-feick-tangi.mp4"><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/feick2020tangi/"><!-- --></a></li></ol>]]></content><author><name></name></author><category term="projects" /><category term="active" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Recruiting PhD Students for 2020</title><link href="/blog/2019/recruiting-for-2020/" rel="alternate" type="text/html" title="Recruiting PhD Students for 2020" /><published>2019-12-07T00:00:00+08:00</published><updated>2019-12-07T00:00:00+08:00</updated><id>/blog/2019/recruiting-for-2020</id><content type="html" xml:base="/blog/2019/recruiting-for-2020/"><![CDATA[<p>RICELab in the Faculty of Information at the University of Toronto is seeking PhD students with a strong interest in HCI/UX research, and a suitable foundation to design, build, and evaluate novel interactive experiences. Current research themes for RICELab include:</p>

<ul>
  <li>Mixed reality (AR/VR) interfaces and tools for collaboration;</li>
  <li>Tools for sharing and learning skills, knowledge and cultural practices, and</li>
  <li>Interfaces sharing experiences.</li>
</ul>

<p>Qualified applicants would meet one or more of the following conditions:</p>

<ul>
  <li>Completed a Masters thesis in behavioural, design, or systems aspects of HCI.</li>
  <li>Interested in pursuing an HCI/UX PhD in an Information School context, building relationships across disciplines.</li>
  <li>Have literacy/experience in HCI user research (quantitative/qualitative/mixed- methods and analysis), and/or HCI system design.</li>
  <li>Have a professor working in HCI/UX write a letter of recommendation.</li>
</ul>

<p>The PhD student will receive full funding through four years (stipends through TA, RA or scholarship), contingent on maintenance of good academic standing.</p>

<p>Interested students should explore Dr Tony Tang’s website to understand the research themes and directions of the lab (http://hcitang.github.io/ * http://ricelab.cpsc.ucalgary.ca/). Please contact Tony for further discussion.</p>

<p>Applications into the program are due by January 15, 2020. https://ischool.utoronto.ca/future-students/apply/phd-apply/</p>

<p>RICELab (Rethinking Interaction Collaboration and Engagement) Lab focuses on designing, building and evaluating novel user experiences for communication and collaboration. We are situated within the Faculty of Information, home to a diverse community of researchers that explore the intersections of information, interaction, culture, and society. We are colocated with the Knowledge Media Design Institute-Semaphore group, which also conducts human-centered research.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[RICELab in the Faculty of Information at the University of Toronto is seeking PhD students with a strong interest in HCI/UX research, and a suitable foundation to design, build, and evaluate novel interactive experiences. Current research themes for RICELab include:]]></summary></entry><entry><title type="html">ILab CHI Club 2018</title><link href="/blog/2018/ilabchiclub2018/" rel="alternate" type="text/html" title="ILab CHI Club 2018" /><published>2018-08-29T00:00:00+08:00</published><updated>2018-08-29T00:00:00+08:00</updated><id>/blog/2018/ilabchiclub2018</id><content type="html" xml:base="/blog/2018/ilabchiclub2018/"><![CDATA[<blockquote>
  <p>CHI Club is designed as opportunities for CHI authors to get feedback from other members of the lab, and for reviewers to gain experience in reading and providing useful critique for others who are writing.</p>
</blockquote>

<p>Your writing sucks.</p>

<p>It sucks because you are not the person who is ultimately going to read your writing, and so you do not know what others’ perspectives on your work are – namely, what the blindspots in your writing are. The purpose of a review process is to help show you: (1) what those blindspots are in your own writing so you can improve the writing to cover them, and (2) how obviously glaring blindspots are to you as a reviewer when you are reading someone else’s work. The final “learning objective” of this process is to show you that your writing is never great on the first go, and really great writing happens after iterating on it several times.</p>

<h1 id="ilab-chi-club-2018">ILab CHI Club 2018</h1>

<p>ILab CHI Club 2018 is here to help improve your CHI 2019 submission. As an author, you will also write two reviews of others’ work, and then provide feedback to principal authors in a small meeting. Thus, as an author, you will get feedback from others, alongside of getting the chance to help others. If you are not an author, you are also more than welcome to join in the fun by reviewing others’ work and helping to improve the lab’s output.</p>

<p>We have an aggressive timeline to help you get on track. But, this is not intended to be a “make it or break it” for folks that cannot hit the deadlines – if you can’t make the deadline, you can still contribute reviews (to help in that way); however, you will need to solicit reviews outside of this little “club.”</p>

<h1 id="timeline">Timeline</h1>

<h2 id="phase-1">Phase 1</h2>
<ul>
  <li>9/5 Wed (noon): Authors: Submit <strong>only the first page</strong> of your submission to your reviewers (cc your co-authors)</li>
  <li>9/6 Thu (3pm): Tony’s talk: The First Page
    <ul>
      <li>Bring your laptops. We will make a new submission to PCS for your submission.</li>
    </ul>
  </li>
  <li>9/7 Fri (noon): Reviewers: Draft a paragraph about what you expect to see in the full paper based on what you have seen in the introduction. Send this to all the co-authors on the submission. If you can, meet with the first author to discuss your suggestions.
    <ul>
      <li>Crucially: make note of what the problem is, the approach, the evaluation, and what the contribution is supposed to be. Tell the authors what factors you’d expect to see in the submission for it to be successful.</li>
      <li>Authors: consider the reviewers’ feedback, and consider particularly how your introduction sets up certain kinds of expectations (one way or another). If it seems like it’s not matching up with how the rest of the submission will go, this should tell you something. Revise the introduction! (or the rest of your submission)</li>
    </ul>
  </li>
</ul>

<h2 id="phase-2">Phase 2</h2>
<ul>
  <li>9/10 Mon (3pm): Authors: Submit a <strong>complete draft</strong> of your submission to your reviewers. (cc your co-authors)</li>
  <li>9/10 Mon (4pm): Tony’s talk: Providing a review and feedback</li>
  <li>9/13 Thu (5pm): Reviewers: send a review back to the authors (cc’ing all authors). Ideally, meet with at least the lead author at this time, too.</li>
</ul>

<!--
# Submission Site

This is a real submission site that is used for many conferences. You will need to create an account on the site if you do not have one, and register to author and review submissions.

* [Submission site](https://easychair.org/conferences/?conf=ilabchiclub2016)
-->

<h1 id="rules">Rules</h1>

<p>The first rule of CHI Club is: you do not talk about CHI Club. The second rule of CHI Club is: you do not talk about CHI Club. (<a href="http://www.diggingforfire.net/fightclub/">Kidding!</a>)</p>

<p>Okay, seriously, here are the rules:</p>

<ol>
  <li>You need to complete at least <strong>2</strong> reviews for every paper you submit as a first author. If your co-authors can contribute additional reviews, or you want to pass one of those reviews off to a co-author, excellent. But, you should do at least one personally.</li>
  <li>You adhere to the timeline – particularly for submitting your full draft, and your reviews.</li>
  <li>You commit as a reviewer to reading drafts of others’ submissions, and working with (and meeting with) the author to improve the work.</li>
  <li>As an author, your responsibility is to arrange the meeting with your reviewer(s)  to go over the feedback to improve your work.</li>
</ol>

<h1 id="faq">FAQ</h1>

<ul>
  <li><strong>I am not a first author. Can I still review submissions?</strong></li>
</ul>

<p>Absolutely! Just let Tony know.</p>

<ul>
  <li><strong>I do not know how to provide feedback. What do I do?</strong></li>
</ul>

<p>Tony will develop a presentation that helps talk you through this issue. Additionally, we will provide a rubric that you can work with you develop your reviewer. Remember that ultimately, you want to provide the author with feedback to help improve their work – the purpose here is not to write an excellent prose review.</p>

<!--
* **I am a post doc.**

That's not a question, but great. You'll be assigned 4-6 reviews. Thanks for your service.
-->

<ul>
  <li><strong>Do I have to do this?</strong></li>
</ul>

<p>Absolutely not. It is voluntary. That is, unless you’re Tony’s student, in which case, it is mandatory.</p>]]></content><author><name></name></author><category term="internal" /><summary type="html"><![CDATA[CHI Club is designed as opportunities for CHI authors to get feedback from other members of the lab, and for reviewers to gain experience in reading and providing useful critique for others who are writing.]]></summary></entry><entry><title type="html">Writing Reviews for HCI</title><link href="/blog/2018/writing-reviews-for-hci/" rel="alternate" type="text/html" title="Writing Reviews for HCI" /><published>2018-07-17T00:00:00+08:00</published><updated>2018-07-17T00:00:00+08:00</updated><id>/blog/2018/writing-reviews-for-hci</id><content type="html" xml:base="/blog/2018/writing-reviews-for-hci/"><![CDATA[<p>My primary goal in this post is to impart my philosophy on reviewing HCI conference paper submissions. This guides how I think a review ought to be formulated – what are its constituent parts, and what is the role of each part. Thus, a secondary goal is to suggest how a review can be constructed by providing a bit of a template.</p>

<p>As context, I have written and reviewed HCI papers since about 2003 or so. I have received a lot of really great and useful reviews, and I have also received a lot of nasty and not-so-useful reviews. This means I have been heartbroken a lot of times. I have also served on a program committee a few times, which means that as one of the people trying to decide what submissions are accepted as papers, I have been on the “first part of” the receiving end of reviews. In this context, I have seen a lot of good reviews (useful) and also a few not-so-useful reviews.</p>

<h2 id="my-philosophy">My Philosophy</h2>

<ul>
  <li><strong>A review is a one-direction communiqué from an “expert.”</strong> I always think about the days of Newton, where he would write letters to his contemporaries, and these were the “papers” of the day. Colleagues would write back, offering suggestions and ideas. This was the main form of intellectual exchange of the day: Newton says, “Hey, I think X now, because I did Y and Z, and got R results! What do you think?” And his friend writes back and says, “Yo, X1 makes sense, but X2 totally does not make sense – it’s not supported by R yet. Maybe you can do Z2, and if you got R2, that would help.” These letters and exchanges are about helping one another and “building science” through writing and supporting arguments.</li>
</ul>

<p>Peer-reviewed conference papers are sort of a “mass-scale” version of this in terms of dissemination. Reviews are, unfortunately, a one-directional communiqué from an “expert.” The one-directional nature means that the author doesn’t really get to do a back-and-forth to clarify problems, and in more recent times, it isn’t clear that the reviewer is always an expert in the space (at least, I rarely feel like I am an expert in most of the things I review).</p>

<p>But, the philosophy should still be the same: the author is saying, “Hey! I think X because I did Y and got result R. What do you think?” And the reviewer is writing back and saying, “Oh yeah, that makes sense,” or “Oh wait, something doesn’t make sense here… Did you think of this?”</p>

<ul>
  <li>
    <p><strong>Authors want to make sure they are understood.</strong> One of the most frustrating things as an author is when I feel like the reviewer didn’t understand the paper as I wrote it. To this end, the reviewer should try to make it clear at the outset: “This is how I understood the paper.” This serves as a gut-check for the authors–Did I write it clearly enough for the reviewer to understand?</p>
  </li>
  <li>
    <p><strong>Assume the authors are doing their best: every work likely has some merit.</strong> I think that authors, when they submit something, are trying to submit only because they think the work has merit and ought to be published. To this end, I try to figure out what the main idea was – even if the authors didn’t do a good enough job of articulating it themselves. What was the intended contribution, if it wasn’t stated?</p>
  </li>
  <li>
    <p><strong>Authors have blindspots, but as a reviewer, I have blindspots too.</strong> Invariably, we all have blindspots when we make arguments. To this end our job as a reviewer is to help identify those blindspots for the authors, and how to try to fix the blindspot. That said, I think particularly in HCI, it is important to be open to new ways of doing research, new ways of doing science, and new ways of contributing knowledge. To this end, I try to be open to new approaches to how research can be done, and avoid being too mono-culture in terms of how I think things can/ought to be done.</p>
  </li>
  <li>
    <p><strong>Most HCI research is not Chemistry.</strong> Most HCI research is not going to have results that are as clean as Grade 11 titration experiments/studies. To this end, it is mostly about a well-thought-through and clear argument in a paper. When uncertain, I provide a higher recommendation rather than a lower one. I think on balance, most papers are right around the middle, and over time, what will matter more is whether it contributes to the overall state of knowledge in the research space. In this sense, don’t worry about accepting a few papers that aren’t that strong – over time, the research community will correct that via citations.</p>
  </li>
  <li>
    <p><strong>A paper is an argument. A review is also an argument.</strong> While this is the last bullet in this list, it is actually the most important thing. A paper is an argument, and to this end, it is made up of a set of arguments (where evidence is used to support the argument). If I am arguing in my review that there is a weakness or a problem, this should be saying that there is a problem <em>with the argument</em> in the sense that “<em>the evidence does not support the argument,”</em> or <em>“the evidence is not strong enough to support the argument.</em>” It is important to think about it this way because it helps to make clear what my review is. My review is also an argument – one that others can disagree with, but that I can strengthen by providing evidence or through logic. <em>Why</em> is this a weakness? How do I argue that it is a weakness?</p>
  </li>
</ul>

<h2 id="my-template">My Template</h2>
<ul>
  <li><strong>Paragraph 1: Demonstrate that I understand what the paper is about.</strong> I try to write this in 4-5 sentences, and it is an attempt to be fact-based. Sentence 1: What is the problem/question the authors are exploring, and what is the motivation? Sentence 2: What is the approach the authors are taking to study the problem/question? Sentence 3: How do they evaluate their approach, or what is the main (set) of findings from the study? Sentence 4: What is the contribution of the work? Sentence 5. Free sentence.</li>
  <li><strong>Paragraph 2: Describe the primary contribution and strengths of the work.</strong> In a longer one paragraph form, I reiterate what the primary contribution of the work is, and particularly in a stronger submission, I discuss why the contribution is sound given the authors’ arguments (work) as described in the paper. This can sometimes be up to two paragraphs in length, and sometimes will be used to praise the mechanics of the submission, too (e.g. “Yo, the framing of the work is super clever”, or “Wow, the study design is ingenious!”, or “Hot damn, I wish I’d thought of this idea.”).</li>
  <li><strong>Paragraph 3: Offer a recommendation to the program committee.</strong> First sentence reiterates my recommendation to the committee (accept, reject, etc.). Then, I now build my own mini-essay to support my recommendation. If I am recommending accept, then I reiterate what was in Paragraph 2, and describe why this should be accepted. Otherwise, I briefly state what the weaknesses are (to be elaborated on below).</li>
  <li><strong>Paragraph 4: Describe each weakness in turn, with recommendations on how to improve it.</strong> To make this legible, sometimes this is done as a set of bullet points, or mini-paragraphs. Remember that this part is about making an argument. If there is something weak in the submission, then it needs to be a weakness in the sense that <em>it does not support the main argument</em> or <em>it does not do a good enough job of supporting the main argument.</em> So, given this is the case, I always try to provide a fix on how to improve the weakness. This is where things get tricky, but the point about the “weakness of the argument” is to try to get away from an opinion (e.g. “well, I didn’t think they talked enough about X” – the question is WHY is it not enough? is that an opinion (i.e. immaterial), or is it because by not talking enough about X, they are actually hindering the argument?). Really think about <em>how can this be improved</em> and offer this up. Doing so makes the review constructive rather than emotionally destructive. Note: if these weaknesses can be fixed in a half day’s worth of work, I consider them to be, something that can be fixed in a revision. These are not something that a submission should be rejected for.</li>
  <li><strong>Paragraph 5: Describe opportunities for improvement.</strong> Here is where the laundry list of “minor nitpicks” can go. None of these are problems with the argument, but usually smaller things with presentation – typos, problems with the figures, and so on.</li>
</ul>

<p>Each paragraph in this structure serves a distinct purpose. Para 1 tells the author “This is how I understood your paper.” Para 2 makes the author feel good. Para 3 is mainly for the program committee – it is about what your recommendation is, and why you think it is what it is. Para 4 plays two roles: it serves to support your para 3 recommendation, and also for the authors on how to improve the work. Para 5 is mainly for the perfectionist in you, letting you get things off your chest.</p>

<h2 id="things-to-avoidother-tips">Things to Avoid/Other Tips</h2>
<ul>
  <li>Avoid expecting the methods to look exactly like you’d see in a textbook. A textbook example of a method is necessarily a very abstract, simplified study so that for the learner, it is easy to see the main idea how the study/methods are executed. Reality is much messier, and much more difficult to work with. To this end, we need to be more flexible. Avoid making judgements on a submission because its study is not perfect – instead, consider the main argument of the study, and whether the evidence is sufficient to make the case.</li>
  <li>Avoid ticking off “Accept this paper”, but then providing only a list of problems. This is really difficult for the PC to deal with, because it is unclear what you really mean. To avoid this, if you are arguing to accept the paper, then you really want to back it up with what the strengths or contributions of the paper are.</li>
  <li>Avoid providing a decision without any supporting evidence. (See above)</li>
  <li>Avoid providing an opinion that is strictly that – an opinion. A recommendation ought to be based on an argument as to why something is a weakness. Ideally, if there is a weakness, a fix ought to be offered up, too.</li>
  <li>Sometimes one or a handful of citations is missing; however, this is rarely a cause to reject a paper submission. Reflect carefully on whether the missing reference is actually weakening the argument the authors are making.</li>
  <li>Focus on the ideas in the paper rather than the writing quality. The ideas are the central point of the idea – often, writing style is a matter of taste.</li>
  <li>Be aware that there are many ways to “arrive at knowledge” in HCI, and be aware of your own intellectual background. Early on, we tend to think of the world and knowledge mainly from the school of thought from which we were trained; it is difficult to understand other schools of thinking and how these other schools arrive at knowledge, but be aware they exist. Think of this as an ongoing learning opportunity for yourself. For me, I was trained from a quantitative experimental psychology background, so seeing ethnographies and interview studies was really difficult in the beginning.</li>
</ul>

<p>My thanks to Carman Neustaedter, who offered comments on an earlier draft of this.</p>

<h2 id="resources">Resources</h2>
<p>Others have written about this before, too.</p>
<ul>
  <li>http://mobilehci.acm.org/2015/download/ExcellenceInReviewsforHCICommunity.pdf</li>
  <li>http://chicourse.acagamic.com/wp-content/uploads/2017/05/How-to-Write-and-Review-CHI-Papers-CHI2017.pdf</li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[My primary goal in this post is to impart my philosophy on reviewing HCI conference paper submissions. This guides how I think a review ought to be formulated – what are its constituent parts, and what is the role of each part. Thus, a secondary goal is to suggest how a review can be constructed by providing a bit of a template.]]></summary></entry></feed>