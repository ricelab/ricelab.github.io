@inproceedings{james2022flavorvideos,
  author = {James, Meetha Nesam and Ranasinghe, Nimesha and Tang, Anthony and Oehlberg, Lora},
  title = { Flavor-Videos: Enhancing the Flavor Perception of Food while Eating with Videos },
  abstract = {People are typically involved in different activities while eating, particularly when eating alone, such as watching television or playing games on their phones. Previous research in Human-Food Interaction (HFI) has primarily focused on studying the motivation, and analysis of the media content watched while eating. However, their impact on human behavioral and cognitive processes, particularly flavor perception and its attributes, remains underexplored. We present a user study to investigate the influence of six types of videos, including mukbang - a new food video genre, on flavor perceptions (taste sensations, liking, and emotions) while eating plain white rice. Our findings revealed that participants perceived positive emotional changes and reported significant differences in their augmented taste sensations (e.g., spicy and salty) with different food-based videos. Our findings provided insights into using our approach to promote digital commensality and healthier eating (digital augmentation without altering the food), highlighting the scope for future research.},
  booktitle = {IMX 2022: ACM International Conference on Interactive Media Experiences},
  pdfurl = {}, 
  Year = {2022},
  type = {conference},
  url = {https://imx.acm.org/2022/},
  Acceptance={40% - 19/47},

}

@inproceedings{pratte2022nlitenworkshop,
  author = {Pratte, Sydney and Hoover, Shannon and Tang, Anthony and Oehlberg, Lora},
  title = { nLITEn: A Wearables Toolkit for Enabling Creativity in Fashion Technology Design },
  abstract = {The nLITEn toolkit is designed to lower barriers to technology and enable creativity in fashion-tech designers during their design process. The toolkit comprises durable, quick connecting components and a small custom-designed board installed with Bekonix OS. With nLITEn, designers can explore the effects of the technology in their garments without losing time in creativity.},
  booktitle = {CHI22 Workshop: Toolkits & Wearables - Developing Toolkits for Exploring Wearable Designs},
  Editor={Çaglar Genç and Oguz Buruk and Shiva Jabari and Lee Jones and Kirill Ragozin and Kate Hartman and Johanna Virkki and Oskar Juhlin and Kai Kunze and Jonna Häkkilä},
  pdfurl = { http://hcitang.org/papers/2022-chi2022workshop-pratte-nliten.pdf}, 
  videourl = { http://hcitang.org/papers/2022-chi2022workshop-pratte-nliten-poster.pdf},
  Year = {2022},
  type = {workshop},
  url = {http://techfashion.design/wearable-toolkits/}
}

@inproceedings{james2022flavorsworkshop,
  author = {James, Meetha Nesam and Ranasinghe, Nimesha and Tang, Anthony and Oehlberg, Lora},
  title = { Watch Your Flavors: Augmenting People's Flavor Perceptions and Associated Emotions based on Videos Watched while Eating},
  abstract = {People engage in different activities while eating alone, such as watching television or scrolling through social media on their phones. However, the impacts of these visual contents on human cognitive processes, particularly related to flavor perception and its attributes, are still not thoroughly explored. This paper presents a user study to evaluate the influence of six different types of video content (including nature, cooking, and a new food video genre known as mukbang) on people’s flavor perceptions in terms of taste sensations, liking, and emotions while eating plain white rice. Our findings revealed that the participants’ flavor perceptions are augmented based on different video content, indicating significant differences in their perceived taste sensations (e.g., increased perception of salty and spicy sensations). Furthermore, potential future implications are revealed to promote digital commensality and healthier eating habits.},
  booktitle = {CHI22 Workshop: Future of Emotion in Human-Computer Interaction},
  Editor={Anna Cox and James Gross and Kristina Höök and Vassilis Kostakos and Peter Koval and Regan Mandryk and Petr Slovák and Wally Smith and Greg Wadley and Sara Webber},
  pdfurl = { http://hcitang.org/papers/2022-chi2022workshop-james-watch-your-flavors.pdf}, 
  Year = {2022},
  type = {workshop},
  url = {https://cis.unimelb.edu.au/hci/emotion-workshop}
}

@inproceedings{kumar2022tourgetherlbw,
  Abstract={Contemporary 360° video players do not provide ways to let people explore the videos together. Tourgether360 addresses this problem for 360° tour videos using a pseudo-spatial navigation technique that provides both an overhead “context” view of the environment as a minimap, as well as a shared pseudo-3D environment for exploring the video. Collaborators appear as avatars along a track depending on their position in the video timeline and can point and synchronize their playback. In this work, we describe the intellectual precedents for this work, our design goals, and our implementation approach of Tourgether360. Finally, we discuss future work based on this prototype.},
  Author={Kumar, Kartikaeya and Poretski, Lev and Li, Jiannan and Tang, Anthony},
  Booktitle={EA CHI '22: Extended Abstracts of the 2022 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Tourgether360: Exploring 360° Tour Videos with Others},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.pdf},
  Year={2022},
  videourl={http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.mp4},
  Acceptance={36.1% (261/722)}
}

@inproceedings{james2022flavorslbw,
  Abstract={People engage in different activities while eating alone, such as watching television or scrolling through social media on their phones. However, the impacts of these visual contents on human cognitive processes, particularly related to flavor perception and its attributes, are still not thoroughly explored. This paper presents a user study to evaluate the influence of six different types of video content (including nature, cooking, and a new food video genre known as mukbang) on people's flavor perceptions in terms of taste sensations, liking, and emotions while eating plain white rice. Our findings revealed that the participants' flavor perceptions are augmented based on different video content, indicating significant differences in their perceived taste sensations (e.g., increased perception of salty and spicy sensations). Furthermore, potential future implications are revealed to promote digital commensality and healthier eating habits.},
  Author={James, Meetha Nesam and Ranasinghe, Nimesha and Tang, Anthony and Oehlberg, Lora},
  Booktitle={EA CHI '22: Extended Abstracts of the 2022 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Watch Your Flavors: Augmenting People's Flavor Perceptions and Associated Emotions based on Videos Watched while Eating},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2022-chi2022lbw-james-flavors.pdf},
  Videourl={http://hcitang.org/papers/2022-chi2022lbw-james-flavors.mp4},
  url={http://hcitang.org/papers/2022-chi2022lbw-james-flavors-poster.pdf},
  Year={2022},
  Acceptance={36.1% (261/722)}
}


@inproceedings{poretski2022learnability,
  Abstract={Learnability is a core aspect of software usability. Video games are not an exception, as game designers need to teach players how to play their creations. We analyzed 40 contemporary video games to identify how video games approach learning experiences. We found that games have advanced far beyond using simple tutorials or demonstration screens and adopt a range of repeatable and reusable design strategies using visual cues to facilitate learning. We provide a detailed descriptive framework of these design strategies, elucidating how and when they can be used, and describing how the visual cues are used to build them. Our research can be useful for both general HCI researchers and practitioners seeking to tap into the rich ideas from video game learnability design looking for practical solutions for their work.},
  Author={Poretski, Lev and Tang, Anthony},
  Booktitle={CHI 2022: Proceedings of the 2022 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Press A to Jump: Design Strategies for Video Game Learnability},
  Type={conference},
  Year={2022},
  Acceptance={24.7% - 638/2579},
  pdfurl={http://hcitang.org/papers/2022-chi2022-poretski-video-game-learnability.pdf},
  videourl={http://hcitang.org/papers/2022-chi2022-poretski-video-game-learnability-talk.mov}
}

@inproceedings{feick2022illusions,
  Abstract={Visuo-haptic illusions are a method to expand proxy-based interactions in VR by introducing unnoticeable discrepancies between the virtual and real-world. Yet, how different design variables affect the illusions with proxies is still unclear. To unpack a subset of variables, we conducted two user studies with 48 participants to explore the impact of (1) different grasping types and movement trajectories, and (2) different grasping types and object masses on the discrepancy which may be introduced. Our Bayes analysis suggests that grasping types and object masses (≤ 500 g) did not noticeably affect the discrepancy, but for movement trajectory, results were inconclusive. Further, we identified a significant difference between un-/restricted movement trajectories. Our data shows considerable differences in participants’ proprioceptive accuracy, which seem to correlate with their prior VR experience. Finally, we illustrate the impact of our key findings on the visuo-haptic illusion design process by showcasing a new design workflow.},
  Author={Feick, Martin and Regitz, Kora Persephone and Tang, Anthony and Kr\"uger, Antonio},
  Booktitle={CHI 2022: Proceedings of the 2022 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass},
  Type={conference},
  Year={2022},
  Acceptance={24.7% - 638/2579},
  pdfurl={http://hcitang.org/papers/2022-chi2022-feick-illusions.pdf},
  videourl={http://hcitang.org/papers/2022-chi2022-feick-illusions.mp4},
  url={http://hcitang.org/papers/2022-chi2022-feick-illusions-talk.mp4}
}

@mastersthesis{stark2020thesis,
  author       = {Stark, Jessica Theresa}, 
  title        = {Redesigning instructional tools for novice makers},
  school       = {University of Calgary},
  year         = {2020},
  month        = {08},
  abstract = {People new to Making often struggle when instructional materials, such as written documentation or video tutorials, do not address certain difficulties they encounter. This thesis explores two examples of instructional materials for novice makers which have been redesigned to help them overcome these difficulties. The first example focuses on written documentation, often created by Makers who document their own projects, providing instructions based on tools and materials the author has which may not be available to the reader. To address this, I present MakeAware, a system designed to support situation awareness and help users make decisions about which tools and materials they can use to accomplish a task. For example, if the documentation instructs the maker to attach two pieces of the project together using screws and a screwdriver but the maker does not have these, MakeAware can help them improvise by guiding them to use a hammer and nails. The second example focuses on video tutorials which are often filmed from a single camera and cannot always capture the necessary information to understand the task. To address this, I designed a multi-camera playback interface that allows viewers to change angles during the tutorial, circumventing the visibility problems that can occur from providing only one angle. For example, in a tutorial about assembling a laser-cut lantern, the instructor’s hands often cover the fasteners between the pieces so the viewer cannot see how the attachment is made. By selecting an angle that shows the fasteners in the foreground instead of the hands when needed, the viewer may be able to better understand the process. I evaluated these interfaces in separate user studies with novice makers as participants. The first study showed that the documentation style of MakeAware helped novice makers customize the instructions to their needs. The second study showed that a video interface providing flexible perspectives could be helpful in avoiding visibility problems while highlighting that this approach may be suited to non-repetitive or asymmetrical tasks. The results of both studies provide design implications that can help us accommodate the needs of novice makers through written documentation and video tutorials.},
  Url={http://hdl.handle.net/1880/112384},
  Type={thesis},
}

@mastersthesis{mitchev2021thesis,
  author       = {Mintchev, Andrey Svetlozarov}, 
  title        = {Applying Videogame Narrative and Level Design To Museum Exhibit Design},
  school       = {University of Toronto},
  year         = {2021},
  month        = {11},
  abstract = {This thesis examines how practices of narrative and level design used in the videogame industry can be applied to the museum sector, and from this research, propose a set of design lessons that can be used for the creation of exhibits. To that end, this thesis hypothesizes that these practices from gaming will serve the purpose of elevating the visitor experience—specifically in the way museum spaces, artifacts and their stories are presented, navigated, and experienced by the visitor. In order to produce this set of design lessons for exhibit creation, a qualitative study of five videogames was conducted for the purpose of identifying methods of narrative delivery, and level design practices. In regard to narrative design, I establish three methods of narrative embedding, partly inspired by Huaxin Wei’s work, in addition to summarizing Ahmed Khalifa et al.’s findings, which contributed to this studies understanding of level design. The combined knowledge of these works where foundational in developing a unit of analysis, which was used to identify narrative and level design practices within my sample. The findings from this research was then translated into applicable design practices for museum exhibit creation, called design lessons. Moreover, the design lessons developed from this study were used in the creation of a prototype exhibit for the purpose of demonstrating its effectiveness. Thus, this study advocates that there are indeed lessons to be learned from videogames which offer new perspectives in design to the museum sector. That in doing so, this study’s interest in providing museum professionals with new views on narrative and level design, will have a positive affect on the visitor experience.},
  Url={http://hdl.handle.net/1807/109299},
  Type={thesis},
}

@article{jones2022rescuecastr,
  Abstract={Wilderness search and rescue (WSAR) is a command-and-control activity, where a Command team manages field teams scattered across a large area looking for a lost person. The challenge is that it can be difficult for Command to maintain awareness of field teams and the conditions of the field. We designed RescueCASTR, an interface that explores the idea of deploying field teams with wearable cameras that stream live video or sequential photos periodically to Command that aid contextual awareness. We ran a remote user study with WSAR managers to get an understanding of the opportunities and challenges of such a system. We found that the awareness provided by the camera footage could give additional confidence and comfort to Command, as well as reduce the need for explicit communications. However, it could also impact workers’ traditional roles and responsibilities, shifting the burden of responsibility toward Command. We conclude that, while wearable-camera footage could be beneficial to Command, they need to have the tools and means to narrow their focus within the abundance of information provided. Furthermore, camera streams should not be thought of as a replacement for more direct communications, but rather as another tool available to help Command supplement their understanding of events in the field and help them narrow their focus.},
  Author={Jones, Brennan and Tang, Anthony and Neustaedter, Carman},
  Journal={Proceedings of the ACM on Human-Computer Interaction (PACMHCI)},
  Title={RescueCASTR: Exploring Photos and Live Streaming to Support Contextual Awareness in the Wilderness Search and Rescue Command Post},
  Type={journal},
  Volume={2, CSCW},
  Year={2022},
  Doi={https://dl.acm.org/doi/10.1145/3512960},
  pdfurl={https://hcitang.org/papers/2022-cscw2022-rescuecastr.pdf},
  videourl={https://hcitang.org/papers/2022-cscw2022-rescuecastr.mp4}

}

@article{labrie2022onlineteaching,
  Abstract={Many instructors in computing and HCI disciplines use hands-on activities for teaching and training new skills. Beyond simply teaching hands-on skills like sketching and programming, instructors also use these activities so students can acquire tacit skills. Yet, current video-conferencing technologies do not effectively support hands-on activities in online teaching contexts. In order to better understand how instructors experience using online conferencing tools for hands-on activities, we conducted 15 interviews with university-level instructors who had quickly pivoted their use of hands-on activities to an online context during the early part of the COVID-19 pandemic. Based on our analysis, we uncovered four pedagogical goals that instructors have when using hands-on activities and how instructors were unable to adequately address them due to technological limitations. Our work provides empirical data about the challenges that many instructors experienced and a critical reflection on how to improve existing video-conferencing systems to better support hands-on activities.},
  Author={Labrie, Audrey and Mok, Terrance and Tang, Anthony and Lui, Michelle and Oehlberg, Lora and Poretski, Lev},
  Journal={Proceedings of the ACM on Human-Computer Interaction (PACMHCI)},
  Title={Toward Video-Conferencing Tools for Hands-On Activities in Online Teaching},
  Type={journal},
  Volume={6, GROUP},
  Year={2022},
  Pdfurl={http://hcitang.org/papers/2022-pacmgroup-labrie-video-conferencing.pdf},
  Article={10}
}

@article{kudo2021balancing,
  Abstract={Head-mounted displays (HMDs) increase immersion into virtual worlds. The problem is that this limits headset users’ awareness of bystanders: headset users cannot attend to bystanders’ presence and activities. We call this HMD boundary. We explore how to make the HMD boundary permeable by comparing different ways of providing informal awareness cues to the headset user about bystanders. We adapted and implemented three visualization techniques (Avatar View, Radar and Presence++) that share bystanders’ location and orientation with headset users. We conducted a hybrid user and simulation study with three different types of VR content (high, medium, low interactively) with twenty participants to compare how these visualization techniques allow people to maintain an awareness of bystanders, and how they affect immersion (compared to a baseline condition). Our study reveals that a see-through avatar representation of bystanders was effective, but led to slightly reduced immersion in the VR content. Based on our findings, we discuss how future awareness visualization techniques can be designed to mitigate the reduction of immersion for the headset user.},
  Author={Kudo, Yoshiki and Tang, Anthony and Fujita, Kazuyuki and Endo, Isamu and Takashima, Kazuki and Kitamura, Yoshifumi},
  Journal={Proceedings of the ACM on Human-Computer Interaction (PACMHCI)},
  Title={Towards Balancing VR Immersion and Bystander Awareness},
  Number={Article 484},
  Pdfurl={http://hcitang.org/papers/2021-pacmiss-balance-immersion-awareness.pdf},
  Videourl={http://hcitang.org/papers/2021-pacmiss-balance-immersion-awareness.mp4},
  Type={journal},
  Volume={5, ISS},
  Year={2021},
  DOI={https://doi.org/10.1145/3486950},
  Notes={Best Paper award},
}

@inproceedings{li2021routetapestries,
  Abstract={An increasingly popular way of experiencing remote places is by viewing 360 virtual tour videos, which show the surrounding view while traveling through an environment. However, finding particular locations in these videos can be difficult because current interfaces rely on distorted frame previews for navigation. To alleviate this usability issue, we propose Route Tapestries, continuous orthographic-perspective projection of scenes along camera routes. We first introduce an algorithm for automatically constructing Route Tapestries from a 360 video, inspired by the slit-scan photography technique. We then present a desktop video player interface using a Route Tapestry timeline for navigation. An online evaluation using a target-seeking task showed that Route Tapestries allowed users to locate targets 22% faster than with YouTube-style equirectangular previews and reduced the failure rate by 75% compared to a more conventional row-of-thumbnail strip preview. Our results highlight the value of reducing visual distortion and providing continuous visual contexts in previews for navigating 360 virtual tour videos.},
  Author={Li, Jiannan and Lyu, Jiahe and Sousa, Mauricio and Balakrishnan, Ravin and Tang, Anthony and Grossman, Tovi},
  Booktitle={UIST 2021: Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology},
  Title={Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations},
  Type={conference},
  Year={2021},
  Acceptance={95/367 - 25.9%},
  PdfUrl={http://hcitang.org/papers/2021-uist2021-li-routetapestries.pdf},
  VideoUrl={http://hcitang.org/papers/2021-uist2021-li-routetapestries.mp4},
  URL={https://www.youtube.com/watch?v=0uAhUxJXgpc},
}

@phdthesis{jones2021dissertation,
  Abstract={Wilderness search and rescue (WSAR) is the search for and extraction of one or more lost people (e.g., hikers, skiers) from a wilderness area. WSAR is time-critical, and even with current technologies, workers still face challenges in effective remote collaboration, information sharing, and awareness. The overarching goal of this dissertation is to understand how user interfaces can be designed to better support WSAR distributed collaboration. I approach this first by understanding how WSAR workers collaborate remotely using today's technologies. In the first phase of my research, I ran an investigative study in which I interviewed WSAR workers and observed a mock WSAR response. My findings demonstrate that the main goal of a system for WSAR distributed collaboration should be to help workers construct and maintain a shared mental model, but there are unique challenges to doing this when scattered and moving around the wilderness. Following this, I designed a prototype of a system for WSAR commanders. This system aims to provide commanders with more implicit awareness of events in the field and the experiences of field teams. It does this through (1) body cameras worn by field teams, streaming photos periodically to the command post; and (2) aggregating existing information channels together into one interface, allowing commanders to explore this information together as part of a bigger picture. I then evaluated this system through a remote user study. I found that the awareness provided by body-camera footage could give commanders additional confidence and comfort while reducing the need for explicit communications with field teams. However, it could also shift the burden of responsibility toward commanders. Overall, this work contributes the following: (1) an understanding of WSAR remote collaboration practices; (2) the design of an interface for providing commanders awareness of events in the field; (3) a method for studying WSAR user-interface technologies remotely through simulated scenarios; and (4) an understanding of the potential opportunities and challenges of new information streams and communication modalities in WSAR. Beyond WSAR, this work contributes more broadly to our understanding of how to design remote collaboration technologies for serious team-based activities in large outdoor environments.},
  Author={Jones, Brennan},
  Month={July},
  Url={https://prism.ucalgary.ca/handle/1880/113590},
  School={University of Calgary},
  Title={Designing Remote Collaboration Technologies for Wilderness Search and Rescue},
  Type={thesis},
  Year={2021}
}


@inproceedings{poretski2021whatsappworkshop,
  author = {Poretski, Lev and Taabassum, Taamannae and Tang, Anthony},
  title = {Immigrant Families' Health-Related Information Behaviour on Instant Messaging Platforms},
  abstract = {Each of the authors comes from an immigrant family, and we became fascinated with the challenge of how our multi-generational families managed conflicting information as it came in during the early days of the COVID-19 pandemic. While the flood of information has abated somewhat, there still remain some intergenerational differences and cross-cultural challenges when it comes to discussing and maintaining a family dialog about COVID-19 and handling it. We provide a brief outline of our personal positions on this matter, then outline a small pilot study where we conducted interviews with members of immigrant families to understand the matter. Our findings from that study suggest that individual’s motivations to discuss, share or verify information are complex, nuanced and most certainly influenced by the family’s culture.},
  booktitle = {Opinions, Intentions, Freedom of Expression, ..., and Other Human Aspects of Misinformation Online - Workshop at CHI 2021},
  pdfurl = {http://hcitang.org/papers/2021-chi2021workshop-whatsapp.pdf },
  videourl = {http://hcitang.org/papers/2021-chi2021workshop-whatsapp-presentation.pdf}, 
  Year = {2021},
  type = {workshop},
  url = {https://events.kmi.open.ac.uk/misinformation/}
}

@inproceedings{poretski2021whatsapplbw,
  Abstract={For immigrant families, instant messaging family groups are a common platform for sharing and discussing health-related information. Immigrants often maintain contact with their family abroad and trust information in shared IM family groups more than the information from local authorities and sources. In this study, we aimed to understand health-related information behaviors of immigrant families in their IM family groups. Based on the interviews with 6 participants from immigrant families to Canada, we found that immigrant families’ discourse on IM platforms is motivated by love and care for other family members. The families used local and international sources of information, judged information credibility by its alignment with their pre-existing knowledge, and mostly did not verify information further. The information shared by different users from different sources often contradicted one another. Yet, family members did not discuss the conflicting information due to their desire to avoid tensions.},
  Author={Poretski, Lev and Taabassum, Taamannae and Tang, Anthony},
  Booktitle={EA CHI '21: Extended Abstracts of the 2021 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Immigrant Families' Health-Related Information Behaviour on Instant Messaging Platforms},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2021-chi2021lbw-poretski-whatsapp.pdf},
  Year={2021},
  url={https://www.youtube.com/watch?v=jS_YljK-i0s},
}

@inproceedings{feick2021illusions,
  Abstract={Providing haptic feedback when manipulating virtual objects is an essential part of immersive virtual reality experiences; however, it is challenging to replicate all of an object’s properties and characteristics. We propose the use of visuo-haptic illusions alongside physical proxies to enhance the scope of proxy-based interactions with virtual objects. In this work, we focus on two manipulation techniques, linear translation and stretching across different distances, and investigate how much discrepancy between the physical proxy and the virtual object may be introduced without participants noticing. In a study with 24 participants, we found that manipulation technique and travel distance significantly affect the detection thresholds, and that visuo-haptic illusions impact performance and accuracy. We show that this technique can be used to enable functional proxy objects that act as stand-ins for multiple virtual objects, illustrating the technique through a showcase VR-DJ application.},
  Author={Feick, Martin and Kleer, Niko and Zenner, Andr\'e and Tang, Anthony and Kr\"uger, Antonio},
  Booktitle={CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality},
  Type={conference},
  Year={2021},
  Acceptance={26.3% - 749/2844},
  pdfurl={http://hcitang.org/papers/2021-chi2021-feick-visio-haptic-illusions.pdf},
  videourl={http://hcitang.org/papers/2021-chi2021-feick-visio-haptic-illusions.mp4}
}

@inproceedings{mahadevan2021hri,
  Abstract={In ad-hoc human-robot collaboration (HRC), humans and robots work on a task without pre-planning the robot's actions prior to execution; instead, task allocation occurs in real-time. However, prior research has largely focused on task allocations that are pre-planned - there has not been a comprehensive exploration or evaluation of techniques where task allocation is adjusted in real-time. Inspired by HCI research on territoriality and proxemics, we propose a design space of novel task allocation techniques including both explicit techniques, where the user maintains agency, and implicit techniques, where the efficiency of automation can be leveraged. The techniques were implemented and evaluated using a tabletop HRC simulation in VR. A 16-participant study, which presented variations of a collaborative block stacking task, showed that implicit techniques enable efficient task completion and task parallelization, and should be augmented with explicit mechanisms to provide users with fine-grained control.},
  Author={Mahadevan, Karthik and Sousa, Mauricio and Tang, Anthony and Grossman, Tovi},
  Booktitle={CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems},
  Title={"Grip-that-there": An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration},
  Type={conference},
  Year={2021},
  Acceptance={26.3% - 749/2844},
  Notes={Best Paper Nominee (top 5{\%} of submissions)},
  pdfurl={http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.pdf},
  videourl={http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.mp4},
  url={https://www.youtube.com/watch?v=Fvn_qjWJqwk},
}

@inproceedings{pratte2021empathy,
  Abstract={Empathy tools are experiences designed to evoke empathic responses by placing the user in another’s lived and felt experience. The problem is that designers do not have a common vocabulary to describe empathy tool experiences; consequently, it is difficult to compare/contrast empathy tools or to think about their efficacy. To address this problem, we analyzed 26 publications on empathy tools to develop a descriptive framework for designers of empathy tools. Based on our analysis, we found that empathy tools can be described along three dimensions: (i) the amount of agency the tool allows, (ii) the type of sensation that are experienced, and (iii) the user’s perspective while using the tool. We show that this framework can be used to describe a wide variety of empathy tools, and provide recommendations for empathy tool designers, as well as techniques for measuring the efficacy of an empathy tool experience.},
  Author={Pratte, Sydney and Tang, Anthony and Oehlberg, Lora},
  Booktitle={TEI 2021: International Conference on Tangible, Embedded and Embodied Interaction},
  Title={Evoking Empathy: A Framework for Describing Empathy Tools},
  Type={conference},
  Year={2021},
  Acceptance={29.9% - 30/134},
  pdfurl={http://hcitang.org/papers/2021-tei2021-pratte-empathy.pdf},
  url={https://www.youtube.com/watch?v=JBCzPt5ILxo},
}


@incollection{jones2020dt4wsar,
  Author={Jones, Brennan and Tang, Anthony and Neustaedter, Carman and Antle, Alissa N. and McLaren, Elgin-Skye},
  Abstract={Wilderness search and rescue (WSAR) is a carefully planned and organized team operation, requiring collaboration and information sharing between many volunteers who are spread out across various locations in the outdoors. Workers play a variety of roles, both on the ground and at a command post, and they need information and awareness specific to those roles. In our work, we are interested in understanding how this information is gathered and passed around, how it helps WSAR workers achieve their goals, and what challenges they face in sending and receiving information as well as in maintaining proper awareness. We conducted a study where we interviewed WSAR workers and observed a simulated search. Our findings reveal that WSAR workers face challenges in maintaining a shared mental model when radio and network connectivity are sparse. Our insights reveal opportunities for new communication modalities, such as (but not limited to) video communication, augmented reality, drones, and team-collaboration platforms to provide awareness and make communication and coordination easier remotely across various locations, but particularly between the field teams and Command workers. However, such technologies should also be designed to anticipate gaps in radio reception, and provide opportunities for workers to communicate asynchronously and see relevant ‘offline’ information in a context-dependent manner. We present design ideas that pursue some of these opportunities.},
  Title={Designing Technology for Shared Communication and Awareness in Wilderness Search and Rescue},
  Booktitle={HCI Outdoors: Theory, Design, Methods and Applications. Human–Computer Interaction Series.},
  Editor={McCrickard, D.S. and Jones, M. and Stelter, T.L.},
  Publisher={Springer},
  URL={https://doi.org/10.1007/978-3-030-45289-6_9},
  PDFUrl={http://hcitang.org/papers/2020-hcio-jones-dt4wsar.pdf},
  Year={2020},
  Pages={175-194},
  ISBN={978-3-030-45289-6},
}


@inproceedings{feick2020vrqt,
  Abstract={In this work, we present the VRQuestionnaireToolkit, which enables the research community to easily collect subjective measures within virtual reality (VR). We contribute a highly customizable and reusable open-source toolkit which can be integrated in existing VR projects rapidly. The toolkit comes with a pre-installed set of standard questionnaires such as NASA TLX, SSQ and SUS Presence questionnaire. Our system aims to lower the entry barrier to use questionnaires in VR and to significantly reduce development time and cost needed to run pre-, in between- and post-study questionnaires.},
  Author={Feick, Martin and Kleer, Niko and Tang, Anthony and Krüger, Antonio},
  Booktitle={AP UIST 2020: Adjunct Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, UIST 2019},
  Title={The Virtual Reality Questionnaire Toolkit},
  Type={poster},
  Year={2020},
  url={https://github.com/MartinFk/VRQuestionnaireToolkit},
  pdfurl={http://hcitang.org/papers/2020-uist2020wip-feick-vrqt.pdf}
  }

@inproceedings{feick2020tangi,
  Abstract={Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit, and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives, and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators, and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi’s proxies outperform traditional controllers, and were generally favored by participants.},
  Author={Feick, Martin and Bateman, Scott and Tang, Anthony and Miede, André and Marquardt, Nicolai},
  Booktitle={ISMAR 2020: 2020 IEEE International Symposium on Mixed and Augmented Reality},
  Title={TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality},
  Type={conference},
  Year={2020},
  Acceptance={28.8% - 87/302},
  pdfurl={http://hcitang.org/papers/2020-ismar2020-feick-tangi.pdf},
  videourl={http://hcitang.org/papers/2020-ismar2020-feick-tangi.mp4}
}

@inproceedings{yan2020zoomwalls,
  Abstract={We focus on the problem of simulating the haptic infrastructure of a virtual environment (i.e. walls, doors). Our approach relies on multiple ZoomWalls---autonomous robotic encounter-type haptic wall-shaped props---that coordinate to provide haptic feedback for room-scale virtual reality. Based on a user's movement through the physical space, ZoomWall props are coordinated through a predict-and-dispatch architecture to provide just-in-time haptic feedback for objects the user is about to touch. To refine our system, we conducted simulation studies of different prediction algorithms, which helped us to refine our algorithmic approach to realize the physical ZoomWall prototype. Finally, we evaluated our system through a user experience study, which showed that participants found that ZoomWalls increased their sense of presence in the VR environment. ZoomWalls represents an instance of \emph{autonomous mobile reusable props}, which we view as an important design direction for haptics in VR.},
  Author={Yixian, Yan and Takashima, Kazuki and Tang, Anthony and Tanno, Takayuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},
  Booktitle={UIST 2020: Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
  Title={ZoomWalls: Dynamic Walls that Simulate HapticInfrastructure for Room-scale VR Worlds},
  Type={conference},
  Year={2020},
  pdfurl = {http://hcitang.org/papers/2020-uist2020-yan-zoomwalls.pdf},
  videourl = {http://hcitang.org/papers/2020-uist2020-yan-zoomwalls.mp4}
  
}

@inproceedings{mok2020audiencebot,
  Abstract={We built a chatbot system--Audience Bot--that simulates an audience for novice live streamers to engage with while streaming. New live streamers on platforms like Twitch are expected to perform and talk to themselves, even while no one is watching. We ran an observational lab study on how Audience Bot assists novice live streamers as they acclimate to multitasking--simultaneously playing a video game while performing for a (simulated) audience.},
  Author={Mok, Terrance and Au Yeung, Colin Matthew and Tang, Anthony and Oehlberg, Lora},
  Booktitle={EA IMX 2020: Extended Abstracts of the ACM International Conference on Interactive Media Experiences},
  Title={Talk Like Somebody is Watching: Understanding and Supporting Novice Live Streamers},
  Type={poster},
  Year={2020},
  pdfurl = {http://hcitang.org/papers/2020-imxposter-mok-audiencebot.pdf}
}


@inproceedings{aseniero2020activityriver,
  author = {Aseniero, Bon Adriel and Perin, Charles and Willett, Wesley and Tang, Anthony and Carpendale, Sheelagh},
  title = {Activity River: Visualizing Planned and Logged Personal Activities for Reflection},
  abstract = {We present Activity River, a personal visualization tool which enables individuals to plan, log and reflect on their own self-defined activities. We are interested in supporting this type of reflective practice as prior work has shown that reflection can help people plan and manage their time effectively. Hence, we designed Activity River based on five design goals (visualize historical and contextual data, provide a comparative view, use delightful visuals, support authorship, and enable open-ended planning and logging) which we distilled from the Information Visualization and Human--Computer Interaction literature. To explore our approach’s strengths and limitations, we conducted a qualitative study of Activity River using role-playing. Through this qualitative exploration, we illustrate how our participants envisioned using our visualization approach to perform dynamic and continuous reflection on their activities. We observed they were able assess their progress towards their plans and adapt to unforeseen circumstances using our tool.},
  booktitle = {AVI 2020: Proceedings of the 2020 International Conference on Advanced Visual Interfaces},
  Year = {2020},
  pages = {10 pages},
  type = {conference},
  pdfurl = {http://hcitang.org/papers/2020-avi2020-aseniero-activity-river.pdf},
  notes = {Honourable mention},
}

@inproceedings{vinod2020targetassistance,
  author = {Asokan, Vinod and Bateman, Scott and Tang, Anthony},
  title = {Assistance for Target Selection in Mobile Augmented Reality},
  abstract = {Mobile augmented reality – where a mobile device is used to view and interact with virtual objects displayed in the real world – is becoming more common. Target selection is the main method of interaction in mobile AR, but is particularly difficult because targets in AR can have challenging characteristics such as moving or being occluded (by digital or real world objects). To address this problem, we conduct a comparative study of target assistance techniques designed for mobile AR. We compared four different cursor-based selection techniques against the standard touch-to-select interaction, finding that a newly adapted Bubble Cursorbased technique performs consistently best across a range of five target characteristics. Our work provides new findings demonstrating the promise of cursor-based target assistance in mobile AR.},
  booktitle = {GI 2020: Proceedings of the 46th Graphics Interface Conference 2020},
  Year = {2020},
  pages = {10 pages},
  type = {conference},
  pdfurl = {http://hcitang.org/papers/2020-gi2020-bateman-targeting-assistance.pdf},
  videourl = {http://hcitang.org/papers/2020-gi2020-bateman-targeting-assistance.mp4},
  presentationurl = {https://www.youtube.com/watch?v=cNYckhMfU5s},
}

@inproceedings{anjani2020mukbang,
  author = {Anjani, Laurensia and Mok, Terrance and Tang, Anthony and Oehlberg, Lora and Boon, Goh Wooi },
  title = {Why do people watch others eat? An empirical study on the motivations and practices of mukbang viewers},
  abstract = {We present a mixed-methods study of viewers on their practices and motivations around watching mukbang—video streams of people eating large quantities of food. Viewers’ experiences provide insight on future technologies for multisensorial video streams and technology-supported commensality (eating with others). We surveyed 104 viewers and interviewed 15 of them about their attitudes and reflections on their mukbang viewing habits, their physiological aspects of watching someone eat, and their perceived social relationship with mukbangers. Based on our findings, we propose design implications for remote commensality, and for synchronized multisensorial video streaming content.},
  booktitle = {CHI 2020: Proceedings of the 2020 SIGCHI Conference on Human Factors in Computing Systems},
  Year = {2020},
  pages = {1--12},
  type = {conference},
  pdfurl = {http://hcitang.org/papers/2020-chi2020-mukbang.pdf },
}

@techreport{feick2020tangi,
  Abstract={Exploring and manipulating complex virtual objects is challenging due to limitations of conventional controllers and free-hand interaction techniques. We present the TanGi toolkit which enables novices to rapidly build physical proxy objects using Composable Shape Primitives. TanGi also provides Manipulators allowing users to build objects including movable parts, making them suitable for rich object exploration and manipulation in VR. With a set of different use cases and applications we show the capabilities of the TanGi toolkit, and evaluate its use. In a study with 16 participants, we demonstrate that novices can quickly build physical proxy objects using the Composable Shape Primitives, and explore how different levels of object embodiment affect virtual object exploration. In a second study with 12 participants we evaluate TanGi's Manipulators, and investigate the effectiveness of embodied interaction. Findings from this study show that TanGi's proxies outperform traditional controllers, and were generally favored by participants.},
  Author={Feick, Martin and Bateman, Scott and Tang, Anthony and Miede, André and Marquardt, Nicolai},
  Title={TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation in Virtual Reality},
  Year={2020},
  Number={2001.03021 [cs.HC]},
  Publisher={arXiv},
  Type={techreport},
  Url={https://arxiv.org/abs/2001.03021},
}


@article{jones2020wsar,
  Abstract={Wilderness search and rescue (WSAR) requires careful communication between workers in different locations. To understand the contexts from which WSAR workers communicate and the challenges they face, we interviewed WSAR workers and observed a mock-WSAR scenario. Our findings illustrate that WSAR workers face challenges in maintaining a shared mental model.  This is primarily done through distributed communication using two-way radios and cell phones for text and photo messaging; yet both implicit and explicit communication suffer. WSAR workers send messages for various reasons and share different types of information with varying levels of urgency. This warrants the use of multiple communication modalities and information streams. However, bringing in more modalities introduces the risk of information overload, and thus WSAR workers today still primarily communicate remotely via the radio. Our work demonstrates opportunities for technology to provide implicit communication and awareness remotely, and to help teams maintain a shared mental model even when synchronous realtime communication is sparse. Furthermore, technology should be designed to bring together multiple streams of information and communication while making sure that they are presented in ways that aid WSAR workers rather than overwhelming them.},
  Author={Jones, Brennan and Tang, Anthony and Neustaedter, Carman},
  Journal={Proceedings of the ACM on Human-Computer Interaction},
  Keywords={Search and rescue, outdoors, team communication, distributed collaboration, awareness},
  Pages={28 pages},
  Pdfurl={http://hcitang.org/papers/2020-group2020-wsar.pdf},
  Title={Remote Communication in Wilderness Search and Rescue: Implications for the Design of Emergency Distributed-Collaboration Tools for Network-Sparse Environments},
  Type={journal},
  Volume={4, GROUP},
  Number={10},
  Year={2020},
  DOI={https://doi.org/10.1145/3375190},
}

@phdthesis{seyed2019dissertation,
  Abstract={From the smart assistant in a home providing the daily news, to the smart-glasses that notify you about your next meeting, Ubiquitous computing has arrived and is here to stay. However, despite our inherent dependence on ubiquitous technologies, a number of challenges still remain, such as how do we seamlessly interact with these environments using our everyday devices, to how do we provide them with context for interactions with ourselves and our data.\\
  My dissertation work is concerned with (1) overcoming challenges in how Ubiquitous computing are designed and how we interact with them using our everyday devices, (2) if and how we can redesign these devices to better fit their context of use in these environments, and (3) how can we enable designers and novices to contribute to the field of uUbiquitous computing environments.\\
  Moving beyond the research work for this dissertation, I also provide entrepreneurial re- flections in each of the aforementioned areas, where I describe my journey and key lessons learned from working in a startup to co-founding multiple startups.},
  Author={Seyed, Teddy},
  Month={September},
  Pdfurl={http://hcitang.org/papers/2019-phddissertation-seyed.pdf},
  School={University of Calgary},
  Title={Entrepreneurial Thinking In The Design Of Ubiquitous Computing},
  Type={thesis},
  Year={2019}
}

@mastersthesis{cavalcanti2019thesis,
  author       = {Cavalcanti, Flavia}, 
  title        = {Flail: A Domain Specific Language for Drone Path Generation},
  school       = {University of Calgary},
  year         = {2019},
  month        = {9},
  abstract = { The main objective of this thesis was to design a domain specific language that would allow users to easily describe flight trajectories for drones. Conventional drone control schemes rely on handheld controllers and, sometimes, on model specific applications that allow users to pre-plan paths (e.g., FreeFlight Pro). The issue with these is that handheld controllers display a large learning curve for new users and flight plan applications rely on waypoint systems, which limits the complexity of the flight plan. Flail is an alternate control scheme for drones that is capable of programmatically pre-specifying complex flight patterns. Addi- tionally, an HTC Vive was used to simplify Flail code generation by allowing users to use the Vive wand to draw out flight trajectories in 3D space. The viability of Flail was examined through RC drone flight tests and simulations.},
  Pdfurl={http://hcitang.org/papers/2019-mscthesis-cavalcanti.pdf},
  Type={thesis},
}


@article{ens2019groupware,
  title = {Revisiting Collaboration through Mixed Reality: The Evolution of Groupware},
  journal = {International Journal of Human-Computer Studies},
  year = {2019},
  issn = {1071-5819},
  volume = {131C},
  month = {November},
  pages = {81--98},
  doi = {https://doi.org/10.1016/j.ijhcs.2019.05.011},
  url = {http://www.sciencedirect.com/science/article/pii/S1071581919300606},
  author = {Ens, Barrett and Lanir, Joel and Tang, Anthony and Bateman, Scott and Lee, Gun and Piumsomboon, Thammaathip and Billinghurst, Mark},
  keywords = {Collaborative Mixed Reality, Mixed Reality, Augmented Reality, Computer Supported Cooperative Work, Collaborative Technology},
  abstract = {Collaborative Mixed Reality (MR) systems are at a critical point in time as they are soon to become more commonplace. However, MR technology has only recently matured to the point where researchers can focus deeply on the nuances of supporting collaboration, rather than needing to focus on creating the enabling technology. In parallel, but largely independently, the field of Computer Supported Cooperative Work (CSCW) has focused on the fundamental concerns that underlie human communication and collaboration over the past 30-plus years. Since MR research is now on the brink of moving into the real world, we reflect on three decades of collaborative MR research and try to reconcile it with existing theory from CSCW, to help position MR researchers to pursue fruitful directions for their work. To do this, we review the history of collaborative MR systems, investigating how the common taxonomies and frameworks in CSCW and MR research can be applied to existing work on collaborative MR systems, exploring where they have fallen behind, and look for new ways to describe current trends. Through identifying emergent trends, we suggest future directions for MR, and also find where CSCW researchers can explore new theory that more fully represents the future of working, playing and being with others.},
  PDFUrl = {http://hcitang.org/papers/2019-ijhcs-revisiting-collaboration.pdf}
}

@inproceedings{seyed2019mannequette,
  author = {Seyed, Teddy and Tang, Anthony},
  title = {Mannequette: Understanding and Enabling Collaboration and Creativity on Avant-Garde Fashion-Tech Runways},
  abstract = {Drawing upon multiple disciplines, avant-garde fashion-tech teams push the boundaries between fashion and technology. Many are well trained in envisioning aesthetic qualities of garments, but few have formal training on designing and fabricating technologies themselves. We introduce Mannequette, a prototyping tool for fashion-tech garments that enables teams to experiment with interactive technologies at early stages of their design processes. Mannequette provides an abstraction of light-based outputs and sensor-based inputs for garments through a DJ mixer-like interface that allows for dynamic changes and recording/playback of visual effects. The base of Mannequette can also be incorporated into the final garment, where it is then connected to the final components. We conducted an 8-week deployment study with eight design teams who created new garments for a runway show. Our results revealed Mannequette allowed teams to repeatedly consider new design and technical options early in their creative processes, and to communicate more effectively across disciplinary backgrounds.},
  booktitle = {DIS 2019: Conference on Designing Interactive Systems 2019},
  Acceptance = {25% - 100/400},
  Year = {2019},
  pages = {10 pages},
  type = {conference},
  notes = {Honourable mention (top 2% of all submissions)},
  pdfurl = {http://hcitang.org/papers/2019-dis2019-mannequette.pdf },
  videourl = {http://hcitang.org/papers/2019-dis2019-mannequette.mp4 }
}

@inproceedings{jones2019remotecollabinsar,
  author = {Jones, Brennan and Tang, Anthony and Neustaedter, Carman},
  title = {Drones for Remote Collaboration in Wilderness Search and Rescue},
  abstract = {Wilderness search and rescue (SAR) is an activity that could potentially be well supported by drones, both as search tools and as devices to help with collaboration between remote helpers and workers on the ground. However, even with this potential, there are still usability challenges that need to be addressed. In our work, we are exploring potential use cases for drones to support wilderness SAR, as well as design solutions for wilderness-SAR drone systems. We discuss these explorations in this position paper, as well as some of our ideas and plans moving forward.},
  booktitle = {iHDI - International workshop on Human-Drone Interaction - Workshop at CHI 2019},
  pdfurl = {http://hcitang.org/papers/2019-chi2019workshop-remote-collab-in-sar.pdf },
  Year = {2019},
  pages = {6 pages},
  type = {workshop},
  url = {http://hdi.famnit.upr.si/}
}


@inproceedings{tolley2019windywall,
  author = {Tolley, David and Nguyen, Thi Ngoc Tram and Tang, Anthony and Ranasinghe, Nimesha and Kawauchi, Kensaku and Yen, Ching-Chiuan},
  title = {WindyWall: Exploring Creative Wind Simulations},
  abstract = {Wind simulations are typically one-off implementations for specific applications. We introduce WindyWall, a platform for creative design and exploration of wind simulations. WindyWall is a three-panel 90-fan array that encapsulates users with 270° of wind coverage. We describe the design and implementation of the array panels, discussing how the panels can be re-arranged, where various wind simulations can be realized as simple effects. To understand how people perceive “wind” generated from WindyWall, we conducted a pilot study of wind magnitude perception using different wind activation patterns from WindyWall. Our findings suggest that: horizontal wind activations are perceived more readily than vertical ones, and that people’s perceptions of wind are highly variable—most individuals will rate airflow differently in subsequent exposures. Based on our findings, we discuss the importance of developing a method for characterizing wind simulations, and provide design directions for others using fan arrays to simulate wind.},
  booktitle = {TEI 2019: International Conference on Tangible and Embedded Interaction 2019},
  videourl = {http://hcitang.org/papers/2019-tei2019-windywall.mp4},
  pdfurl = {http://hcitang.org/papers/2019-tei2019-windywall.pdf},
  Acceptance = {34% - 37/110},
  Year = {2019},
  pages = {635-644},
  type = {conference}
}

@inproceedings{ichikawa2018safaripark,
  author    = {Ichikawa, Shotaro and
               Takashima, Kazuki and
               Tang, Anthony and
               Kitamura, Yoshifumi},
  title     = {{VR} Safari Park: a Concept-Based World Building Interface using Blocks
               and World Tree},
  booktitle = {VRST 2018: Proceedings of the 24th {ACM} Symposium on Virtual Reality Software
               and Technology},
  pages     = {6:1--6:5},
  year      = {2018},
  type = {conference},
  doi       = {https://doi.org/10.1145/3281505.3281517},
  abstract = {We present a concept-based world building approach, realized in a system called VR Safari Park, which allows users to rapidly create and manipulate a world simulation. Conventional world building tools focus on the manipulation and arrangement of entities to set up the simulation, which is time consuming as it requires frequent view and entity manipulations. Our approach focuses on a far simpler mechanic, where users add virtual blocks which represent world entities (e.g. animals, terrain, weather, etc.) to a World Tree, which represents the simulation. In so doing, the World Tree provides a quick overview of the simulation, and users can easily set up scenarios in the simulation without having to manually perform fine-grain manipulations on world entities. A preliminary user study found that the proposed interface is effective and usable for novice users without prior immersive VR experience.},
  pdfurl = {http://hcitang.org/papers/2018-vrst2018-vr-safari-park.pdf},
  videourl = {http://hcitang.org/papers/2018-vrst2018-vr-safari-park.mp4},
  Acceptance = {32/145 - 22%},
  type={conference}
}

@inproceedings{aseniero2018visathome,
  Abstract={We conducted a qualitative study pilot to gather requirements for integrating health visualizations at home. We focused on finding dynamics between (1) people: who the visualizations are made for, and others who will see them; (2) visualization: what visual repre- sentations people expect for their homes; and (3) location: where people expect to find the visualizations. We describe our study methodology and the results. We found hints of concepts for con- sideration such as (1) privacy can be derived from visualization style, (2) visualizations can be installed at high traffic locations but data sensitivity should be considered, and (3) location of related tools at home can influence where visualizations should be located.},
  Author={Aseniero, Bon Adriel and Tang, Anthony and Carpendale, Sheelagh},
  Booktitle={Poster Program of IEEE VIS 2018},
  Title={Health Visualizations at Home: Who Sees What Where},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2018-vis2018posters-health-vis-at-home.pdf},
  videourl={http://hcitang.org/papers/2018-vis2018posters-health-vis-at-home.mp4},
  Year={2018},
  Pages={2},
}

@inproceedings{onishi2018livingwall,
 author = {Onishi, Yuki and Kudo, Yoshiki and Takashima, Kazuki and Tang, Anthony and Kitamura, Yoshifumi},
 title = {The Living Wall Display: Physical Augmentation of Interactive Content Using an Autonomous Mobile Display},
 booktitle = {SIGGRAPH Asia 2018 Emerging Technologies},
 series = {SA '18},
 year = {2018},
 location = {Tokyo, Japan},
 pages = {15:1--15:2},
 articleno = {15},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3275476.3275489},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {VR, multi-modal display, robotic display},
 pdfurl = {http://hcitang.org/papers/2018-sa2018etech-livingwall.pdf},
 videourl = {http://hcitang.org/papers/2018-sa2018etech-livingwall.mp4},
 abstract = {The Living Wall Display displays interactive content on a mobile wall screen that moves in concert with content animation. To aug- ment the interaction experience, the display dynamically changes its position and orientation, responding to the content animation triggered by user interactions. We implement three proof of concept prototypes that represent pseudo force impact of the interactive content using physical screen movement. Pilot studies show that the Living Wall augments content expressiveness, and increases the sense of presence of the screen content.},
 type={conference}
} 

@mastersthesis{feick2017thesis,
  author       = {Feick, Martin}, 
  title        = {Implementing a Humanoid Tele-Robotic Prototype for Investigating Issues in Remote Collaboration},
  school       = {University of Applied Sciences, Saarland},
  year         = {2017},
  month        = {9},
  abstract = { We designed and developed a novel system, ReMa (Remote Manipulator), for supporting remote collaboration on physical tasks through a physical telepresence humanoid robot. The system captures and reproduces object manipulations on a proxy object at a remote location.

The prototype combines latest robotics and motion capture technologies, exploring their capabilities and limitations. We found that directly mapping human and robot action is problematic due to the arrangement and limits of the robot joints.

We applied ReMa to investigate how limited perspective in current video-mediated systems affects remote collaboration. We also explored the impact of a physical proxy manipulated by a remote person. We conducted two user studies and contrasted the results with recent research systems designed for remote collaboration.

Our main findings are: (1) a shared perspective is more effective and preferred compared to the opposing perspective offered by con- ventional video chat systems, and (2) the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions. These research findings validate both the design and implementation of ReMa as an effective research platform.},
  Pdfurl={http://hcitang.org/papers/2017-bscthesis-feick.pdf},
  Type={thesis},
}


@inproceedings{jones2018wildernesssar,
 abstract={Wilderness search and rescue (SAR) is a critical operation that requires careful team collaboration. Even with current technologies, SAR workers still face communication difficulties when working outside in challenging conditions. The goal of my thesis work is to understand how communication and collaboration interfaces can be designed to better support wilderness-SAR teams while distributed in an outdoor environment with challenging terrain and conditions.},
 author = {Jones, Brennan},
 title = {Designing for Distributed Collaboration in Wilderness Search and Rescue},
 booktitle = {CSCW '18 Companion: Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing},
 year = {2018},
 pages = {77--80},
 doi = {https://doi.org/10.1145/3272973.3272978},
 keywords = {distributed collaboration, outdoors, search and rescue, situation awareness, team communication},
 pdfurl={http://hcitang.org/papers/2018-cscw2018dc-jones-distributed-collaboration-in-wilderness-sar.pdf},
 notes = {doctoral consortium contribution},
 type={poster}
}

@inproceedings{jones2018managerawarenessinsar,
  Abstract={We present a tangible interface for supporting wilderness search-and-rescue (SAR) managers in maintaining awareness of a large SAR incident, where there are numerous field teams searching for a lost person in a wilderness area. This interface consists of physical and digital representations of the search area and elements of the search activity (e.g., the locations of search teams, weather information, and clues from the field). It is intended to allow SAR managers to inspect information about the response and search area from different perspectives and aid them in planning by allowing them to physically manipulate the representations and explore the data through touch.},
  Author={Jones, Brennan and Tang, Anthony and Neustaedter, Carman and Antle, Alissa Nicole and McLaren, Elgin-Skye},
  Booktitle={CSCW '18 Companion: Companion of the 2018 ACM Conference on Computer Supported Cooperative Work and Social Computing},
  Title={Designing a Tangible Interface for Manager Awareness in Wilderness Search and Rescue},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2018-cscw2018posters-tangible-sar-manager-poster.pdf},
  Year={2018},
  Pages={161--164},
  Doi={https://doi.org/10.1145/3272973.3274045}
}

@inproceedings{feick2018objectfocusedremotecollaboration,
  Abstract={In this paper we outline the design of a mixed-reality system to support object-focused remote collaboration. Here, being able to adjust collaborators’ perspectives on the object as well as understand one another’s perspective is essential to support effective collaboration over distance. We propose a low-cost mixed-reality system that allows users to: (1) quickly align and understand each other’s perspective; (2) explore objects independently from one another, and (3) render gestures in the remote’s workspace. In this work, we focus on the expert’s role and we introduce an interaction technique allowing users to quickly manipulation 3D virtual objects in space.},
  Author={Feick, Martin and Tang, Anthony and Bateman, Scott},
  Booktitle={UIST '18 Adjunct: Adjunct Proceedings of  Symposium on User Interface Technology},
  Title={Mixed-Reality for Object-Focused Remote Collaboration},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2018-uist2018posters-mr-for-object-focused-remote-collaboration.pdf},
  Year={2018},
  Pages={63--65},
  Doi={http://doi.acm.org/10.1145/3266037.3266102},
  Notes={2 pages + poster},
}

@mastersthesis{dillman2018thesis,
  author       = {Dillman, Kody}, 
  title        = {A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality},
  school       = {University of Calgary},
  year         = {2018},
  month        = {9},
  abstract = { Based on an analysis of 49 popular contemporary video games, I develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. I demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, I show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences and use the framework as a guide to create three novel AR prototypes. Finally, I suggest design considerations for the creation of visual interaction cues for augmented reality and examine how the framework might be employed in future work.},
  Pdfurl={http://hcitang.org/papers/2018-mscthesis-dillman.pdf},
  Type={thesis},
}

@inproceedings{pham2018hologestures,
  Abstract={Because gesture design for augmented reality (AR) remains idiosyncratic, people cannot necessarily use gestures learned in one application in another. To design discoverable gestures, we need to understand what gestures people expect to use. We explore how the scale of AR affects the gestures people expect to use to interact with 3D holograms. Using an elicitation study, we asked participants to generate gestures in response to holographic task referents, where we varied the scale of holograms from desktop-scale to room-scale objects. We found that the scale of objects and scenes in the AR experience moderates the generated gestures. Most gestures were informed by physical interaction, and when people interacted from a distance, they sought a good perspective on the target object before and during the interaction. These results suggest that gesture designers need to account for scale, and should not simply reuse gestures across different hologram sizes.},
  Author={Pham, Tran and Vermeulen, Jo and Tang, Anthony and MacDonald, Lindsay},
  Booktitle={DIS 2018: Proceedings of the 2018 Conference on Designing Interactive Systems},
  Title={Scale Impacts Elicited Gestures for Manipulating Holograms: Implications for AR Gesture Design},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2018-dis2018-hologestures.pdf},
  Url={http://hcitang.org/papers/2018-dis2018-hologestures-referents-supplement.pdf},
  Doi={https://doi.org/10.1145/3196709.3196719},
  Year={2018},
  Pages={227--240},
  Acceptance={23% - 92/405},
  Notes={Includes supplemental material illustrating the referents used in the study.}
}

@inproceedings{stark2018rbiforolderadults,
  Abstract={Serious games on mobile and desktop devices often target older adults for the development and maintenance of cognitive skills; however, there is little evidence that these games are effective at transferring those skills to real life [9]. We are interested in using reality-based interaction to investigate whether tangible user interfaces and augmented reality can improve the transferability of divided attention skills from electronic devices to real life while also providing opportunities for engaging entertainment. We plan to design an interface for older adults who wish to play cognitive development games that are both dynamic and physical.},
  Author={Stark, Jessi and Tang, Anthony and Sharlin, Ehud},
  Booktitle={Designing Interactions for the Ageing Populations - Workshop at CHI 2018},
  Editor={Sayan Sarcar and Cosmin Munteanu and Jussi Jokinen and Antti Oulasvirta and Neil Charness and Mark Dunlop and Xiangshi Ren},
  Pdfurl={http://hcitang.org/papers/2018-chi2018workshop-rbi-gaming-experience-for-older-adults.pdf},
  Title={Toward A Reality-Based Interaction Gaming Experience for Older Adults},
  Type={workshop},
  Url={http://olderadults.mobi/},
  Year={2018},
}

@inproceedings{jones2018seriouscollaboration,
  Abstract={We are seeing increasingly widespread usage of remote communication, and in particular video communication, for outdoor activities such as tours, shopping, and searching large environments. However, current technologies still do not provide sufficient awareness to remote communicators or sufficient means for them to indicate their intents or contribute to collaborative activities meaningfully. We describe some of the work that we have done in the past to study the challenges that people face in remote communication in the outdoors, and to design technology solutions that aim to address those challenges. We also describe our current work to address such challenges in the domain of wilderness search and rescue (SAR).},
  Author={Jones, Brennan and Tang, Anthony and Neustaedter, Carman},
  Booktitle={HCI Outdoors: Understanding Human-Computer Interaction in the Outdoors - Workshop at CHI 2018},
  Editor={Michael Jones and Zann Anderson and Joanna Hakkila and Keith Cheverst and Florian Daiber},
  Pdfurl={http://hcitang.org/papers/2018-chi2018workshop-serious-collaborative-activities.pdf},
  Title={Designing Outdoor Remote-Communication Tools for Serious Collaborative Activities},
  Type={workshop},
  Url={http://hcioutdoors.net/},
  Year={2018},
}

@inproceedings{dillman2018visualinteractioncue,
  Abstract={Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.  },
  Author={Dillman, Kody and Mok, Terrance and Tang, Anthony and Oehlberg, Lora and Mitchell, Alex},
  Booktitle={CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems},
  Title={A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2018-chi2018-visual-interaction-cues.pdf},
  Url={http://hcitang.org/papers/2018-chi2018-visual-interaction-cues-game-examples-supplement.zip},
  Doi={http://doi.acm.org/10.1145/3173574.3173714},
  VideoUrl={https://www.youtube.com/watch?v=3FoZStToALQ},
  Year={2018},
  Pages={Paper 140},
  Acceptance={25.7% - 667/2595},
  Notes={10 pages; Includes raw supplemental material of the game examples described in the paper}
}

@inproceedings{feick2018rema,
  Abstract={Remote collaborators working together on physical objects have difficulty building a shared understanding of what each person is talking about. Conventional video chat systems are insufficient for many situations because they present a single view of the object in a flattened image. To understand how this limited perspective affects collaboration, we designed the Remote Manipulator (ReMa), which can reproduce orientation manipulations on a proxy object at a remote site. We conducted two studies with ReMa, with two main findings. First, a shared perspective is more effective and preferred compared to the opposing perspective offered by conventional video chat systems. Second, the physical proxy and video chat complement one another in a combined system: people used the physical proxy to understand objects, and used video chat to perform gestures and confirm remote actions.},
  Author={Feick, Martin and Mok, Terrance and Tang, Anthony and Oehlberg, Lora and Sharlin, Ehud},
  Booktitle={CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Perspective on and Re-Orientation of Physical Proxies in Object-Focused Remote Collaboration},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2018-chi2018-rema.pdf},
  Videourl={http://hcitang.org/papers/2018-chi2018-rema.mp4},
  Url={http://chi2018.acm.org},
  Doi={http://doi.acm.org/10.1145/3173574.3173855},
  Year={2018},
  Pages={Paper 281},
  Acceptance={25.7% - 667/2595},
  Notes={Honourable Mention Award - Top 5% of all submissions; 10 pages},
}

@inproceedings{heshmat2018beam,
  Abstract={People often enjoy sharing outdoor activities together such as walking and hiking. However, when family and friends are separated by distance it can be difficult if not impossible to share such activities. We explore this design space by investigating the benefits and challenges of using a telepresence robot to support outdoor leisure activities. In our study, participants participated in the outdoor activity of geocaching where one person geocached with the help of a remote partner via a telepresence robot. We compared a wide field of view (WFOV) camera to a 360° camera. Results show the benefits of having a physical embodiment and a sense of immersion with the 360° view. Yet challenges related to a lack of environmental awareness, safety issues, and privacy concerns resulting from bystander interactions. These findings illustrate the need to design telepresence robots with the environment and public in mind to provide an enhanced sensory experience while balancing safety and privacy issues resulting from being amongst the general public.},
  Author={Heshmat, Yasamin and Jones, Brennan and Xiong, Xiaoxuan and Neustaedter, Carman and Tang, Anthony and Riecke, Bernhard E. and Yang, Lillian},
  Booktitle={CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2018-chi2018-geocaching-with-a-beam.pdf},
  Url={http://chi2018.acm.org},
  Doi={http://doi.acm.org/10.1145/3173574.3173933},
  Year={2018},
  Pages={Paper 359},
  Acceptance={25.7% - 667/2595},
  Notes={10 pages}
}

@inproceedings{wuertz2018awareness,
  Abstract={In the physical world, teammates develop situation awareness about each other’s location, status, and actions through cues such as gaze direction and ambient noise. To support situation awareness, distributed multiplayer games provide awareness cues—information that games automatically make available to players to support cooperative gameplay. The design of awareness cues can be extremely complex, impacting how players experience games and work with teammates. Despite the importance of awareness cues, designers have little beyond experiential knowledge to guide their design. In this work, we describe a design framework for awareness cues, providing insight into what information they provide, how they commu- nicate this information, and how design choices can impact play experience. Our research, based on a grounded theory analysis of current games, is the first to provide a characteriza- tion of awareness cues, providing a palette for game designers to improve design practice and a starting point for deeper research into collaborative play.},
  Author={Wuertz, Jason and Alharthi, Sultan A. and Hamilton, William A. and Bateman, Scott and Gutwin, Carl and Tang, Anthony and Toups, Zachary O. and Hammer, Jessica},
  Booktitle={CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems},
  Title={A Design Framework for Awareness Cues in Distributed Multiplayer Games},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2018-chi2018-awarenses-cues.pdf},
  Url={http://chi2018.acm.org},
  Doi={http://doi.acm.org/10.1145/3173574.3173817},
  Year={2018},
  Pages={Paper 243},
  Acceptance={25.7% - 667/2595},
  Notes={10 pages}
}

@inproceedings{feick2018wayyoumove,
  Abstract={In this paper, we discuss the role of the movement trajectory and velocity enabled by our tele-robotic system (ReMa) for remote collaboration on physical tasks. Our system reproduces changes in object orientation and position at a remote location using a humanoid robotic arm. However, even minor kinematics differences between robot and human arm can result in awkward or exaggerated robot movements. As a result, user communication with the robotic system can become less efficient, less fluent and more time intensive.},
  Author={Feick, Martin and Oehlberg, Lora and Tang, Anthony and Miede, Andre and Sharlin, Ehud},
  Booktitle={HRI '18: Extended Abstracts of the International Conference on Human-Robot Interaction},
  Title={The Way You Move: The Effect of a Robot Surrogate Movement in Remote Collaboration},
  Type={poster},
  Pdfurl={http://hcitang.org/papers/2018-hri2018lbw-way-you-move.pdf},
  Url={http://hcitang.org/papers/2018-hri2018lbw-way-you-move-poster.pdf},
  Year={2018},
  Pages={2 pages + poster},
}

@inproceedings{shakeri2017distributedescaperooms,
  Abstract={In real-life escape rooms, players try to escape a locked room by solving a series of puzzles. Currently, escape rooms involve collocated collaboration; however, there is potential for them to be distributed. We explored the design of a distributed escape room that connected two distance-separated rooms through audio/video links and shared artifacts. We evaluated it with pairs of participants to explore the design factors that affected player experiences. Results show that an audio connection created feelings of social presence. Video links augmented this connection to help players share knowledge and artifacts, however, showing less over the video feed created curiosity. Players expected a parallel setup between artifacts and puzzles in the rooms, despite the rooms being designed to vary in similarity and how closely players needed to collaborate. These results suggest directions for the design of audio connections to represent remote players, video feeds for sharing artifacts and promoting curiosity, and the use of both similar and dissimilar artifacts as a part of puzzles.},
  Author={Shakeri, Hanieh and Singhal, Samarth and Pan, Rui and Neustaedter, Carman and Tang, Anthony},
  Booktitle={CHI PLAY '17: Proceedings of the 2017 Annual Symposium on Computer-Human Interaction in Play},
  Title={Escaping Together: The Design and Evaluation of a Distributed Real-Life Escape Room},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2017-chiplay2017-distributed-escape-rooms.pdf},
  Url={https://chiplay.acm.org/2017/},
  Doi={},
  Year={2017},
  Pages={14 pages},
  Acceptance={25.2% - 45/178},
}


@inproceedings{ore2017goodreads,
  Abstract={This project focuses on the question of how to design a visualization that reveals underlying relationships between books (i.e. texts) based on user-generated tags as they relate to these books. The core problem the project addresses is that main mechanisms to classify texts--either based on literary classifications or text-analytics-based classifications are either too high-level, or too low-level. Rather, a perhaps more valuable way to explore relationships between texts is to use reader-generated tags. Here, we make use of data scraped from GoodReads, a social media site where the principal artefact under discussion is a book, and accumulated book reviews. In particular, we visualize the "bookshelves" each book is classified with (these "bookshelves" are user-generated tags), and we present two sets of visualizations that allow prospective readers with the ability to compare the thematic differences between different books.},
  Author={Arowobusoye, Oreoluwa and Huynh, Tina Thanh and Tang, Anthony},
  Booktitle={Poster Proceedings of Graphics Interface 2017},
  Pdfurl={http://hcitang.org/papers/2017-gi2017posters-visualizinggoodreads.pdf},
  Url={http://hcitang.org/papers/2017-gi2017posters-visualizinggoodreads-poster.pdf},
  Title={Visualizing Reader-Created Tags for Books on GoodReads},
  Type={poster},
  Year={2017},
  Notes={2-page abstract + poster},
}

@inproceedings{pham2017hololens,
  Abstract={Advancements in augmented reality hardware help are helping to make the idea of “physically” interacting with 3D volumentric scans into a reality. In this poster, we explore how medical practitioners and technicians can explore and study 3D volumentric scans with head-mounted augmented reality technologies. We explore specifically how to design gestures for interacting with volumetric data given head-mounted AR. To inform this exploration, we designed and conducted a preliminary elicitation study, where eight participants created gestures to target resulting changes in the scene based on their expectation of how a working system would behave. Based on these gestures, we discuss how future gesture designers for head-mounted AR tools should explore interaction with 3D volumetric data.},
  Author={Pham, Tran and Tang, Anthony},
  Booktitle={Poster Proceedings of Graphics Interface 2017},
  Pdfurl={http://hcitang.org/papers/2017-gi2017posters-hololensgestures.pdf},
  Url={http://hcitang.org/papers/2017-gi2017posters-hololensgestures-poster.pdf},
  Title={User-Defined Gestures for Holographic Medical Analytics},
  Type={poster},
  Year={2017},
  Notes={2-page abstract + poster},
}

@inproceedings{kinzel2017smartwatch,
  Abstract={Lacking the large screen size of mobile phones, smartwatch interaction faces challenges with regards to finger occlusion and dense hard to target controls that arise from their extremely tiny screens and input surfaces. Attempts to reconcile this problem often burden the user with bulky, expensive, extra hardware and lack simplicity and accuracy. Our approach uses a set of pinching gestures between the thumb and individual fingers on the smartwatch hand to provide a simple, socially acceptable, and comfortable input method. Our implementation makes use of common onboard sensors and machine learning to process the noisy, complex sensor data. Our method can be used to augment and extend traditional touch screen input on smartwatch devices. We evaluate our approach by measuring classification accuracy in a small user study with two different scenarios one where the user is stationary and one involving movement. Our results indicate that the approach is a feasible extension to touchscreen interaction but further work is needed to increase classification during movement.},
  Author={Kinzel, Christopher and Tang, Anthony},
  Booktitle={Poster Proceedings of Graphics Interface 2017},
  Pdfurl={http://hcitang.org/papers/2017-gi2017posters-gesturesmartwatch.pdf},
  Url={http://hcitang.org/papers/2017-gi2017posters-gesturesmartwatch-poster.pdf},
  Title={The Feasibility of Recognizing Pinch Gestures with Commodity Smartwatch Hardware and Machine Learning},
  Type={poster},
  Year={2017},
  Notes={2-page abstract + poster},
}

@inproceedings{tang2017360videochat,
  Abstract={We designed a videochat experience where one participant can experience a remote environment from a 360° camera. This allows the remote user to view and explore the environment without necessitating interaction from the local participant. We designed and conducted an observational study to understand the experience, and the challenges that people might encounter. In a study with 32 participants (16 pairs), we found that remote participants could actively participate in the experience with the environment in ways that are not possible with current mobile video chat. However, we also found that participants had challenges in communicating location and orientation information because many of common communication resources we rely on in collocated chat are not available. Based on these findings, we discuss how future mobile video chat systems need to balance immersion with interaction ease.},
  Author={Tang, Anthony and Fakourfar, Omid and Neustaedter, Carman and Bateman, Scott},
  Booktitle={DIS 2017: Conference on Designing Interactive Systems 2017 },
  Title={Collaboration in 360° Videochat: Challenges and Opportunities},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2017-dis2017-360videochat.pdf},
  Videourl={},
  Url={http://dis2017.org/},
  Doi={http://dx.doi.org/10.1145/3064663.3064707},
  Year={2017},
  Pages={1327--1339},
  Acceptance={24% - 110/458},
  Notes={Appendix material: http://dspace.ucalgary.ca/handle/1880/51950},
}

@techreport{fakourfar2016sciencerap,
  Abstract={Urban youth of color from low socioeconomic status are generally known to be less engaged in STEM in the United States. On the other hand, the same demographic is usually engaged in hip-hop culture and rap music. Emdin et. al. (2016) have introduced a 12-week teaching model through a hip-hop based science programme which encourages students to come up with hip-hop songs by connecting their everyday life to scientific concepts. This method has shown considerable promise: students have used it mainly as a way of disclosing their emotion while learning scientific concepts at the same time. However, the length of this programme could dissuade teachers from adopting this method. In this work, we introduce a software tool to facilitate the same process and achieve many of its outcomes all within a single instructional period, i.e., an hour.},
  Author={Fakourfar, Omid and Tse, Edward and Boyle, Michael and Tang, Anthony},
  Title={A Software Tool to Greately Reduce the Instructional Time Needed to Implement the Science Genius Rap Programme},
  Year={2016},
  Number={2016-1093-12},
  Publisher={Department of Computer Science, University of Calgary},
  Type={techreport},
  Url={http://hdl.handle.net/1880/51775},
}

@inproceedings{tang2017watching360together,
  Abstract={360° videos are made using omnidirectional cameras that capture a sphere around the camera. Viewers get an immersive experience by freely changing their field of view around the sphere. The problem is that current interfaces are designed for a single user, and we do not know what challenges groups of people will have when viewing these videos together. We report on the findings of a study where 16 pairs of participants watched 360° videos together in a “guided tour” scenario. Our findings indicate that while participants enjoyed the ability to view the scene independently, this caused challenges establishing joint references, leading to breakdowns in conversation. We conclude by discussing how gaze awareness widgets and gesturing mechanisms may support smoother collaborative interaction around collaborative viewing of 360° videos.},
  Author={Tang, Anthony and Fakourfar, Omid},
  Booktitle={CHI 2017: Proceedings of the 2017 SIGCHI Conference on Human Factors in Computing Systems },
  Title={Watching 360° Videos Together},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2017-chi2017-watching360video.pdf},
  Videourl={http://hcitang.org/papers/2017-chi2017-watching360video.mp4},
  Notes={Presentation - https://www.youtube.com/watch?v=OPc4mBD7pgw},
  Url={http://chi2017.acm.org},
  Doi={http://dx.doi.org/10.1145/3025453.3025519},
  Year={2017},
  Pages={4501--4506},
  Acceptance={25% - 600/2400},
}

@inproceedings{wuertz2017dota2,
  Abstract={In many multiplayer online games, success often relies upon tight coordination between teammates. In Dota 2, a multiplayer online battle arena (MOBA), two gesturing tools are provided to facilitate communication and coordination: annotations – freely drawn lines on top of the gamespace – and pings – a combination of animation and sound indicating a point of interest. While Dota 2 players make frequent use of these tools, there is little information about how and why people use them in an effort to win games. To gather this information, we performed two complimentary studies: an interaction analysis of game replays, and a survey of experienced players. Our findings include: six distinct motivations for the use of gesturing tools; when and how frequently gesture motivations occur during games; and, that players find pings an essential tool for winning. Our findings provide new directions for the design of gesturing and communication tools in collaborative games.},
  Author={Wuertz, Jason and Bateman, Scott and Tang, Anthony},
  Booktitle={CHI 2017: Proceedings of the 2017 SIGCHI Conference on Human Factors in Computing Systems },
  Title={Why Players use Pings and Annotations in Dota 2},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2017-chi2017-dota2.pdf},
  Videourl={},
  Url={http://chi2017.acm.org},
  Doi={http://dx.doi.org/10.1145/3025453.3025967},
  Year={2017},
  Pages={1978--2018},
  Acceptance={25% - 600/2400},
}

@mastersthesis{jones2016thesis,
  author       = {Jones, Brennan}, 
  title        = {Elevating Communication, Collaboration, and Shared Experiences between Peers in Mobile Video Communication using Drones},
  school       = {University of Calgary},
  year         = {2016},
  month        = {12},
  abstract = {
People are increasingly using mobile video conferencing (e.g., Skype, FaceTime, Hangouts) to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this thesis, I study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user (e.g., who is at home or in an office) who can explore the environment from the drone’s perspective. I describe findings from a study where pairs collaborated to complete shared navigation and search tasks. I illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, I articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing, where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.
},
  Pdfurl={http://hcitang.org/papers/2016-mscthesis-jones.pdf},
  Type={thesis},
}

@mastersthesis{fakourfar2016thesis,
  author       = {Fakourfar, Omid}, 
  title        = {Stabilized Annotations for Mobile Remote Assistance},
  school       = {University of Calgary},
  year         = {2016},
  month        = {12},
  abstract = {
  Recent mobile technology has provided new opportunities for creating remote assistance systems. However, mobile support systems present a particular challenge: both the camera and display are held by the user, leading to shaky video. When pointing or drawing annotations, this means that the desired target often moves, causing the gesture to lose its intended meaning. To address this problem, this thesis investigates an annotation stabilization technique, which allows annotations to stick to their intended location. I studied two different forms of annotation systems, with both tablets and head-mounted displays. To differentiate my work from the prior research, I considered a number of task factors that might influence system performance in remote assistance scenarios. My analysis suggests that stabilized annotations and head-mounted displays are only beneficial in certain situations. I conclude with reflections on system limitations and potential future work.
},
  Pdfurl={http://hcitang.org/papers/2016-mscthesis-fakourfar.pdf},
  Type={thesis},
}

@mastersthesis{alizadeh2016thesis,
  author       = {Talarposhti, Hesam Alizadeh }, 
  title        = {HappyFeet: Embodiments for Joint Remote Dancing},
  school       = {University of Calgary},
  year         = {2016},
  month        = {09},
  abstract = {
  Prior research has demonstrated that exercise is more fun and engaging when we exercise with others. Yet for many people, it is challenging to exercise with partners that are co-present due to several reasons (e.g. lack of access to a partner). In this thesis, I explore the challenge of designing an exercise system that effectively embodies a remote participant. The result of my exploration is HappyFeet, a dancing system that supports the dancing experience for remotely located partners. HappyFeet uses 3D representations of dancers’ feet in a shared virtual dance space to emphasize timing and placement of feet. My work demonstrates that the feet embodiment provides the dancers with a better understanding of dance moves, helps them to synchronize timing of their dance steps, and provides them with a dance space in which they can freely create dance moves with their partners.
},
  Pdfurl={http://hcitang.org/papers/2016-mscthesis-alizadeh.pdf},
  Type={thesis},
}

@mastersthesis{rahman2016thesis,
  author       = {Rahman, S. M. Waliur}, 
  title        = {Design and Evaluation of a Self-monitoring Application for Chronic Headaches},
  school       = {University of Calgary},
  year         = {2015},
  month        = {11},
  abstract = {
  Chronic headache sufferers use headache diaries to learn about their headache symptoms and triggers. But the existing headache diaries do not support identification of probable headache triggers which is a critical requirement for self-monitoring of headaches. The literature describes several applications that keep track of headaches, but none of them allow the patients to identify potential headache triggers by exploring the correlations between the self-tracked factors and the onset of headaches. In this thesis, a self-monitoring application is designed that supports reviewing of headache trends and enables interactive visual exploration of potential correlations between the headaches and the putative triggers based on temporal data analysis. The design of the application reflects the data collection and the analytical needs of the headache patients. The evaluation results suggest that the application can be useful for the headache patients to identify their potential headache triggers, and hence enable better self-monitoring of headaches. 
},
  Pdfurl={http://hcitang.org/papers/2015-mscthesis-rahman.pdf},
  Type={thesis},
}

@inproceedings{graham2016sharedreflection,
  Abstract={Dramatic advances in sensor and computing miniaturization for personal data collection are making Personal Informatics (PI) tools a reality. Yet, advances in data collection have not been matched with similar advances in tools to promote, support, and facilitate reflection on this data. This gap leaves people with large swaths of data, but very little understanding of how to make sense of the data or to derive actionable insights. In this work, we explore a process called shared reflection, where individuals are paired with other data collectors, and asked (through prompts) to reflect on one another’s data. Based on a six-week study where 15 participants collected different kinds of personal data and engaged in a shared reflection process, we show that participants gained transformative insights from others’ reflections on their data. While this was promising, we discuss practical challenges in deploying this idea into real world personal informatics tools. In particular, while shared reflection can be appropriated to effectively bootstrap reflection on one’s data, this needs to be balanced against privacy and control concerns. },
  Author={Graham, Lisa and Tang, Anthony and Neustaedter, Carman},
  Booktitle={GROUP '16: Proceedings of the 19th International Conference on Supporting Group Work, Sanibel Island, FL, USA, November, 2016},
  Keywords={Shared reflection; personal informatics; personal data analytics; reflection.},
  Pdfurl={http://hcitang.org/papers/2016-group2016-shared-reflection.pdf},
  Title={Help Me Help You: Shared Reflection for Personal Data},
  Type={conference},
  Year={2016},
  Pages={99--109},
  Acceptance={36/111 - 32%},
}

@mastersthesis{graham2016thesis,
  author       = {Graham, Lisa}, 
  title        = {Help Me Help You: Shared Reflection for Personal Data},
  school       = {University of Calgary},
  year         = {2016},
  month        = {05},
  abstract = {
  Although our ability to collect personal information has increased dramatically through personal
informatics tools such as personal digital tracking technologies for step counts, location, and
more, minimal attention has been paid to designing tools to generate actionable insight from
data. Self-reflection is important to generate these insights; however, without scaffolding to
support the process, it is often ineffective. In this thesis, I introduce and explore shared reflection
– the reciprocal process of reflecting on others’ data and having others reflect on one’s own data
– as a means to bootstrap the reflection process. I synthesize literature on personal informatics
and social learning theories, design and conduct a six-week personal data collection study, and
evaluate the results. Shared reflection appears to show promise; however, users value privacy
and control over their personal data when sharing in a social context. Finally, the potential
application of shared reflection to new personal informatics tools is explored. 
},
  Pdfurl={http://hcitang.org/papers/2016-mscthesis-graham.pdf},
  Type={thesis},
}

@mastersthesis{tang2015thesis,
  author       = {Tang, Richard}, 
  title        = {Physio at Home - Exploring Visual Guidance and Feedback
Techniques for At-home Rehabilitation},
  school       = {University of Calgary},
  year         = {2015},
  month        = {06},
  abstract = {
  Physiotherapy patients learn exercises for rehabilitation with the help of a physiotherapist, but
are at risk of re-injury while exercising alone at home. This thesis explores the design and usage
of visualizations for guiding patients through physiotherapy exercises at home. I interviewed a
practicing physiotherapist to gain knowledge on physiotherapy practices, and then developed a
set of visual characteristics for movement guidance: plane/range of movement, positions/angles
to maintain, extent of movement, and rate of movement. I applied these in the design of
movement-guiding visualizations in two prototype systems: Zipples and Physio@Home. Zipples
was a Microsoft Kinect-based prototype featuring robust movement recording and playback
functionality, supported by a variety of visualizations. Physio@Home was a Vicon-based
iteration that improved on Zipples with an annotation tool, an iteratively-designed Wedge
visualization, and multiple camera perspectives. I evaluated both systems with laboratory studies
to measure their effectiveness in having participants follow pre-recorded exercises. I conclude
with findings from both systems and studies, and discuss potential areas for future work.},
  Pdfurl={http://hcitang.org/papers/2015-mscthesis-tang.pdf},
  Type={thesis},
}

@inproceedings{alizadeh2016happyfeet,
  Abstract={We present HappyFeet, a dancing system designed for supporting dancing experience between remotely located dance partners. HappyFeet uses 3D representation of dancers’ feet in a shared virtual dance space to emphasize timing and placement of feet. It has two modes of operation: a learning mode where the user can dance with pre-recorded dance lessons, and a second mode where the system provides a shared dance floor for remotely located dancers. We evaluated our system in a laboratory study where we investigated the role of the feet embodiment by comparing its’ use to a video-only condition. The feet embodiment provided our participants with a better understanding of dance moves, helped them to synchronize timing of their dance steps, and provided them with a dance space in which they could freely create dance moves with their partners.},
  Author={Alizadeh, Hesam and Witcraft, Anna and Sharlin, Ehud and Tang, Anthony},
  Booktitle={GI '16: Proceedings of the 2016 Graphics Interface Conference},
  Title={HappyFeet: Embodiments for Joint Remote Dancing},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2016-gi2016-happyfeet.pdf},
  Videourl={http://hcitang.org/papers/2016-gi2016-happyfeet.mp4},
  Url={http://graphicsinterface.org/proceedings/gi2016/gi2016-15/},
  Doi={10.20380/GI2016.15},
  Year={2016},
  Pages={117--124},
  Acceptance={13/33 - 39%},
}

@inproceedings{fakourfar2016annotations,
  Abstract={Recent mobile technology has provided new opportunities for creating remote assistance systems. However, mobile support systems present a particular challenge: both the camera and display are held by the user, leading to shaky video. When pointing or drawing annotations, this means that the desired target often moves, causing the gesture to lose its intended meaning. To address this problem, we investigate annotation stabilization techniques, which allow annotations to stick to their intended location. We studied two annotation systems, using three different forms of annotations, with both tablets and head-mounted displays. Our analysis suggests that stabilized annotations and head-mounted displays are only beneficial in certain situations. However, the simplest approach of automatically freezing video while drawing annotations was surprisingly effective in facilitating the completion of remote assistance tasks},
  Author={Fakourfar, Omid and Ta, Kevin and Tang, Richard and Bateman, Scott and Tang, Anthony},
  Booktitle={CHI 2016: Proceedings of the 2016 SIGCHI Conference on Human Factors in Computing Systems},
  Title={Stabilized Annotations for Mobile Remote Assistance},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2016-chi2016-annotations.pdf},
  Url={http://chi2016.acm.org/},
  VideoUrl={https://www.youtube.com/watch?v=na9OBQiVjLk},
  Year={2016},
  Acceptance={565/2435 - 23.2%},
  Pages={1548--1560},
}

@inproceedings{singhal2016bystanders,
  Abstract={We are observing an increase in the use of smartphones and wearable devices in public places for streaming and recording video. Yet the use of cameras in these devices can infringe upon the privacy of the people in the surrounding environment by inadvertently capturing them. This paper presents findings from an in-situ exploratory study that investigates bystanders’reactions and feelings towards streaming and recording videos with smartphones and wearable glasses in public spaces. We use the interview results to guide an exploration of design directions for mobile video.},
  Author={Singhal, Samarth and Neustaedter, Carman and Schiphorst, Thecla and Tang, Anthony and Abihisekh, Patra and Pan, Rui},
  Booktitle={EA CHI '16: Extended Abstracts of the 2016 SIGCHI Conference on Human Factors in Computing Systems},
  Keywords={Wearable glasses; privacy; streaming; recording.},
  Pdfurl={http://hcitang.org/papers/2016-chi2016wip-bystanders.pdf},
  Url={http://hcitang.org/papers/2016-chi2016wip-bystanders-poster.pdf},
  Title={You are Being Watched: Bystanders’ Perspective on the Use of Camera Devices in Public Spaces},
  Type={poster},
  Year={2016},
  Notes={6-page abstract + poster},
  Acceptance={281/647 - 43.4%},
  Pages={3197--3203},
}

@inproceedings{neustaedter2016mobiledevicesathome,
  Abstract={Mobile devices have begun to raise questions around the potential for overuse when in the presence of family or friends. As such, we conducted a diary and interview study to understand how people use mobile devices in the presence of others at home, and how this shapes their behavior and household dynamics. Results show that family members become frustrated when others do non-urgent activities on their phones in the presence of others. Yet people often guess at what others are doing because of the personal nature of mobile devices. In some cases, people developed strategies to provide a greater sense of activity awareness to combat the problem. Mobile phone usage was sometimes perceived as beneficial by providing a mechanism for needed disengagement from family members.  These findings suggest several opportunities for redesigning mobile device software to mitigate emergent frustrations, and open up new opportunities for nurturing social interactions among family members.
  },
  Author={Odour, Erick and Neustaedter, Carman and Odom, William and Tang, Anthony and Moallem, Niala and Tory, Melanie and Irani, Pourang},
  Booktitle={DIS 2016: Proceedings of the ACM conference on Designing Interactive Systems in 2016},
  Title={The Frustrations and Benefits of Mobile Device Usage in the Home when Co-Present with Family Members},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2016-dis2016-mobile-device-use-at-home.pdf},
  Url={http://www.dis2016.org/},
  Year={2016},
  Acceptance={108/418 - 26%},
  Notes={Honourable Mention - Top 5% of all submissions},
  Pages={1315-1327}
}

@inproceedings{jones2016dronevideoconferencing,
  Author={Jones, Brennan and Dillman, Kody and Tang, Richard and Tang, Anthony and Sharlin, Ehud and Oehlberg, Lora and Neustaedter, Carman and Bateman, Scott},
  Abstract={People are increasingly using mobile video to communicate, collaborate, and share experiences while on the go. Yet this presents challenges in adequately sharing camera views with remote users. In this paper, we study the use of semi-autonomous drones for video conferencing, where an outdoor user (using a smartphone) is connected to a desktop user who can explore the environment from the drone’s perspective. We describe findings from a study where pairs collaborated to complete shared navigation and search tasks. We illustrate the benefits of providing the desktop user with a view that is elevated, manipulable, and decoupled from the outdoor user. In addition, we articulate how participants overcame challenges in communicating environmental information and navigational cues, negotiated control of the view, and used the drone as a tool for sharing experiences. This provides a new way of thinking about mobile video conferencing where cameras that are decoupled from both users play an integral role in communication, collaboration, and sharing experiences.},
  Booktitle={DIS 2016: Proceedings of the ACM conference on Designing Interactive Systems in 2016},
  Title={Elevating Communication, Collaboration, and Shared Experiences in Mobile Video through Drones},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2016-dis2016-mobile-video-through-drones.pdf},
  Url={http://www.dis2016.org/},
  Year={2016},
  Acceptance={108/418 - 26%},
  Pages={1123-1135}
}

@techreport{alizadeh2016culturalprobe,
  Abstract={ This report explores the question of how older adults understand and experience exercise (and physical activity) in their everyday lives. We provide this understanding by deploying a cultural probe study with seven participants in two local retirement communities. The result of our work is a set of design goals for a system to support remote activity.},
  Author={Alizadeh, Hesam and Witcraft, Anna and Sharlin, Ehud and Tang, Anthony},
  Title={ Exploring and Understanding Physical Activity Amoung Older Adults},
  Keywords={Physical activity, older adults, cultural probe study},
  Year={2015},
  Number={2015-1081-14},
  Publisher={Department of Computer Science, University of Calgary},
  Url={http://hdl.handle.net/1880/51025},
}

@incollection{huron2016constructivevis,
  Author={Huron, Samuel and Thudt, Alice and Aseniero, Bon Adriel and Tang, Anthony and Carpendale, Sheelagh},
  Abstract={In this chapter, with my collaborators, I provide a pictorial overview of two papers that address these challenges (Huron, Jansen, and Carpendale 2014; Huron, Carpendale, et al. 2014). In these paper we defne construction as a design paradigm for non-experts to author simple and dynamic visualizations. This paradigm is inspired by well-established theories in developmental psychological as well as past and existing practices of authoring visualization with tangible elements. We describe the simple conceptual components and processes underlying this paradigm and a preliminary study we employed to assess it. The results of this study confrm that non-experts in InfoVis can create, update, and annotate a visualization in a short period of time. Moreover, this study allowed us to articulate a primary model of how people perform the authoring of visual mappings using this paradigm.},
  Title={Constructive Visualization: A New Paradigm to Empower People to Author Visualization},
  Booktitle={SURFNET / Designing Digital Surface Applications},
  Editor={Maurer, Frank},
  Publisher={NSERC Surfnet, University of Calgary},
  URL={http://dspace.ucalgary.ca/handle/1880/50450},
  PDFUrl={http://hcitang.org/papers/2016-surfnet-constructive-visualization.pdf},
  Year={2016},
  Pages={169-191},
  Chapter={9},
  ISBN={978-0-88953-388-2},
}


@incollection{dillman2016athomephysiotherapy,
  Author={Dillman, Kody and Tang, Richard and Tang, Anthony},
  Abstract={ We are guided by three central questions in this work: first, what are the communication practices in traditional face-to-face physiotherapy that must be preserved; second, what challenges does video media space present to these practices, and third, how can technologies be designed to overcome these challenges? We explore these questions in this chapter through two explorations. In the first, we worked with physiotherapists to understand how to design tools to enable patients to work with physiotherapists live—for diagnosis and exercise training. In the second, we explored the ‘at-home’ case of doing exercises between physiotherapist visits. We make two contributions in this work. First, we provide insights into a specifc domain (physiotherapy) that can be used to guide design of video media spaces for remote work in this area. Second, from this work, we explore the concept of the body as a workspace, developing this idea through both sketches and critical refection of our experiences. Our ongoing work involves designing tools for effective remote physiotherapy, though the fndings should also support other domains where it is important to remotely teach activities that require specifc movements (e.g. dance, personal training, martial arts, etc.).},
  Title={Towards At-Home Physiotherapy: Next Generation Teleconferencing and Surface Based Interventions},
  Booktitle={SURFNET / Designing Digital Surface Applications},
  Editor={Maurer, Frank},
  Publisher={NSERC Surfnet, University of Calgary},
  URL={http://dspace.ucalgary.ca/handle/1880/50450},
  PDFUrl={http://hcitang.org/papers/2016-surfnet-towards-at-home-physio.pdf},
  Year={2016},
  Pages={289-304},
  Chapter={16},
  ISBN={978-0-88953-388-2},
}


@proceedings{gi2015,
  editor    = {Hao (Richard) Zhang and
               Anthony Tang},
  title     = {Proceedings of the 41st Graphics Interface Conference, Halifax, NS,
               Canada, June 3-5, 2015},
  publisher = {{ACM}},
  year      = {2015},
  url       = {http://dl.acm.org/citation.cfm?id=2788890},
  isbn      = {978-0-9947868-0-7},
  timestamp = {Mon, 15 Jun 2015 15:04:36 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/graphicsinterface/2015},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{marquardt2015excite,
  Abstract={A central issue in designing collaborative multi-surface environments is evaluating the interaction techniques, tools, and applications that we design. We often analyze data from studies using inductive video analysis, but this is extremely time-consuming because of the volume of data. We designed EXCITE, which gives analysts the ability to analyze studies by quickly querying aspects of people’s interactions with devices around them using a logical syntax. These queries provide simple, immediate visual access to matching incidents in the interaction stream, video data, and motion-capture data. The query language can help filter and review the data streams based on criteria such as distance or orientation between people and devices. This general approach allows analysts to provisionally develop theories and evaluate them rapidly, and aids the coding process for deeper analysis in the use of multi-surface environments.},
  Author={Marquardt, Nicolai and Schardong, Frederico and Tang, Anthony},
  Booktitle={INTERACT 2015: Proceedings of INTERACT 2015 - 16th IFIP TC 13 International Conference},
  Title={EXCITE: EXploring Collaborative Interaction in Tracked Environments},
  Type={conference},
  Pdfurl={http://hcitang.org/papers/2015-interact2015-excite.pdf},
  Url={http://www.interact2015.org/},
  Year={2015},
  Acceptance={93/319 - 29.6%},
  Pages={89--97},
}


@inproceedings{seyed2015ciphercard,
  Abstract={We present CipherCard, a physical token that defends against shoulder-surfing attacks on user authentication on capacitive touchscreen devices. When CipherCard is placed over a touchscreen’s pin-pad, it remaps a user’s touch point on the physical token to a different location on the pin-pad. It hence translates a visible user password into a different system password received by a touchscreen, but is hidden from observers as well as the user. CipherCard enhances authentication security through Two-Factor Authentication (TFA), in that both the correct user password and a specific card are needed for successful authentication. We explore the design space of CipherCard, and describe three implemented variations each with unique capabilities. Based on user feedback, we discuss the security and usability implications of CipherCard, and describe several avenues for continued exploration.},
  Author={Seyed, Teddy and Yang, Xing-Dong and Tang, Anthony and Greenberg, Saul and Gu, Jiawei and Zhu, Bin and Cao, Xiang},
  Booktitle={INTERACT 2015: Proceedings of INTERACT 2015 - 16th IFIP TC 13 International Conference},
  Pdfurl={http://hcitang.org/papers/2015-interact2015-ciphercard.pdf},
  Videourl={http://hcitang.org/papers/2015-interact2015-ciphercard.mov},
  Title={CipherCard: A Token-based Approach against Camera- based Shoulder Surfing Attacks on Common Touchscreen Devices},
  Type={conference},
  Url={http://www.interact2015.org/},
  Year={2015},
  Acceptance={93/319 - 29.6%},
  Pages={436--454},
}

@mastersthesis{aseniero2014thesis,
  author       = {Aseniero, Bon Adriel}, 
  title        = {STRATOS: The Design of Visualization to Support Decision-making in Software Release Planning},
  school       = {University of Calgary},
  year         = {2014},
  month        = {12},
  abstract = {
Software is typically developed in incremental stages or releases. Planning releases involves deciding on which features of the software should have implementation priority. This is a complex planning process involving numerous constraints and factors, trade-offs, that often make decisions difficult. Since the success of a product depends on this plan, it is vital for planners to examine the trade-offs between different alternatives in order to make an informed choice. To support this type of decision-making, my exploration involved designing and implementing STRATOS—a visualization tool showing several software release plans simultaneously within a singular layout, helping planners understand the differences among them. Through a qualitative evaluation, I found that it enabled a range of decision-making processes, ultimately helping participants in choosing an optimal release plan. My contributions include the hybrid visualization, STRATOS, and the findings from its evaluation that implicate design for future visualizations supporting decision-making.
  },
  Pdfurl={http://hcitang.org/papers/2014-mscthesis-aseniero.pdf},
  Type={thesis},
}

@mastersthesis{dunlap2013thesis,
  author       = {Dunlap, Matthew A.}, 
  title        = {Science Caching: Applying Geocaching to Mobile Citizen Science},
  school       = {University of Calgary},
  year         = {2013},
  month        = {5},
  abstract = {
  	Citizen science occurs in part when scientists work with volunteers to collect science data in particular field locations. The benefit is that citizen science eases and lessens the cost of collecting such information. Yet it has a variety of known problems. This document focuses on four specific citizen science problems concerning difficulties in data collection, data validation, volunteer training and volunteer coordination. The thesis is that these problems can be mitigated by applying aspects from another thriving location-based activity: the geocaching treasure hunt as enabled by mobile devices. Citizen science can exploit geocaching‘s location-based design, its use of physical objects, and its user maintained content. To explore and critique this thesis, a prototype mobile system called Science Caching was developed, along with various scenarios that describe how it addresses issues in collection, validation, training and coordination. The system and scenarios – which serve as a working sketch – were shown to citizen science experts via an interview-based design critique. In particular, they provided feedback on the choice of the problems addressed by the system, the approach to the problems as realized by Science Caching, how those approaches could be extended, and what other areas in citizen science they could be applied to. The results were analyzed via affinity diagramming, which uncovered various overarching themes. Generally, the combination of geocaching and mobility was received quite positively, where participants indicated various areas where it would be applicable. Problems and improvements were also suggested, giving insight into future iterations of the method and the system.
  },
  Pdfurl={http://hcitang.org/papers/2013-mscthesis-dunlap.pdf},
  Type={thesis},
}


@article{dunlap2015sciencecaching,
  Abstract={Site-based citizen science occurs when volunteers work with scientists to collect data at particular field locations. The benefit is greater data collection at lesser cost. Yet difficulties exist. We developed ScienceCaching, a prototype citizen science aid designed to mitigate four specific problems by applying aspects from another thriving location-based activity: geocaching as enabled by mobile devices. Specifically, to ease problems in data collection, ScienceCaching treats sites as geocaches: Volunteers find sites opportunistically via geocaching methods and use equipment and other materials pre-stored in cache containers. To ease problems in data validation, ScienceCaching flags outlier data as it is entered so that on-site volunteers can be immediately check and correct data. Additionally, other volunteers are directed to that site at a later time for further readings that provide data redundancy. To ease volunteer training, ScienceCaching directs volunteers to training sites on an as-needed basis, where they are taught and tested against known measures. To ease volunteer coordination, ScienceCaching automatically directs volunteers to particular sites of interest, and real-time communication between volunteers and scientist is enabled as needed. We developed ScienceCaching primarily as a technology probe—a working but quite limited system—to embody these ideas and to evaluate their worthiness by eliciting reactions from scientists involved in citizen science. Scientists saw many opportunities in using fixed location caches and geocaching techniques to aid citizen science. Yet they expanded the discussion. Amongst these, they emphasized practical concerns that must be addressed, and they argued that future systems should carefully consider the role of the social experience—both the “online” experience and the shared physical experience of visiting sites.},
  Author = {Dunlap, Matthew A. and Tang, Anthony and Greenberg, Saul},
  Journal = {Personal and Ubiquitous Computing},
  Year = {2015},
  Title = {Applying geocaching principles to site-based citizen science and eliciting reactions via a technology probe},
  Type = {journal},
  Doi={http://dx.doi.org/10.1007/s00779-015-0837-0},
  Pdfurl={http://hcitang.org/papers/2015-puc-sciencecaching.pdf},
  Pages = {897-913},
  Publisher = {Springer},
  Volume={19},
  Issue={5-6},
  ISSN={1617-4909},
}

@inproceedings{payne2015physvis,
  Abstract={Physical visualization has been demonstrated to be more efficient than on-screen representation for information visualization tasks [5]. In addition, it has been suggested that physical visualization might be particularly memorable or engaging [12], but this has yet to be explored empirically. This paper describes a potential evaluative study that would explore differences between flat, on-screen bar graphs and extruded bar graphs, a simple form of physical visualization. Specifically, memorability of information, user engagement, accuracy and efficiency of information retrieval using physical and on-screen graphs would be compared. Broader challenges and possibilities for the evaluation of physical visualization are discussed.},
  Author={Payne, Jennifer and Johnson, Jason and Tang, Anthony},
  Booktitle={Exploring the Challenges of Making Data Physical - Workshop at CHI 2015},
  Editor={Jason Alexander and Yvonne Jansen and Kasper Hornbaek and Johan Kildal and Abhijit Karnik},
  Pdfurl={http://hcitang.org/papers/2015-chi2015workshop-physvis.pdf},
  Title={Exploring Physical Visualization},
  Type={workshop},
  Url={http://www.scc.lancs.ac.uk/physicaldata2015/},
  Year={2015},
}

@inproceedings{jones2015mobilevideoconferencing,
  Abstract={Video conferencing allows people to collaborate and share experiences across a variety of work and play- related scenarios. Mobile video conferencing enables entirely new usage scenarios where one or more parties are physically moving around in a large activity environment. One major benefit of mobile devices is that they can potentially support activities that focus on large environments—activities such as searching, navigating, and touring. However, today’s mobile video conferencing tools are not designed to support such activities. We discuss potential design ideas for mobile video conferencing tools that could better support these kinds of activities.},
  Author={Jones, Brennan and Tang, Anthony},
  Booktitle={Everyday Telepresence: Emerging Practices and Future Research Directions - Workshop at CHI 2015},
  Editor={Irene Rae and Bilge Mutlu and Gary M. Olson and Judith S. Olson and Leila A. Takayama and Gina Venolia},
  Pdfurl={http://hcitang.org/papers/2015-chi2015workshop-mobile-video-conferencing.pdf},
  Title={Improving Collaboration and Shared Experiences in Out-and-About Mobile Video Conferencing},
  Type={workshop},
  Url={http://hci.cs.wisc.edu/workshops/chi2015/},
  Year={2015},
}

@inproceedings{witcraft2015asymmetry,
  Abstract={While there exist substantial efforts to support independent living among the elderly, a significant portion of this demographic will continue to require assisted living facilities. Many of the residents in these facilities may experience extreme feelings of loneliness and depression. Our interest is in addressing these feelings of loneliness. We report here on an early pilot study of a paper prototype for a communication system and our early insights from this process. Specifically, we discuss the asymmetry of desiring communication, the cost of interaction, the accessibility of tools and the topics of conversation.},
  Author={Witcraft, Anna and Tang, Anthony and Hushlak, Gerald},
  Booktitle={Smart for Life: Designing Smart Home Technologies that Evolve with Users- Workshop at CHI 2015},
  Editor={Sarah Mennicken and Amy Hwang and Rayoung Yang and Jesse Hoey and Alex Mihailidis and Elaine M. Huang},
  Pdfurl={http://hcitang.org/papers/2015-chi2015workshop-asymmetry-of-needs.pdf},
  Title={Asymmetry of Needs},
  Type={workshop},
  Url={http://www.zpac.ch/smartforlife/cfp.html},
  Year={2015},
}

@inproceedings{jones2015mobilecamerawork,
  Abstract={Mobile video conferencing, where one or more participants are out from behind a desk and moving about in the real world, enables entirely new interaction scenarios (for example, asking a remote party for help to construct or repair an object, or showing a physical location to someone). While we have a good understanding of the challenges of video conferencing in office or home environments, we do not fully understand the mechanics of camera work—how people use mobile devices to communicate with one another—during mobile video calls. To understand these mechanics, we conducted an observational study where pairs of participants completed tasks using a mobile video conferencing system. Our analysis suggests that people use the camera view deliberately to support their interactions—for example, to convey a message or to ask questions—but the limited field of view, and the lack of camera control can make it a frustrating experience for remote parties.},
  Author = {Jones, Brennan and Witcraft, Anna and Tang, Anthony and Bateman, Scott and Neustaedter, Carman},
  Booktitle = {CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems},
  Doi = {http://dx.doi.org/10.1145/2702123.2702345},
  Title={Mechanics of Camera Work in Mobile Video Collaboration},
  Type={conference},
  Year={2015},
  PdfUrl={http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.pdf},
  VideoUrl={http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.mp4},
  Pages = {957-966},
  Publisher={ACM},
}

@inproceedings{tang2015physioathome,
  Abstract = {Physiotherapy patients exercising at home alone are at risk of re-injury since they do not have corrective guidance from a therapist. To explore solutions to this problem, we designed Physio@Home, a prototype that guides people through pre-recorded physiotherapy exercises using real-time visual guides and multi-camera views. Our design addresses several aspects of corrective guidance, including: plane and range of movement, positions and angles of joints to maintain, and extent of movement. We evaluated our design, comparing how closely participants could follow exercise movements in various feedback conditions. Participants were most accurate when using the visual guide and multi-views. Based on our qualitative findings on the visual complexity of the feedback, we conclude with suggestions for exercise guidance systems.},
  Author = {Tang, Richard and Yang, Xing-Dong and Tang, Anthony and Bateman, Scott and Jorge, Joaquim},
  Booktitle = {CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems},
  Doi = {http://dx.doi.org/10.1145/2702123.2702401},
  Title={Physio@Home: Exploring visual guidance and feedback techniques for physiotherapy patients at home},
  Type={conference},
  Year={2015},
  Pdfurl={http://hcitang.org/papers/2015-chi2015-physio-at-home.pdf},
  Videourl={http://hcitang.org/papers/2015-chi2015-physio-at-home.mp4},
  Pages = {4123-4132},
  Publisher={ACM},
}

@inproceedings{aseniero2015stratos,
  Abstract = {Software is typically developed incrementally and released in stages. Planning these releases involves deciding which features of the system should be implemented for each release. This is a complex planning process involving numerous constraints and factors—trade-offs that often make decisions difficult. Since the success of a product depends on this plan, it is important to understand the trade-offs between different release plans and to make an informed choice. We present STRATOS, a tool that visualizes several optimized software release plans simultaneously. The visualization shows several attributes about each plan that are important to planners. Multiple plans are shown in a single layout to help planners find and understand the trade-offs between different plans. We evaluated our tool via a qualitative study, finding that STRATOS enables a range of decision-making processes, and ultimately helping our participants optimize their planning.},
  Author = { Aseniero, Bon Adriel and Wun, Tiffany and Ledo, David and Ruhe, Guenther and Tang, Anthony and Carpendale, Sheelagh },
  Booktitle = {CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems},
  Title = {STRATOS: Using Visualization to Support Decisions in Strategic Software Release Planning},
  Doi={http://dx.doi.org/10.1145/2702123.2702426},
  Type={conference},
  Year={2015},
  Pdfurl={http://hcitang.org/papers/2015-chi2015-stratos.pdf},
  Videourl={http://hcitang.org/papers/2015-chi2015-stratos.mp4},
  Pages={1479-1488},
  Publisher={ACM},
}

@inproceedings{reilly2015mixedreality,
  Abstract = {We present results from a study examining how the physical layout of a project room and task affect the cognitive maps acquired of a connected virtual environment during mixed-presence collaboration. Results indicate that a combination of physical layout and task impacts cognitive maps of the virtual space. Participants did not form a strong model of how different physical work regions were situated relative to each other in the virtual world when the tasks performed in each region differed. Egocentric perspectives of multiple displays enforced by different furniture arrangements encouraged cognitive maps of the virtual world that reflected these perspectives, when the displays were used for the same task. These influences competed or coincided with document-based, audiovisual and interface cues, influencing collaboration. We consider the implications of our findings on WYSIWIS mappings between real and virtual for mixed-presence collaboration.},
  Author = { Reilly, Derek and Echenique, Andy and Wu, Andy and Tang, Anthony and Edwards, Keith},
  Booktitle = {CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems},
  Title = {Mapping out Work in a Mixed Reality Project Room},
  Type={conference},
  Year={2015},
  Doi = {http://dx.doi.org/10.1145/2702123.2702506},
  Pdfurl = {http://hcitang.org/papers/2015-chi2015-mixed-reality.pdf},
  Pages = {887-896},
  Publisher={ACM},
}



@article{huang2015pva,
  Abstract={Data surrounds each and every one of us in our daily lives, ranging from exercise logs, to archives of our interactions with others on social media, to online resources pertaining to our hobbies. There is enormous potential for us to use these data to understand ourselves better and make positive changes in our lives. Visualization (Vis) and Visual Analytics (VA) offer substantial opportunities to help individuals gain insights about themselves, their communities and their interests; however, designing tools to support data analysis in non-professional life brings a unique set of research and design challenges. We investigate the requirements and research directions required to take full advantage of Vis and VA in a personal context. We develop a taxonomy of design dimensions to provide a coherent vocabulary for discussing Personal Visualization and Personal Visual Analytics. By identifying and exploring clusters in the design space, we discuss challenges and share perspectives on future research. This work brings together research that was previously scattered across disciplines. Our goal is to call research attention to this space and engage researchers to explore the enabling techniques and technology that will support people to better understand data relevant to their personal lives, interests, and needs.},
  Author = {Huang, Dandan and Tory, Melanie and Aseniero, Bon Adriel and Bartram, Lyn and Bateman, Scott and Carpendale, Stacey and Tang, Anthony and Woodbury, Robert},
  Journal = {IEEE Transactions on Visualization and Computer Graphics},
  Keywords = {Taxonomy, personal context, interaction design, mobile and ubiquitous visualization},
  Year = {2015},
  Pdfurl={http://hcitang.org/papers/2015-tvcg-pva.pdf},
  Title={Personal Visualization and Personal Visual Analytics},
  Type={journal},
  Doi={http://doi.ieeecomputersociety.org/10.1109/TVCG.2014.2359887},
  Pages={420-433},
  Volume={21},
  Issue={3},
  ISSN={1077-2626},
}

@inproceedings{jones2014spherosumo,
  Abstract={The Sphero is a robotic remote-controlled ball capable of rolling around on its own in any direction at multiple speeds. Numerous games have been designed for the Sphero for smartphones and tablets. However, most of these games provide an interface for controlling the Sphero that is far from natural. These games also do not put a strong focus on the physical environment around the Sphero. This work discusses a control scheme used to control a Sphero with another Sphero, and a pervasive game leveraging this scheme that emphasizes physical properties of the environment to create an immersive experience.},
  Author={Jones, Brennan and Dillman, Kody and Manesh, Setareh Aghel and Sharlin, Ehud and Tang, Anthony},
  Booktitle={EA CHI PLAY '14: ACM SIGCHI Annual Symposium on Computer-Human Interaction in Play},
  Keywords={Sphero; robotic ball; pervasive gaming; controllers; natural mappings; immersion; gaming experience},
  Pdfurl={http://hcitang.org/papers/2014-chiplay2014-spherosumo.pdf},
  Title={Designing an Immersive and Entertaining Pervasive Gameplay Experience with Spheros as Game and Interface Elements},
  Type={poster},
  Year={2014},
  Doi={http://dx.doi.org/10.1145/2658537.2661301},
  Notes={2-page abstract + poster}
}

@inproceedings{seyed2014exploring,
  author={Seyed, Teddy and Rodrigues, Francisco Marinho and Maurer, Frank and Tang,
    Anthony},
  title={Exploring 3D volumetric medical data using mobile devices},
  booktitle={3DUI 2014: 2014 IEEE Symposium on 3D User Interfaces},
  year={2014},
  pages={173-174},
  type={poster},
  publisher={IEEE},
  abstract={Medical imaging specialists have traditionally used keyboard and mouse
    based techniques and interfaces for examining both 2D and 3D medical images,
    but with newer imaging technologies resulting in significantly larger volumes
    of 3D medical images, these techniques that have become increasingly cumbersome
    for imaging specialists. To replace traditional techniques, using mobile devices
    present an effective means for navigating and exploring complex 3D medical
    data sets, as they provide increased fluidity and flexibility, leveraging people's
    existing skills with tangible objects. 3D interactions using mobile devices
    may provide benefit for imaging specialists, but little is known about using
    these interactions in the medical imaging domain. In this paper, we explore
    the design of 3D interaction techniques using mobile devices and preliminary
    feedback from imaging specialists suggests that these interactions may be a
    viable solution for the medical imaging domain.},
  pdfurl={http://hcitang.org/papers/2014-3duiposter-exploring-3d-volumetric-data.pdf},
  doi={http://dx.doi.org/10.1109/3DUI.2014.6798876}
}

@inproceedings{huron2014constructiveviz,
  Abstract={If visualization is to be democratized, we need to provide a means
    for non-experts to create visualizations that allow them to engage with datasets.
    We present constructive visualization as a new paradigm for the simple creation
    of flexible, dynamic visualizations. Constructive visualization is simple---that
    skills required to build and manipulate the visualizations are akin to kindergarten
    play; it is expressive---one can build within the constraints of the chosen
    environment, and it also supports dynamics as these visualizations can be rebuilt
    and adjusted. We describe the conceptual components and processes underlying
    constructive visualization, and describe real-world examples to illustrate
    the utility of this approach. The constructive visualization approach builds
    on our inherent understanding and experience with physical building blocks,
    the model enables non-experts to create entirely novel visualizations, and
    to engage with datasets in a manner that would not have otherwise been possible.},
  Acceptance={26{\%} - 105/402},
  Author={Huron, Samuel and Carpendale, Sheelagh and Thudt, Alice and Tang, Anthony
    and Mauerer, Michael},
  Booktitle={DIS 2014: Proceedings of the ACM conference on Designing Interactive
    Systems in 2014},
  Doi={http://dx.doi.org/10.1145/2598784.2598806},
  Keywords={Design; visualization; construction; assembling; constructivism; constructionism;
    education},
  Notes={Best Paper Nominee (top 2{\%} of submissions)},
  Pdfurl={http://hcitang.org/papers/2014-dis2014-constructive-visualization.pdf},
  Publisher={ACM},
  Title={Constructive Visualization},
  Type={conference},
  Year={2014},
  Bdsk-Url-1={http://dx.doi.org/10.1145/2598784.2598806},
  pages={433-442 }
}

@inproceedings{alizadeh2014haptics,
  Abstract={Group exercise provides motivation to follow and maintain a healthy
    daily exercise schedule while enjoying beneficial encouragement and social
    support from friends and exercise partners. However, mobility and transportation
    issues frequently prevent seniors from engaging in group activities. To address
    this problem, we investigated the exercise needs of seniors and developed a
    prototype remote exercise system. Our system uses haptic feedback to simulate
    assistive pushing and pulling of limbs when exercising with a partner, and
    we developed three distinct vibration metaphors -- constant push/pull, corrective
    feedback, and notification -- to convey engagement and connection between exercise
    partners. We conducted a preliminary evaluation of our system and our vibration
    metaphors to determine the validity of our design concepts. We contribute a
    set of lessons on the use of haptic feedback for remote group exercise.},
  Acceptance={49{\%} - 241/496},
  Author={Alizadeh, Hesam and Tang, Richard and Sharlin, Ehud and Tang, Anthony},
  Booktitle={CHI EA '14: CHI '14 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Added={2014-02-11 03:05:40 +0000},
  Date-Modified={2014-06-06 17:09:00 +0000},
  Doi={http://dx.doi.org/10.1145/2559206.2581318},
  Keywords={Exergame; Interpersonal Synchronization; Haptic feedback},
  Notes={6-page abstract + poster.},
  Pages={2401-2406},
  Pdfurl={http://hcitang.org/papers/2014-chi2014wip-remote-haptics.pdf},
  Publisher={ACM},
  Title={Haptics in Remote Collaborative Exercise Systems for Seniors},
  Type={poster},
  Url={http://hcitang.org/papers/2014-chi2014wip-remote-haptics-poster.pdf},
  Year={2014},
  Bdsk-Url-1={http://hcitang.org/papers/2014-chi2014wip-remote-haptics-poster.pdf},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2559206.2581318}
}

@inproceedings{jones2014arttherapy,
  Abstract={Art therapy provides therapeutic benefit to people suffering from chronic
    pain, and recent work has explored supporting art therapy through online tools
    such as chat forums and discussion boards. These tools give people the benefit
    of engaging in art therapy without the burden of having to leave one's home
    (when transportation may be a challenge), and allowing people to reveal their
    identities through dialog and activity rather than through one's appearance.
    However, these tools also do not provide much opportunity for collaboration
    and shared art-making. Because group members are not aware of each other's
    actions and non-verbal cues in a chat room, they cannot collaborate with each
    other easily. We discuss the design and development of tools that promote enhanced
    awareness of non-verbal cues and shared creative experiences in online group
    art therapy.},
  Acceptance={49{\%} - 241/496},
  Author={Jones, Brennan and Hankinson, Sara Prins and Collie, Kate and Tang, Anthony},
  Booktitle={CHI EA '14: CHI '14 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Added={2014-02-11 03:04:14 +0000},
  Date-Modified={2014-06-06 17:08:47 +0000},
  Doi={http://dx.doi.org/10.1145/2559206.2581302},
  Keywords={Art therapy; online therapy; telehealth; art making; collaboration;
    user awareness; user representation},
  Notes={6-page abstract + poster.},
  Pages={1759-1764},
  Pdfurl={http://hcitang.org/papers/2014-chi2014wip-art-therapy.pdf},
  Publisher={ACM},
  Title={Supporting Non-Verbal Visual Communication in Online Group Art Therapy},
  Type={poster},
  Url={http://hcitang.org/papers/2014-chi2014wip-art-therapy-poster.pdf},
  Year={2014},
  Bdsk-Url-1={http://hcitang.org/papers/2014-chi2014wip-art-therapy.pdf},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2559206.2581302}
}

@inproceedings{tang2014physioathome,
  Abstract={Patients typically undergo physiotherapy with the help of a physiotherapist,
    who teach, guide, and correct the patient as they perform exercises. The problem
    is when the patient is at home, without guidance and corrective feedback from
    a physiotherapist, the patient will not know if they are doing their exercises
    incorrectly. To address this problem, we implemented a prototype that guides
    patients through pre-recorded exercise movements using visual guides overlaid
    atop a mirror-view of the patient on a wall-mounted display. We conducted informal
    evaluations with pilot studies to evaluate our current designs. We identified
    some working designs and design characteristics, and in our pilot study, participants
    showed improvement in movement accuracy with the guides over recorded video.
    Collected data will assist in developing future iterations of the system and
    designing improved guides for physiotherapy instruction at home.},
  Acceptance={49{\%} - 241/496},
  Author={Tang, Richard and Alizadeh, Hesam and Tang, Anthony and Bateman, Scott
    and Jorge, Joaquim},
  Booktitle={CHI EA '14: CHI '14 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Added={2014-02-11 03:02:43 +0000},
  Date-Modified={2014-06-06 17:08:28 +0000},
  Doi={http://dx.doi.org/10.1145/2559206.2581197},
  Keywords={Physiotherapy; Human Factors; Design; Measurement},
  Notes={6-page abstract + poster.},
  Pages={1651-1656},
  Pdfurl={http://hcitang.org/papers/2014-chi2014wip-physio@home.pdf},
  Publisher={ACM},
  Title={Physio@Home: Design Explorations to Support Movement Guidance},
  Type={poster},
  Url={http://hcitang.org/papers/2014-chi2014wip-physio@home-poster.pdf},
  Year={2014},
  Bdsk-Url-1={http://hcitang.org/papers/2014-chi2014wip-physio@home-poster.pdf},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2559206.2581197}
}

@inproceedings{seyed2014medical,
  Abstract={3D volumetric medical images, such as MRIs, are commonly explored and
    interacted with by medical imaging experts using systems that require keyboard
    and mouse-based techniques. These techniques have presented challenges for
    medical imaging specialists: 3D spatial navigation is difficult, in addition
    to the detailed selection and analysis of 3D medical images being difficult
    due to depth perception and occlusion issues. In this work, we explore a potential
    solution to these challenges by using tangible interaction techniques with
    a mobile device to simplify 3D interactions for medical imaging specialists.
    We discuss preliminary observations from our design sessions with medical imaging
    specialists and argue that tangible 3D interactions using mobile devices are
    viable solution for the medical imaging domain, as well as highlight that domain
    plays an important role in 3D interaction techniques. },
  Acceptance={49{\%} - 241/496},
  Author={Seyed, Teddy and Rodrigues, Francisco Marinho and Maurer, Frank and Tang,
    Anthony},
  Booktitle={CHI EA '14: CHI '14 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Added={2014-02-11 02:58:06 +0000},
  Date-Modified={2014-06-06 17:08:35 +0000},
  Doi={http://dx.doi.org/10.1145/2559206.2581301},
  Keywords={Mobile devices, medical imaging, 3D navigation, volumetric imaging},
  Notes={6-page abstract + poster.},
  Pages={2341-2346},
  Pdfurl={http://hcitang.org/papers/2014-chi2014wip-medical-imaging.pdf},
  Publisher={ACM},
  Title={Medical Imaging Specialists and 3D: A Domain Perspective on Mobile 3D
    Interactions},
  Type={poster},
  Url={http://hcitang.org/papers/2014-chi2014wip-medical-imaging-poster.pdf},
  Year={2014},
  Bdsk-Url-1={http://hcitang.org/papers/2014-chi2014wip-medical-imaging-poster.pdf},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2559206.2581301}
}

@techreport{neustaedter2004interpersonalawareness,
  Address={Department of Computer Science, University of Calgary, Calgary, Alberta,
    Canada T2N 1N4},
  Author={Neustaedter, Carman and Elliot, Kathryn and Tang, Anthony and Greenberg,
    Saul},
  Date-Added={2014-01-24 04:02:35 +0000},
  Date-Modified={2014-01-24 04:04:07 +0000},
  Institution={University of Calgary},
  Keywords={Interpersonal awareness, ubiquitous groupware},
  Month={October},
  Number={2004-760-25},
  Pdfurl={http://dspace.ucalgary.ca/bitstream/1880/45886/2/2004-760-25.pdf},
  Title={Where Are You and When Are You Coming Home? Foundations of Interpersonal
    Awareness},
  Type={techreport},
  Url={http://hdl.handle.net/1880/45886},
  Year={2004},
  Publisher={Department of Computer Science, University of Calgary},
  Bdsk-Url-1={http://hdl.handle.net/1880/45886}
}

@techreport{tang2006reference,
  Address={Department of Computer Science, University of British Columbia, Vancouver,
    British Columbia, Canada V6T 1Z4},
  Annote={Usable digital tabletop design hinges on a deep understanding of people's
    natural work practices over traditional tables. We present an ethnographic
    study of engineering project teams that highlights the use of reference material---artifacts
    not the primary product or focus of work activity, but referred to or inspected
    while the work activity is carried out---in tabletop work. We show how the
    variety of reference material forms and their role in tabletop work suggest
    that digital tabletop systems must recognize external artifacts and should
    allow reconfiguration of external work surfaces and information. },
  Author={Tang, Anthony and Fels, Sid},
  Date-Added={2014-01-24 03:59:13 +0000},
  Date-Modified={2014-01-24 04:00:26 +0000},
  Institution={University of British Columbia},
  Keywords={Tabletop groupware, collaboration, reference material},
  Month={July},
  Number={TR-2008-08},
  Pdfurl={http://www.cs.ubc.ca/cgi-bin/tr/2006/TR-2006-05.pdf},
  Publisher={Department of Computer Science, University of British Columbia},
  Title={"What I Want, Where I Want:" Reference Material Use in Tabletop Work},
  Type={techreport},
  Url={http://www.cs.ubc.ca/cgi-bin/tr/2006/TR-2006-05},
  Year={2006},
  Bdsk-Url-1={http://www.cs.ubc.ca/cgi-bin/tr/2006/TR-2006-05}
}

@techreport{tang2008activityslittear,
  Abstract={In prior work, we introduced a visualization technique for analyzing
    fixed position video streams called slit-tear visualizations. This technique
    supports exploratory data analysis by interactively generating views about
    the video stream that can provide insight into the spatial/temporal relationships
    of the entities contained within. These insights are necessarily grounded in
    context of the specific video being analyzed, and in this paper, we provide
    a general typology of the kinds of slit-tears an analyst may use. Further,
    we discuss the kinds of analytic primitives that often signal relevant events
    given these slit-tear types. The work is relevant to human-centered computing
    because the technique provides the most insight in the presence of human interpretation.},
  Address={Department of Computer Science, University of British Columbia, Vancouver,
    British Columbia, Canada V6T 1Z4},
  Author={Tang, Anthony and Lanir, Joel and Greenberg, Saul and Fels, Sid},
  Date-Added={2014-01-24 03:56:26 +0000},
  Date-Modified={2014-01-24 03:58:49 +0000},
  Institution={University of British Columbia},
  Keywords={video analysis, exploratory data analysis, information visualization,
    video history},
  Month={July},
  Number={TR-2008-08},
  Pdfurl={http://www.cs.ubc.ca/cgi-bin/tr/2008/TR-2008-08.pdf},
  Publisher={Department of Computer Science, University of British Columbia},
  Title={Uncovering Activity and Patterns in Video using Slit-Tear Visualizations},
  Type={techreport},
  Url={http://www.cs.ubc.ca/cgi-bin/tr/2008/TR-2008-08},
  Year={2008},
  Bdsk-Url-1={http://www.cs.ubc.ca/cgi-bin/tr/2008/TR-2008-08}
}

@techreport{ledo2012onespace,
  Address={Department of Computer Science, University of Calgary, Calgary, Alberta,
    Canada T2N 1N4},
  Author={Ledo, David and Aseniero, Bon Adriel and Boring, Sebastian and Tang,
    Anthony},
  Date-Added={2014-01-24 03:50:40 +0000},
  Date-Modified={2014-01-24 03:51:43 +0000},
  Institution={University of Calgary},
  Month={December},
  Number={2012-1033-16},
  Title={OneSpace: Shared Depth-Corrected Video Interaction},
  Type={techreport},
  Year={2012},
  Publisher={Department of Computer Science, University of Calgary},
  PdfUrl={http://hcitang.org/papers/2012-tr-onespace-2012-1033-16.pdf}
}

@inproceedings{tang2005awarenessmpg,
  Abstract={Mixed presence groupware (MPG) is software that connects both collocated
    and distributed collaborators together in a shared visual workspace. Our early
    study of this new genre is that people focus their collaborative energy on
    collocated partners at the expense of remote partners, which imbalances collaboration.
    We call this problem presence disparity, caused by the imbalance of awareness
    exuded by virtual embodiments versus actual people. VideoArms is an embodiment
    technique that mitigates presence disparity by enhancing awareness of remote
    collaborators in a mixed presence workspace. We describe how VideoArms works,
    and the design principles behind its construction},
  Author={Tang, Anthony and Greenberg, Saul},
  Booktitle={Awareness Systems: Known Results, Theory, Concepts and Future Challenges
    - Workshop at CHI 2005},
  Date-Added={2014-01-22 04:45:45 +0000},
  Date-Modified={2014-01-22 04:49:06 +0000},
  Editor={Paulos Markopoulos and Boris De Ruyter and Wendy Mackay},
  Keywords={Mixed presence groupware, awareness, consequential communication, embodiments,
    gestures},
  Pdfurl={http://hcitang.org/papers/2005-chi2005workshop-awareness-in-mpg.pdf},
  Title={Supporting Awareness in Mixed Presence Groupware},
  Type={workshop},
  Url={http://www.awareness-research.org/WS_programme.html},
  Year={2005},
  Bdsk-Url-1={http://www.awareness-research.org/WS_programme.html}
}

@inproceedings{tang2006egocentric,
  Abstract={Instant messaging (IM) allows us to maintain relationships with our
    social network through messaging and status information. We present early iterations
    of visualizations of IM interactions that help to visually identify several
    different types of relationships, such as intimate socials, long-lost-friend,
    and asymmetric relationships. Our work is motivated by an interest in designing
    awareness systems that can help reflect or even affect our desired social relationships.},
  Author={Tang, Anthony and Neustaedter, Carman},
  Booktitle={Social Visualization: Exploring Text, Audio and Video Interactions},
  Date-Added={2014-01-22 04:44:44 +0000},
  Date-Modified={2014-01-22 04:45:35 +0000},
  Pdfurl={http://hcitang.org/papers/2006-chi2006-visualizing-social-relationships-in-im.pdf},
  Title={Visualizing Egocentric Relationships in Instant Messaging},
  Type={workshop},
  Url={http://social.cs.uiuc.edu/soc-viz.html},
  Year={2006},
  Bdsk-Url-1={http://social.cs.uiuc.edu/soc-viz.html}
}

@inproceedings{agrawala2006porn,
  Abstract={As an extreme case of browsing, the pornographic browsing experience
    has several unique UI characteristics: it requires simple, lightweight controls;
    usage needs to be discrete; users' mental and physical context need to be respected,
    and common, repeated interactions need to be supported. While we identify design
    goals for user interfaces to better support browsing of pornographic images
    and movies, the same goals are applicable to other non-controversial browsing
    activities},
  Author={Agrawala, Anand and Tang, Anthony and Greenberg, Saul},
  Booktitle={Sexual Interactions - Workshop at CHI 2006},
  Date-Added={2014-01-22 04:43:19 +0000},
  Date-Modified={2014-01-22 04:44:28 +0000},
  Editor={Joanna Brewer and Jofish Kaye and Amanda Williams and Susan Wyche},
  Pdfurl={http://hcitang.org/papers/2006-chi2006-browsing-porn.pdf},
  Title={Browsing Pornography: An Interface Design Perspective},
  Type={workshop},
  Url={http://www.ics.uci.edu/~johannab/sexual.interactions.2006/chi2006.sex.FRONT.htm},
  Year={2006},
  Bdsk-Url-1={http://www.ics.uci.edu/~johannab/sexual.interactions.2006/chi2006.sex.FRONT.htm}
}

@inproceedings{tang2008bystandersworkshop,
  Abstract={In this paper, we reflect on the design and deployment process of MAGICBoard,
    a public display deployed in a university setting that solicits the electronic
    votes and opinions of bystanders on trivial but amusing topics. We focus on
    the consequences of our design choices with respect to encouraging bystanders
    to interact with the public display. Bystanders are individuals around the
    large display who may never fully engage with the application itself, but are
    potential contributors to the system. Drawing on our recent experiences with
    MAGICBoard, we present a classification of bystanders, and then discuss three
    design themes relevant to the design of systems for bystander use: graduated
    proximal engagement, lowering barriers for interaction and supporting covert
    engagement.},
  Author={Tang, Anthony and Finke, Matthias and Blackstock, Michael and Leung,
    Rock and Deutscher, Meghan and Tain, Gavin and Giesbrecht Catherine},
  Booktitle={Designing and Evaluation Mobile Phone-based Interaction with Public
    Displays},
  Date-Added={2014-01-22 04:41:08 +0000},
  Date-Modified={2014-01-22 04:43:02 +0000},
  Editor={Corina Sas and Alan Dix},
  Pdfurl={http://hcitang.org/papers/2008-chi2008-workshop-design-for-bystanders.pdf},
  Title={Designing for Bystanders: Reflections on Building a Public Digital Forum},
  Type={workshop},
  Url={http://www.comp.lancs.ac.uk/~corina/CHI08Workshop/},
  Year={2008},
  Bdsk-Url-1={http://www.comp.lancs.ac.uk/~corina/CHI08Workshop/}
}

@inproceedings{tang2008fourlessons,
  Abstract={While many researchers are interested in developing and designing technologies
    for multiple-display environments (MDEs), a core problem remains: we do not
    fully understand the role these display technologies can play in real-world
    activities. Our approach has been to study traditional MDEs (i.e. offices and
    laboratory environments) to understand both the tasks supported by traditional
    displays, and the roles the displays play in these tasks. We discuss a set
    of lessons from studies of traditional displays, and discuss how designers
    of MDEs can learn from these lessons in their designs. In so doing, we contribute
    to the growing understanding of the potential role of MDEs in supporting real-world
    work, and MDE design.},
  Author={Tang, Anthony and Fels, Sid},
  Booktitle={Beyond the Laboratory: Supporting Authentic Collaboration with Multiple
    Displays - Workshop at CSCW 2008},
  Date-Added={2014-01-22 04:39:22 +0000},
  Date-Modified={2014-01-22 04:40:59 +0000},
  Editor={Jacob Biehl and Gene Golovchinsky and Kent Lyons},
  Pdfurl={http://hcitang.org/papers/2008-cscw2008-workshop-four-lessons-for-mdes.pdf},
  Title={Four Lessons from Traditional MDEs},
  Type={workshop},
  Url={http://workshops.fxpal.com/cscw2008/},
  Year={2008},
  Bdsk-Url-1={http://workshops.fxpal.com/cscw2008/}
}

@inproceedings{tang2010expressiveness,
  Abstract={In the broad design space of telepresence systems, we are interested
    in contexts where users collaborate over a shared workspace. Our work involving
    connecting distributed touch surfaces (e.g. distributed tabletops) has shed
    light on the problem of supporting reference space---the ability of collaborators
    to point at, and refer to objects in the workspace. A promising approach to
    support this gestural communication has been to capture video of users' arms
    as they work over the surface, transmitting and overlaying that video at remote
    workstations. The problem with this approach is that the video image is a flat
    projection of users' 3D bodies, limiting the expressiveness of users' gestures,
    and occasionally providing false information. In our most recent work, we have
    begun exploring {\^a}{\euro}?non-photorealistic{\^a}{\euro}? visualizations
    of users' bodies to support expressiveness in reference space.},
  Author={Tang, Anthony and Genest, Aaron and Shoemaker, Garth and Gutwin, Carl
    and Fels, Sid and Booth, Kellogg S.},
  Booktitle={New Frontiers in Telepresence - CSCW 2010 Workshop},
  Date-Added={2014-01-22 04:37:38 +0000},
  Date-Modified={2014-01-22 04:39:14 +0000},
  Editor={Gina Venolia and Kori Inkpen and Judith Olson and David Nguyen},
  Pdfurl={http://hcitang.org/papers/2010-cscw2010-workshop-expressiveness.pdf},
  Title={Enhancing Expressiveness in Reference Space},
  Type={workshop},
  Url={http://research.microsoft.com/en-us/events/nft2010/},
  Year={2010},
  Bdsk-Url-1={http://research.microsoft.com/en-us/events/nft2010/}
}

@inproceedings{lanir2010studentcontrol,
  Author={Lanir, Joel and Booth, Kellog S. and Tang, Anthony},
  Booktitle={EIST 2010: Next Generation of HCI and Education: Workshop on UI Technology and
    Education Pedagogy - Workshop at CHI 2010},
  PdfUrl={http://hcitang.org/papers/2010-eist2010-student-control.pdf},
  Date-Added={2014-01-22 04:32:57 +0000},
  Date-Modified={2014-01-22 04:36:48 +0000},
  Editor={Edward Tse and Johannes Sch{\"o}ning and Yvonne Rogers and Chia Shen
    and Gerald Morrison},
  Title={Enabling Student Control of a Classroom's Shared Screen},
  Type={workshop},
  Year={2010}
}

@inproceedings{tang2011interstitial,
  Abstract={Multi-display environments comprise large shared displays as well as
    personal devices. In this work, we discuss how the interstitial space---the
    space between displays and de- vices---can also be made into an interactive
    space. Such a space can support collaborative data analysis by providing a
    focus+context workspace; providing a means to transition between collaborative
    and individual work, and by provid- ing a means to transition tasks between
    devices.},
  Author={Tang, Anthony and Irani, Pourang},
  Booktitle={DEXIS 2011 Workshop on Data Exploration for Interaction Surfaces -
    Workshop at ITS 2011},
  Date-Added={2014-01-22 04:30:53 +0000},
  Date-Modified={2014-01-22 04:32:12 +0000},
  Editor={Petra Isenberg and Sheelagh Carpendale and Tobias Hesselmann and Tobias
    Isenberg and Bongshin Lee},
  Pdfurl={http://hcitang.org/papers/2012-dexis-interstitial-space.pdf},
  Title={Interstitial Space in MDEs for Data Analysis},
  Type={workshop},
  Url={http://www.aviz.fr/dexis2011/pmwiki.php},
  Year={2011},
  Bdsk-Url-1={http://www.aviz.fr/dexis2011/pmwiki.php}
}

@inproceedings{neustaedter2012mrgames,
  Author={Neustaedter, Carman and Moulder, Vicky and Wakkary, Ron and Judge, Tejinder
    and Tang, Anthony},
  Booktitle={Mixed Reality Games - Workshop at CSCW 2012},
  Date-Added={2014-01-22 04:28:47 +0000},
  Date-Modified={2014-01-22 04:30:27 +0000},
  Editor={Elizabeth Bonsignore and Derek L. Hansen and Zachary O. Toups and Lennart
    E. Nacke and Anastasia Salter and Wayne Lutters},
  Pdfurl={http://hcitang.org/papers/2012-cscw2012-workshop-designing-mixed-reality-games.pdf},
  Title={Designing Mixed Reality Games to Study Culture, Family Practices and Social
    Engagement},
  Type={workshop},
  Url={http://mixedrealitygames.selfloud.net/},
  Year={2012},
  Bdsk-Url-1={http://mixedrealitygames.selfloud.net/}
}

@inproceedings{genest2012expressivenessmatters,
  Abstract={Crisis command centres often gather data from disparate locations and
    collect it for visualization on a large, shared screen. However, the resulting
    visualization often lacks expressiveness: it fails to express nuanced mediating
    characteristics about the information, such as specificity, urgency, awareness,
    or reliability. We suggest that our previous work on creating expressive realtime
    embodiments can inform the design of crisis management embodiments on large
    displays, improving communication and decision-making. We make several recommendations
    for future research directions},
  Author={Genest, Aaron and Bateman, Scott and Tang, Anthony and Scott, Stacey
    and Gutwin, Carl},
  Booktitle={Collaboration and Crisis Informatics - Workshop at CSCW 2012},
  Date-Added={2014-01-22 04:26:38 +0000},
  Date-Modified={2014-01-22 04:28:31 +0000},
  Editor={Volkmar Pipek and Jonas Landgren and Leysia Palen},
  Pdfurl={http://hcitang.org/papers/2012-cscw2012-workshop-expressiveness-in-c-and-c.pdf},
  Title={Why Expressive Matters in Command and Control Visualizations},
  Type={workshop},
  Url={http://crisisinformatics.wineme.fb5.uni-siegen.de/},
  Year={2012},
  Bdsk-Url-1={http://crisisinformatics.wineme.fb5.uni-siegen.de/}
}

@inproceedings{reilly2011organicui,
  Author={Reilly, Derek and Tang, Anthony and Wu, Andy and Echenique, Andy and
    Massey, Jonathan and Mathiasen, Niels and Mazalek, Ali and Edwards, W. Keith},
  Booktitle={Second International Workshop on Organic User Interfaces at TEI 2011},
  Date-Added={2014-01-22 04:16:01 +0000},
  Date-Modified={2014-01-22 04:24:50 +0000},
  Editor={Audrey Girouard and Roel Vertegaal and Ivan Poupyrev},
  Title={Organic UIs in Cross-Reality Spaces},
  Type={workshop},
  Url={http://www.organicui.org/workshop},
  PdfUrl={http://hcitang.org/papers/2011-tei2011workshop-organic-cross-reality.pdf},
  Year={2011},
  Bdsk-Url-1={http://www.organicui.org/workshop}
}

@inproceedings{perteneder2012ideaplayground,
  Abstract={Creativity and innovation are much sought-after qualities of indi-
    viduals and organizations, but existing creativity practises are not cohesively
    integrated with digital workflows or digital artefacts. We introduce a set
    of design considerations for digital systems to support creative processes,
    specifically supporting the three ongo- ing, iterative activities of creative
    processes: gathering inspiration, generating ideas, and refining ideas. We
    present Idea Playground, a system built upon these considerations that supports
    diverse input sources, synchronous and asynchronous use, and freeform information
    structuring.},
  Author={Peterender, Florian and Grossauer, Christian and Seifried, Thomas and
    Walney, Jagoda and Brosz, John and Tang, Anthony and Carpendale, Sheelagh and
    Haller, Michael},
  Booktitle={Designing Collaborative Interactive Spaces - Workshop at AVI 2012},
  Date-Added={2014-01-18 23:42:38 +0000},
  Date-Modified={2014-01-18 23:47:01 +0000},
  Editor={Hans-Christian Jetter and Florian Geyer and Harald Reiterer and Raimund
    Dachselt and Gerhard Fischer and Rainer Groh and Michael Haller and Thomas
    Herrmann},
  Keywords={Design process, brainstorming, interactive environment, interactive
    surface, pen input, multi-user input},
  Pdfurl={http://hcitang.org/papers/2012-avi2012workshop-idea-playground.pdf},
  Title={Idea Playground: When Brainstorming is Not Enough},
  Type={workshop},
  Year={2012}
}

@inproceedings{grossauer2012mathsketch,
  Abstract={Traditional whiteboards are a common medium for mathematics education,
    and are particularly suited to the high school and college level where conceptual
    understanding of the subject matter is emphasized above procedural understanding.
    Although considerable work has been done to apply sketch-based interaction
    to mathematics learning, very few have addressed this from the perspective
    of teaching mathematics in a conventional classroom environment. We provide
    a set of design considerations for dynamic whiteboards in instructional contexts
    and present an embodiment of these considerations in our prototype, MathSketch.},
  Author={Grossauer, Christian and Perteneder, Florian and Haller, Michael and
    Walny, Jagoda and Brosz, John and Tang, Anthony and Carpendale, Sheelagh},
  Booktitle={EIST 2012: Educational Interfaces, Software and Technology - Workshop
    at CHI 2012},
  Date-Added={2014-01-18 23:28:38 +0000},
  Date-Modified={2014-01-18 23:41:44 +0000},
  Editor={Edward Tse and Lynn V. Marentette and Syed Ishtianque Ahmed and Alex
    Thayler and Jochen Huber and Max M{\"u}hlh{\"a}user and Si Jung "Jun" Kim and
    Quincy Brown},
  Keywords={Electronic whiteboards, mathematics, education, sketching, representation},
  Pdfurl={http://hcitang.org/papers/2012-chi2012workshop-mathsketch.pdf},
  Title={MathSketch: Designing a Dynamic Whiteboard for Instructional Contexts},
  Type={workshop},
  Year={2012},
  PdfUrl={http://hcitang.org/papers/2012-chi2012workshop-mathsketch.pdf}
}

@inproceedings{grevet2012dinnerprototype,
  Abstract={We aim to explore the inherent social qualities around food consumption
    and the potential role of social systems in the kitchen. We present a prototype
    called FoodApp that allows a group of friends to share each other's dinnertime
    activities. This system is informed by prior work in domestic computing, social
    computing and technology probes design. The main features of the system are
    to support lightweight social cues about cooking and eating activity, as well
    as support for activity coincidences. Future studies of this system will explore
    whether social connectedness in the kitchen can encourage positive behaviors
    around food.},
  Author={Grevet, Catherine and Tang, Anthony and Mynatt, Elizabeth},
  Booktitle={Food and Interaction Design - Workshop at CHI 2012},
  Date-Added={2014-01-18 23:25:40 +0000},
  Date-Modified={2014-01-18 23:28:23 +0000},
  Editor={Rob Comber and Eva Ganglbauer and Jaz Hee-jeong Choi and Jettie Hoonhout
    and Yvonne Rogers and Kenton O'Hara and Julie Maitland},
  Keywords={Social computing, HCI, Food, Awareness},
  Pdfurl={http://hcitang.org/papers/2012-chi2012workshop-dinner-prototype.pdf},
  Title={Description of a Prototype for a Social Awareness System Used During Dinner},
  Type={workshop},
  Year={2012}
}

@inproceedings{polleti2012ecobalance,
  Abstract={Engaging people to regularly reflect on their own behavior is a crucial
    step towards changing it for the better. In this paper we present ECO|Balance,
    a set of interactive design metaphors for mobile devices that aim towards enticing
    continued engagement through using visual appeal and avoiding factors that
    chastise. As a bridge between data visualization and information art the idea
    is to keep people interested in repeatedly working with and analyzing their
    data. We chose the scenario of personal mobility and reducing one's carbon
    footprint and created a series of four animated designs: Pie Flow, Jelly Fish,
    Footprints and Organic Flowers that explore different approaches to representation
    and interaction. We discuss our main goals of long-term enticement through
    visual appeal and subtlety. },
  Author={Polleti, Julia and Baur, Dominikus and Tang, Anthony and Carpendale,
    Sheelagh},
  Booktitle={Personal Informatics in Practice: Improving Quality of Life Through
    Data - Workshop at CHI 2012},
  Date-Added={2014-01-18 23:23:54 +0000},
  Date-Modified={2014-01-18 23:25:27 +0000},
  Editor={Ian Li and Yevgeniy Medynskiy and Jon Froehlich and Jakob Eg Larsen},
  Keywords={Persuasive computing; mobile; personal informatics},
  Pdfurl={http://hcitang.org/papers/2012-chi2012workshop-ecobalance.pdf},
  Title={ECO|Balance - Exploring Design Issues for Mobile Persuasion},
  Type={workshop},
  Year={2012}
}

@inproceedings{aseniero2012deeppersonalization,
  Abstract={Personal informatics (PI) tools that support reflection are ``personalized''
    insofar as the data consists of an individual's data. Typically, this data
    is presented in visualizations that are generic and non-individuated. In this
    paper, we argue for deep personalization, where the visualizations are constructed
    by individuals. Such functionality gives individuals the power to build visualizations
    that are personally meaningful, allowing them to meet and address personal
    needs. We illustrate the power of this approach by considering a case study
    of the design of a multi-faceted reflection tool. Reflecting on the deep personalization
    of the design process, we propose an approach that will allow individuals to
    personalize their own visualizations.},
  Author={Aseniero, Bon Adriel and Carpendale, Sheelagh and Tang, Anthony},
  Booktitle={Personal Informatics in Practice: Improving Quality of Life Through
    Data - Workshop at CHI 2012},
  Date-Added={2014-01-18 23:19:27 +0000},
  Date-Modified={2014-01-18 23:23:16 +0000},
  Editor={Ian Li and Yevgeniy Medynskiy and Jon Froehlich and Jakob Eg Larsen},
  Keywords={Deep personalization, Self-awareness, Aesthetic Design, Information
    Visualization, Feedback techniques},
  Pdfurl={http://hcitang.org/papers/2012-chi2012workshop-deep-personalization.pdf},
  Title={Deep Personalization in Tools for Reflection},
  Type={workshop},
  Year={2012}
}

@inproceedings{macleod2013patientdata,
  Author={MacLeod, Haley and Tang, Anthony},
  Booktitle={Patient-Clinician Communication - Workshop at CHI 2013},
  Date-Added={2014-01-18 23:14:10 +0000},
  Date-Modified={2014-01-18 23:19:12 +0000},
  Editor={Rupa Patel and Lauren Wilcox and Anthony Back and Mary Czerwinski and
    Eric Horvitz and Paul Gorman and Wanda Pratt},
  Pdfurl={http://hcitang.org/papers/2013-chi2013workshop-patient-data.pdf},
  Title={Shared Displays for Facilitating Discussions on Patient Data},
  Type={workshop},
  Year={2013}
}

@inproceedings{ledo2013onespaceworkshop,
  Abstract={Video conferencing commonly employs a video portal met- aphor to connect
    individuals from remote spaces. In this work, we explore an alternate metaphor,
    a shared depth- mirror, where video images of two spaces are fused into a single
    shared, depth-corrected video space. We realize this metaphor in OneSpace,
    where the space respects virtual spatial relationships between people and objects
    as if all parties were looking at a mirror together. We report prelim- inary
    observations of OneSpace's use, noting that it encour- ages cross-site, full-body
    interactions, and that participants employed the depth cues in their interactions.
    Based on these observations, we argue that the depth mirror offers new opportunities
    for shared video interaction in the form of a shared stage.},
  Author={Ledo, David and Aseniero, Bon Adriel and Boring, Sebastian and Greenberg,
    Saul and Tang, Anthony},
  Booktitle={Future of Personal Video Communications: Beyond Talking Heads - Workshop
    at CHI 2013},
  Date-Added={2014-01-18 23:09:42 +0000},
  Date-Modified={2014-01-18 23:13:58 +0000},
  Editor={Erick Oduor and Carman Neustaedter and Gina Venolia and Tejinder Judge},
  Keywords={Video communication; media spaces},
  Pdfurl={http://hcitang.org/papers/2013-chi2013workshop-onespace.pdf},
  Title={OneSpace: Bringing Depth to Remote Interactions},
  Type={workshop},
  Year={2013}
}

@inproceedings{macleod2012personalinformatics,
  Abstract={Personal informatics tools allow individuals to collect and reflect
    on data related to their personal lives. While consumer tools have mainly focused
    on everyday aspects of life, such as physical activity or personal finance,
    they may also provide value to individuals suffering from chronic health conditions.
    These individuals may have unique needs not yet addressed through the current
    design of personal informatics tools. We address this problem through an interview
    study with 15 individuals focused on those with chronic health conditions,
    in which we explored how data collection supported the management of their
    illnesses. We found that data collection gives these individuals a means to
    understand their conditions, as well as a mechanism to manage them.},
  Author={MacLeod, Haley and Tang, Anthony},
  Booktitle={WISH 2012: Workshop on Interactive Systems in Healthcare},
  Date-Added={2014-01-18 23:06:03 +0000},
  Date-Modified={2014-01-18 23:08:55 +0000},
  Pdfurl={http://hcitang.org/papers/2012-wish2012poster-personal-informatics.pdf},
  Title={Understanding Personal Informatics Needs of Individuals with Chronic Health
    Conditions},
  Type={poster},
  Year={2012}
}

@inproceedings{yarosh2013sharetablevideo,
  Author={Yarosh, Svetlana and Tang, Anthony and Mokashi, Sanika and Abowd, Gregory},
  Booktitle={CSCW 2013 Video Showcase Program},
  Date-Added={2014-01-18 21:02:23 +0000},
  Date-Modified={2014-01-18 21:04:54 +0000},
  Notes={People's choice award},
  Title={"Almost Touching": Parent-Child Remote Communication Using the ShareTable
    System - The Video},
  Type={video},
  Videourl={https://www.youtube.com/watch?v=n8k5mYbCXhs},
  Year={2013}
}

@inproceedings{procyk2014sharedgeocachingvideo,
  Abstract={Shared geocaching is an outdoor activity where pairs of individuals
    geocache together but in different locations. Video streaming allows two players
    to see each remote person's view and converse during the activity. This allows
    players to help each other out while searching for geocaches. We envision that
    shared geocaching will provide a way for family or friends to share experiences
    together over distance where they are both participating in the same activity
    at the same time, only in different locations.},
  Author={Procyk, Jason and Neustaedter, Carman and Pang, Carolyn and Tang, Anthony
    and Judge, Tejinder K.},
  Booktitle={CSCW 2014 Video Program},
  Date-Added={2014-01-18 20:32:35 +0000},
  Date-Modified={2014-06-06 17:08:39 +0000},
  Doi={http://dx.doi.org/10.1145/2556420.2557635},
  Notes={video + 4 page abstract},
  Pdfurl={http://hcitang.org/papers/2014-cscw2014videos-shared-geocaching.pdf},
  Publisher={ACM},
  Title={Shared Geocaching Over Distance with Mobile Video Streaming},
  Type={video},
  Videourl={http://hcitang.org/papers/2014-cscw2014videos-shared-geocaching.mov},
  Year={2014},
  Bdsk-Url-1={http://dx.doi.org/10.1145/2556420.2557635}
}

@inproceedings{reilly2010twinspace,
  Author={Reilly, Derek and Tang, Anthony and Wu, Andy and Echenique, Andy and
    Chamoli, Shashank and Massey, Jonathan and Edwards, Warren Keith},
  Booktitle={UIST 2010 Demos: Demonstration Program of ACM Symposium on User Interface
    Technology (UIST) 2010},
  Date-Added={2014-01-18 20:26:14 +0000},
  Date-Modified={2014-01-18 20:28:08 +0000},
  Title={Or de l'Acadie: a Twinspace Demo},
  Type={demo},
  Year={2010}
}

@inproceedings{procyk2014sharedgeocaching,
  Acceptance={22.8{\%} - 471/2064},
  Author={Procyk, Jason and Neustaedter, Carman and Pang, Carolyn and Tang, Anthony
    and Judge, Tejinder K.},
  Booktitle={CHI '14 Proceedings of the SIGCHI Conference on Human Factors in Computing
    Systems},
  Date-Added={2014-01-18 20:24:50 +0000},
  Date-Modified={2014-06-06 17:06:58 +0000},
  Doi={http://dx.doi.org/10.1145/2556288.2557198},
  Pages={2163-2172},
  Pdfurl={http://hcitang.org/papers/2014-chi2014-video-streaming.pdf},
  Publisher={ACM},
  Title={Exploring Video Streaming in Public Settings: Shared Geocaching Over Distance
    Using Mobile Video Chat},
  Type={conference},
  Year={2014},
  Bdsk-Url-1={http://dx.doi.org/10.1145/2556288.2557198}
}

@inproceedings{hunter2014waazaam,
  Abstract={We present the design, implementation and evaluation of WaaZam, a video
    mediated communication system designed to support creative play in personalized
    environments. Users can interact together in sets composed of digital assets
    layered in 3D space. The goal of the project is to support creative play and
    increase social engagement during video sessions of geographically separated
    families. We focus particularly on understanding the value of customization
    for families with children ages 6-12. We present interviews with creativity
    experts, a pilot study and a formal evaluation of families playing together
    in four conditions: separate windows, merged windows, digital play sets, and
    personalized digital environments. We found that playing in the same video
    space enables new activities and increases social engagement for families.
    Personalization allows families to customize environments for their needs and
    supports more creative play activities that embody the imagination of the child.},
  Acceptance={22.8{\%} - 471/2064},
  Author={Hunter, Seth and Maes, Pattie and Tang, Anthony and Inkpen, Kori},
  Booktitle={CHI '14 Proceedings of the SIGCHI Conference on Human Factors in Computing
    Systems},
  Date-Added={2014-01-18 20:23:27 +0000},
  Date-Modified={2014-06-06 17:07:48 +0000},
  Doi={http://dx.doi.org/10.1145/2556288.2557382},
  Keywords={Video mediated communication; customized video environments; composited
    video; family play; remote play; shared experiences at a distance; play at
    a distance},
  Notes={Honourable Mention - Top 5{\%} of all submissions},
  Pages={1197-1206},
  Pdfurl={http://hcitang.org/papers/2014-chi2014-waazam.pdf},
  Publisher={ACM},
  Title={WaaZaam! Supporting Creative Play at a Distance in Customized Video Environments},
  Type={conference},
  Year={2014},
  Bdsk-Url-1={http://dx.doi.org/10.1145/2556288.2557382}
}

@inproceedings{cohen2014onespace,
  Abstract={Children engage in free play for emotional, physical and social development;
    researchers have explored supporting free play between physically remote playmates
    using videoconferencing tools. We show that the configuration of the video
    conferencing setup affects play. Specifically, we show that a shared visual
    scene configuration promotes fundamentally active forms of engaged, co-operative
    play.},
  Acceptance={22.8{\%} - 471/2064},
  Author={Cohen, Maayan and Dillman, Kody and MacLeod, Haley and Hunter, Seth and
    Tang, Anthony},
  Booktitle={CHI '14 Proceedings of the SIGCHI Conference on Human Factors in Computing
    Systems},
  Date-Added={2014-01-18 20:20:57 +0000},
  Date-Modified={2014-06-06 17:07:53 +0000},
  Doi={http://dx.doi.org/10.1145/2556288.2557117},
  Notes={Honourable Mention - Top 5{\%} of all submissions},
  Pages={2177-2180},
  Pdfurl={http://hcitang.org/papers/2014-chi2014-freeplay.pdf},
  Publisher={ACM},
  Title={OneSpace: Shared Visual Scenes for Active Freeplay},
  Type={conference},
  Videourl={http://hcitang.org/papers/2014-chi2014-freeplay-in-onespace.mov},
  Year={2014},
  Bdsk-Url-1={http://dx.doi.org/10.1145/2556288.2557117}
}

@inproceedings{macleod2013towards,
  Abstract={Many people with chronic illness suffer from debilitating symptoms
    that inhibit normal day-to-day function. It is unclear how to design tools
    to support this -- many see self-tracking tools as a burden to use. We report
    here on an interview study with 12 individuals with chronic illnesses who collect
    data about their conditions. We reflect on ways to support the design of tools
    that will be more easily adopted by engaging curiosity, self-discovery and
    exploration rather than focusing on behavior change. },
  Author={MacLeod, Haley and Tang, Anthony and Carpendale, Sheelagh},
  Booktitle={GRAND 2013: RNote Proceedings of the GRAND NCE 2013 Meeting},
  Date-Added={2014-01-17 05:52:13 +0000},
  Date-Modified={2014-01-17 05:53:11 +0000},
  Pdfurl={http://hcitang.org/papers/2013-grand2013-towards-personal-informatics-tools.pdf},
  Title={Towards Personal Informatics Tools for Chronic Illness Management},
  Year={2013}
}

@inproceedings{tang2013annotation,
  Abstract={Just as we annotate digital documents with digital annotations for
    collaborative work, we frequently annotate physical objects using physical
    annotations (e.g. by using Post-It notes). In the physical world, we are limited
    by the size of the physical Post-It note, and further, too many Post-It notes
    clutter the physical space. In this work, we explore the use of handheld projectors
    combined with a tablet to create digital annotations for physical objects,
    and to visualize these annotations around such physical objects. Our design
    allows people to use a flashlight metaphor for visualizing digital ``post-it''
    notes, which can be created in-place by pointing the projector at an object,
    and then adding the annotation using the tablet. We realized this design in
    a prototype to informally assess the effectiveness of the metaphors we used,
    and to gather suggestions for future work in this area},
  Author={Tang, Richard and Tang, Anthony},
  Booktitle={GRAND 2013: RNote Proceedings of the GRAND NCE 2013 Meeting},
  Date-Added={2014-01-17 05:50:19 +0000},
  Date-Modified={2014-01-17 05:51:35 +0000},
  Pdfurl={http://hcitang.org/papers/2013-grand2013-in-place-annotation.pdf},
  Title={In-Place Annotation of Physical Objects with Pico-Projectors},
  Year={2013}
}

@inproceedings{weigel2013focus,
  Abstract={Focus plus context displays combine high-resolution detail and lower-resolution
    overview using displays of different pixel densities. Historically, they employed
    two fixed-size displays of different resolutions, one embedded within the other.
    In this paper, we explore focus plus context displays using one or more mobile
    projectors in combination with a stationary display. The portability of mobile
    projectors as applied to focus plus context displays contributes in three ways.
    First, the projector's projection on the stationary display can transition
    dynamically from being the focus of one's interest (i.e. providing a high resolution
    view when close to the display) to providing context around it (i.e. providing
    a low resolution view beyond the display's borders when further away from it).
    Second, users can dynamically reposition and resize a focal area that matches
    their interest rather than repositioning all content into a fixed high-resolution
    area. Third, multiple users can manipulate multiple foci or context areas without
    interfering with one other. A proof-of-concept implementation illustrates these
    contributions. },
  Author={Weigel, Martin and Boring, Sebastian and Marquardt, Nicolai and Steimle,
    Juergen and Greenberg, Saul and Tang, Anthony},
  Booktitle={GRAND 2013: RNote Proceedings of the GRAND NCE 2013 Meeting},
  Date-Added={2014-01-17 05:47:05 +0000},
  Date-Modified={2014-01-17 05:49:59 +0000},
  Keywords={Focus plus context, portable projectors, multiple users, multiple displays},
  Notes={Honourable mention},
  Pdfurl={http://hcitang.org/papers/2013-grand2013-focus-to-context-and-back.pdf},
  Title={From Focus to Context and Back: Combining Mobile Projectors and Stationary
    Displays},
  Year={2013}
}

@techreport{dillman2015bodyasworkspace,
  Abstract={Many common injuries can be treated effectively with physiotherapy, but accessing this treatment is difficult for those in rural locations. We seek to design video-based systems to support remote physiotherapy, so patients can access and engage with therapy and a professional from their homes. We conducted design sessions with practicing physiotherapists to iteratively design and build technology sketches to understand communication challenges and practices for remote therapy. Our analysis of these sessions reveals new challenges in designing video media space tools for telerehabilitation. Chief among these lessons: supporting body-based communication between therapist and patient is challenging because the object of conversation is the patient’s body rather than an external object that can be manipulated.},
  Author={Dillman, Kody and Tang, Anthony},
  Institution={University of Calgary},
  Keywords={Physiotherapy, remote collaboration, telerehabilitation},
  Month={April},
  Number={2015-1071-04},
  Pdfurl={http://dspace.ucalgary.ca/bitstream/1880/50412/1/2015-1071-04.pdf},
  Publisher={Department of Computer Science, University of Calgary},
  Title={Body as a Workspace: Design for Remote Physiotherapy},
  Type={techreport},
  Year={2015},
}  

@techreport{dillman2013remotephysiotherapy,
  Abstract={Many common injuries can be treated effectively withphysiotherapy,
    but getting access to this treatment isdifficult for those living in remote
    or rural locations. Tohelp formulate design requirements for next-generationtools
    for supporting remote physiotherapy (i.e.telerehabilitation), we conducted
    design sessions with fivepracticing physiotherapists. We developed three technologyprobes
    as prompts for these discussions, helping us to gainan understanding of physiotherapists'
    activities andcommunication practices. Our analysis shows thattelerehabilitation
    tools should be specifically designed toaddress communication and work in relation
    to clients'physical bodies, and that visual communication can beenhanced through
    augmentation to videoconferencing toolsand accessible hardware to account for
    a lack of tactilecommunication.},
  Author={Dillman, Kody and Tang, Anthony},
  Citation_Abstract_Html_Url={http://dspace.ucalgary.ca/jspui/handle/1880/49845},
  Citation_Authors={Tang, Anthony; Dillman, Kody},
  Citation_Date={2013-09-25},
  Citation_Keywords={Remote Physiotherapy, injuries; Technical Report},
  Citation_Language={eng},
  Citation_Pdf_Url={http://dspace.ucalgary.ca/jspui/bitstream/1880/49845/1/2013-1048-15.pdf},
  Citation_Title={Towards Next-Generation Remote Physiotherapy with Videoconferencing
    Tools},
  Date-Added={2014-01-16 05:18:04 +0000},
  Date-Modified={2014-01-16 05:27:39 +0000},
  Dc.Identifier={2013-1048-15},
  Dc.Language={eng},
  Dc.Type={Technical Report},
  Dcterms.Available={2013-09-25T19:34:33Z},
  Dcterms.Dateaccepted={2013-09-25T19:34:33Z},
  Dcterms.Issued={2013-09-25},
  Generator={DSpace},
  Institution={University of Calgary},
  Keywords={Videoconferencing Tools, Telerehabilitation},
  Month={September},
  Number={2013-1048-15},
  Pdfurl={http://dspace.ucalgary.ca/jspui/bitstream/1880/49845/1/2013-1048-15.pdf},
  Publisher={Department of Computer Science, University of Calgary},
  Title={Towards Next-Generation Remote Physiotherapy with Videoconferencing Tools},
  Type={techreport},
  Url={http://hdl.handle.net/1880/49845},
  Year={2013},
  Bdsk-Url-1={http://hdl.handle.net/1880/49845}
}

@inproceedings{boring2012fat,
  Abstract={Modern mobile devices allow a rich set of multi-finger interactions
    that combine modes into a single fluid act, for example, one finger for panning
    blending into a two-finger pinch gesture for zooming. Such gestures require
    the use of both hands: one holding the device while the other is interacting.
    While on the go, however, only one hand may be available to both hold the device
    and interact with it. This mostly limits interaction to a single-touch (i.e.,
    the thumb), forcing users to switch between input modes explicitly. In this
    paper, we contribute the Fat Thumb interaction technique, which uses the thumb's
    contact size as a form of simulated pressure. This adds a degree of freedom,
    which can be used, for example, to integrate panning and zooming into a single
    interaction. Contact size determines the mode (i.e., panning with a small size,
    zooming with a large one), while thumb movement performs the selected mode.
    We discuss nuances of the Fat Thumb based on the thumb's limited operational
    range and motor skills when that hand holds the device. We compared Fat Thumb
    to three alternative techniques, where people had to precisely pan and zoom
    to a predefined region on a map and found that the Fat Thumb technique compared
    well to existing techniques.},
  Acceptance={25{\%} - 54/212},
  Acmid={2371582},
  Address={New York, NY, USA},
  Author={Boring, Sebastian and Ledo, David and Chen, Xiang 'Anthony' and Marquardt,
    Nicolai and Tang, Anthony and Greenberg, Saul},
  Booktitle={Proceedings of the 14th International Conference on Human-computer
    Interaction with Mobile Devices and Services},
  Date-Added={2014-01-11 20:51:34 +0000},
  Date-Modified={2014-01-11 20:54:23 +0000},
  Doi={http://doi.acm.org/10.1145/2371574.2371582},
  Isbn={978-1-4503-1105-2},
  Keywords={mobile device, single-handed interaction, touch-screen},
  Location={San Francisco, California, USA},
  Numpages={10},
  Pages={39--48},
  Publisher={ACM},
  Series={MobileHCI '12},
  Title={The Fat Thumb: Using the Thumb's Contact Size for Single-handed Mobile
    Interaction},
  Type={conference},
  Videourl={http://hcitang.org/papers/2012-mobilehci2012-fatthumb.mp4},
  Year={2012},
  PdfUrl={http://hcitang.org/papers/2012-mobilehci2012-fatthumb.pdf},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2371574.2371582},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2371574.2371582}
}

@inproceedings{boring2012fatdemo,
  Abstract={Modern mobile devices allow a rich set of multi-finger interactions
    that combine modes into a single fluid act, for example, one finger for panning
    blending into a two-finger pinch gesture for zooming. Such gestures require
    the use of both hands: one holding the device while the other is interacting.
    While on the go, however, only one hand may be available to both hold the device
    and interact with it. This mostly limits interaction to a single-touch (i.e.,
    the thumb), forcing users to switch between input modes explicitly. In this
    paper, we contribute the Fat Thumb interaction technique, which uses the thumb's
    contact size as a form of simulated pressure. This adds a degree of freedom,
    which can be used, for example, to integrate panning and zooming into a single
    interaction. Contact size determines the mode (i.e., panning with a small size,
    zooming with a large one), while thumb movement performs the selected mode.
    We discuss nuances of the Fat Thumb based on the thumb's limited operational
    range and motor skills when that hand holds the device. We compared Fat Thumb
    to three alternative techniques, where people had to precisely pan and zoom
    to a predefined region on a map and found that the Fat Thumb technique compared
    well to existing techniques.},
  Address={New York, NY, USA},
  Author={Boring, Sebastian and Ledo, David and Chen, Xiang 'Anthony' and Marquardt,
    Nicolai and Tang, Anthony and Greenberg, Saul},
  Booktitle={Proceedings of the 14th International Conference on Human-computer
    Interaction with Mobile Devices and Services Companion},
  Date-Added={2014-01-11 20:50:16 +0000},
  Date-Modified={2014-01-11 20:53:42 +0000},
  Doi={http://doi.acm.org/10.1145/2371664.2371711},
  Isbn={978-1-4503-1443-5},
  Keywords={mobile device, single-handed interaction, touch-screen},
  Location={San Francisco, California, USA},
  Numpages={2},
  Pages={207--208},
  Publisher={ACM},
  Series={MobileHCI '12},
  Title={The Fat Thumb: Using the Thumb's Contact Size for Single-handed Mobile
    Interaction},
  Type={demo},
  Videourl={http://hcitang.org/papers/2012-mobilehci2012-fatthumb.mp4},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2371664.2371711},
  Bdsk-Url-2={http://dx.doi.org/10.1145/2371664.2371711}
}

@article{neustaedter2013creating,
  Abstract={Location-based games seek to move computer gaming out from behind the
    PC and into the ``real world'' of cities, streets, parks, and other locations.
    This real-world physicality makes the experience fun for game players, yet
    it brings the unique challenge of creating and orchestrating such a game. That
    is, location-based games are often difficult to create, grow, and maintain
    over long periods of time. Our research investigates how location-based games
    can be designed to overcome this challenge of scalability. We studied the well-established
    location-based game of Geocaching through active participation and an online
    survey to better understand how it has succeeded in maintaining user involvement
    and growth over the last decade. Findings show that Geocaching benefits by
    having players directly create game content, including both lightweight and
    elaborate creations. Geocaching has also made it simple for players to perform
    game orchestration by monitoring game content, other players, and even non-players.
    We then characterize location-based games according to various attributes and
    suggest how the lessons learned from Geocaching could be applied more generally
    to the design of other location-based games and in which cases they should
    not be applied.},
  Address={London, UK, UK},
  Author={Neustaedter, Carman and Tang, Anthony and Judge, Tejinder K.},
  Date-Modified={2014-01-11 07:00:43 +0000},
  Doi={http://dx.doi.org/10.1007/s00779-011-0497-7},
  Issn={1617-4909},
  Journal={Personal Ubiquitous Comput.},
  Keywords={Geocaching, Pervasive games, Location-based games, Community, Global
    positioning system (GPS)},
  Number={2},
  Pages={335--349},
  Pdfurl={http://hcitang.org/papers/2013-puc-scalable-location-based-games.pdf},
  Publisher={Springer-Verlag},
  Title={Creating scalable location-based games: lessons from Geocaching},
  Type={journal},
  Volume={17},
  Year={2013},
  Bdsk-Url-1={http://dx.doi.org/10.1007/s00779-011-0497-7}
}

@article{isenberg2013data,
  Abstract={Interactive tabletops and surfaces (ITSs) provide rich opportunities
    for data visualization and analysis and consequently are used increasingly
    in such settings. A research agenda of some of the most pressing challenges
    related to visualization on ITSs emerged from discussions with researchers
    and practitioners in human-computer interaction, computer-supported collaborative
    work, and a variety of visualization fields at the 2011 Workshop on Data Exploration
    for Interactive Surfaces (Dexis 2011) },
  Author={Isenberg, Petra and Isenberg, Tobias and Hesselmann, Tobias and Lee,
    Bongshin and Von Zadow, Ulrich and Tang, Anthony},
  Date-Modified={2014-01-11 05:50:35 +0000},
  Doi={http://doi.ieeecomputersociety.org/10.1109/MCG.2013.24},
  Journal={Computer Graphics and Applications, IEEE},
  Keywords={Data visualization,Interactive systems,Computer graphics,User interfaces,multitouch,Data
    visualization,Interactive systems,Computer graphics,User interfaces,computer
    graphics,visualization,interactive surfaces,display technology},
  Number={2},
  Pages={16--24},
  Pdfurl={http://hcitang.org/papers/2013-cga-data-visualization-on-interactive-surfaces.pdf},
  Publisher={IEEE},
  Title={Data visualization on interactive surfaces: A research agenda},
  Type={journal},
  Volume={33},
  Year={2013},
  Bdsk-Url-1={http://doi.ieeecomputersociety.org/10.1109/MCG.2013.24}
}

@article{tang2005understanding,
  Abstract={Mixed Presence Groupware (MPG) supports both co-located and distributed
    participants working over a shared visual workspace. It does so by connecting
    multiple single-display groupware workspaces together through a shared data
    structure. Our implementation and observations of MPG systems exposes two problems:
    the first is display disparity, where connecting heterogeneous displays introduces
    issues in how people are seated around the workspace and how workspace artifacts
    are oriented; the second problem is presence disparity, where the perceived
    presence of collaborators is markedly different depending on whether they are
    co-located or remote. Presence disparity is likely caused by inadequate consequential
    communication between remote participants, which in turn disrupts group collaborative
    and communication dynamics. To mitigate display and presence disparity problems,
    we determine virtual seating positions and replace conventional telepointers
    with digital arm shadows that extend from a person's side of the table to their
    pointer location.},
  Author={Tang, Anthony and Boyle, Michael and Greenberg, Saul},
  Date-Modified={2014-01-17 05:14:49 +0000},
  Journal={Journal of research and practice in information technology},
  Notes={Invited article},
  Number={2},
  Pages={193--210},
  Pdfurl={http://hcitang.org/papers/2005-jrpit-tang-display-and-presence-disparity.pdf},
  Publisher={Sydney, Australia: Australian Computer Society, c2000-},
  Title={Understanding and mitigating display and presence disparity in mixed presence
    groupware},
  Type={journal},
  Volume={37},
  Year={2005}
}

@article{jeffrey2006chasing,
  Abstract={We report on our experiences with building and deploying a collaborative
    location-based mobile game. The Fugitive is a multiplayer game that is played
    using mobile TabletPCs in a natural campus environment. The objective is to
    track and capture a hidden object called the Fugitive on a digital campus map
    using annotations for communication among one's teammates. We discuss the design,
    development, and network infrastructure as well as focus group and observational
    findings from our field study. Our findings suggest that the effect of location-awareness
    on collaboration and game play strategies is an intriguing area for study,
    and we share our insights from this project with the Canadian Game Studies
    community. },
  Author={Jeffrey, Phillip and Blackstock, Mike and Finke, Matthias and Tang, Anthony
    and Lea, Rodger and Deutscher, Meghan and Miyaoku, Kento},
  Date-Modified={2014-01-17 05:12:24 +0000},
  Journal={Loading.. Journal},
  Number={1},
  Pdfurl={http://hcitang.org/papers/2006-cgsa2006-chasing-the-fugitive-full.pdf},
  Title={Chasing the Fugitive on Campus: Designing a Location-based Game for Collaborative
    Play},
  Type={journal},
  Volume={1},
  Year={2006}
}

@article{miyaoku2007c,
  Author={Miyaoku, Kento and Tang, Anthony and Fels, Sidney},
  Journal={Information Processing Society of Japan Journal},
  Number={3},
  Pages={1361--1371},
  Title={C-Band: A Ring Tag System Using A Color Pattern Code},
  Type={journal},
  Volume={48},
  Year={2007}
}

@inproceedings{seyed2013skyhunter,
  Abstract={The process of oil and gas exploration and its result, the decision
    to drill for oil in a specific location, relies on a number of distinct but
    related domains. These domains require effective collaboration to come to a
    decision that is both cost effective and maintains the integrity of the environment.
    As we show in this paper, many of the existing technologies and practices that
    support the oil and gas exploration process overlook fundamental user issues
    such as collaboration, interaction and visualization. The work presented in
    this paper is based upon a design process that involved expert users from an
    oil and gas exploration firm in Calgary, Alberta, Canada. We briefly present
    knowledge of the domain and how it informed the design of SkyHunter, a prototype
    multi-surface environment to support oil and gas exploration. This paper highlights
    our current prototype and we conclude with a reflection on multi-surface interactions
    and environments in this domain.},
  Acceptance={29{\%} - 35/121},
  Address={New York, NY, USA},
  Author={Seyed, Teddy and Costa Sousa, Mario and Maurer, Frank and Tang, Anthony},
  Booktitle={ITS '13: Proceedings of the 2013 ACM international conference on Interactive
    tabletops and surfaces},
  Date-Modified={2014-01-11 04:35:50 +0000},
  Doi={http://doi.acm.org/10.1145/2512349.2512798},
  Isbn={978-1-4503-2271-3},
  Keywords={Oil and gas; multi-surface environments; tabletops; gestures; cross-device
    interaction; mobile devices},
  Location={St. Andrews, Scotland, United Kingdom},
  Pages={15--22},
  Pdfurl={http://hcitang.org/papers/2013-its2013-skyhunter.pdf},
  Publisher={ACM},
  Title={SkyHunter: a multi-surface environment for supporting oil and gas exploration},
  Type={conference},
  Year={2013},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2512349.2512798}
}

@inproceedings{weigel2013projectorkit,
  Abstract={Researchers have developed interaction concepts based on mobile projectors.
    Yet pursuing work in this area - particularly in building projector-based interactions
    techniques within an application - is cumbersome and time-consuming. To mitigate
    this problem, we contribute ProjectorKit, a flexible open-source toolkit that
    eases rapid prototyping mobile projector interaction techniques.},
  Acceptance={22{\%} - 53/238},
  Address={New York, NY, USA},
  Author={Weigel, Martin and Boring, Sebastian and Steimle, J\"{u}rgen and Marquardt,
    Nicolai and Greenberg, Saul and Tang, Anthony},
  Booktitle={MobileHCI '13: Proceedings of the 15th international conference on
    Human-computer interaction with mobile devices and services},
  Date-Modified={2014-01-11 05:37:56 +0000},
  Doi={http://doi.acm.org/10.1145/2493190.2493242},
  Isbn={978-1-4503-2273-7},
  Keywords={mobile projectors, toolkit, rapid prototyping},
  Location={Munich, Germany},
  Pages={247--250},
  Pdfurl={http://hcitang.org/papers/2013-mobilehci2013-projectorkit.pdf},
  Publisher={ACM},
  Title={ProjectorKit: easing rapid prototyping of interactive applications for
    mobile projectors},
  Type={conference},
  Url={http://grouplab.cpsc.ucalgary.ca/cookbook/index.php/Toolkits/ProjectorKit},
  Year={2013},
  Bdsk-Url-1={http://grouplab.cpsc.ucalgary.ca/cookbook/index.php/Toolkits/ProjectorKit},
  Bdsk-Url-2={http://doi.acm.org/10.1145/2493190.2493242}
}

@inproceedings{macleod2013personal,
  Abstract={Many people with chronic illness suffer from debilitating symptoms
    or episodes that inhibit normal day-to-day function. Pervasive tools offer
    the possibility to help manage these conditions, particularly by helping people
    understand their conditions. But, it is unclear how to design these tools,
    as prior designs have focused on effortful tracking and many see those tools
    as a burden to use. We report here on an interview study with 12 individuals
    with chronic illnesses who collect personal data. We learn that these people
    are motivated through self- discovery and curiosity. We explore how these concepts
    may support the design of tools that engage curiosity and encourage self-discovery,
    rather than emphasize the behaviour change aspect of chronic illness management.},
  Acceptance={38{\%} - 16/42},
  Address={Toronto, Ont., Canada, Canada},
  Author={MacLeod, Haley and Tang, Anthony and Carpendale, Sheelagh},
  Booktitle={GI '13: Proceedings of the 2013 Graphics Interface Conference},
  Date-Modified={2014-01-11 05:46:58 +0000},
  Isbn={978-1-4822-1680-6},
  Keywords={Personal informatics, Healthcare, Chronic disease management, Qualitative
    studies.},
  Location={Regina, Sascatchewan, Canada},
  Pages={149--156},
  Pdfurl={http://hcitang.org/papers/2013-gi2013-personal-informatics.pdf},
  Publisher={Canadian Information Processing Society},
  Title={Personal informatics in chronic illness management},
  Type={conference},
  Year={2013}
}

@inproceedings{yarosh2013almost,
  Abstract={We deployed the ShareTable - a system that provides easy-to-initiate
    videochat and a shared tabletop task space - in four divorced households. Throughout
    the month of its use, the families employed the ShareTable to participate in
    shared activities, share emotional moments, and communicate closeness through
    metaphorical touch. The ShareTable provided a number of advantages over the
    phone and was easier to use than standard videoconferencing. However, it did
    also introduce concerns over privacy and new sources of conflict about appropriate
    calling practices. We relate our findings to the larger research landscape
    and present implications for future work.},
  Acceptance={35.5{\%} - 139/390},
  Address={New York, NY, USA},
  Author={Yarosh, Svetlana and Tang, Anthony and Mokashi, Sanika and Abowd, Gregory
    D.},
  Booktitle={CSCW '13: Proceedings of the 2013 conference on Computer supported
    cooperative work},
  Date-Modified={2014-01-11 06:55:49 +0000},
  Doi={http://doi.acm.org/10.1145/2441776.2441798},
  Isbn={978-1-4503-1331-5},
  Keywords={Computer-mediated communication, divorced families, children, tabletop,
    camera-projector system, home},
  Location={San Antonio, Texas, USA},
  Pages={181--192},
  Pdfurl={http://hcitang.org/papers/2013-cscw2013-almost-touching.pdf},
  Publisher={ACM},
  Rating={4},
  Title={"Almost touching": parent-child remote communication using the sharetable
    system},
  Type={conference},
  Videourl={http://hcitang.org/papers/2013-cscw2013-almost-touching.m4v},
  Year={2013},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2441776.2441798}
}

@inproceedings{genest2013kinectarms,
  Abstract={Gestures are a ubiquitous part of human communication over tables,
    but when tables are distributed, gestures become difficult to capture and represent.
    There are several problems: extracting arm images from video, representing
    the height of the gesture, and making the arm embodiment visible and understandable
    at the remote table. Current solutions to these problems are often expensive,
    complex to use, and difficult to set up. We have developed a new toolkit --
    KinectArms -- that quickly and easily captures and displays arm embodiments.
    KinectArms uses a depth camera to segment the video and determine gesture height,
    and provides several visual effects for representing arms, showing gesture
    height, and enhancing visibility. KinectArms lets designers add rich arm embodiments
    to their systems without undue cost or development effort, greatly improving
    the expressiveness and usability of distributed tabletop groupware},
  Acceptance={35.5{\%} - 139/390},
  Address={New York, NY, USA},
  Author={Genest, Aaron M. and Gutwin, Carl and Tang, Anthony and Kalyn, Michael
    and Ivkovic, Zenja},
  Booktitle={CSCW '13: Proceedings of the 2013 conference on Computer supported
    cooperative work},
  Date-Modified={2014-01-11 06:56:05 +0000},
  Doi={http://doi.acm.org/10.1145/2441776.2441796},
  Isbn={978-1-4503-1331-5},
  Keywords={Distributed tabletops, gestures, embodiments, toolkits},
  Location={San Antonio, Texas, USA},
  Pages={157--166},
  Pdfurl={http://hcitang.org/papers/2013-cscw2013-kinectarms.pdf},
  Publisher={ACM},
  Title={KinectArms: a toolkit for capturing and displaying arm embodiments in
    distributed tabletop groupware},
  Type={conference},
  Videourl={http://hcitang.org/papers/2013-cscw2013-kinectarms.m4v},
  Year={2013},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2441776.2441796}
}

@inproceedings{seyed2012eliciting,
  Abstract={Multi-display environments (MDEs) have advanced rapidly in recent years,
    incorporating multi-touch tabletops, tablets, wall displays and even position
    tracking systems. Designers have proposed a variety of interesting gestures
    for use in an MDE, some of which involve a user moving their hands, arms, body
    or even a device itself. These gestures are often used as part of interactions
    to move data between the various components of an MDE, which is a longstanding
    research problem. But designers, not users, have created most of these gestures
    and concerns over implementation issues such as recognition may have influenced
    their design. We performed a user study to elicit these gestures directly from
    users, but found a low level of convergence among the gestures produced. This
    lack of agreement is important and we discuss its possible causes and the implication
    it has for designers. To assist designers, we present the most prevalent gestures
    and some of the underlying conceptual themes behind them. We also provide analysis
    of how certain factors such as distance and device type impact the choice of
    gestures and discuss how to apply them to real-world systems. },
  Acceptance={29{\%} - 30/103},
  Address={New York, NY, USA},
  Author={Seyed, Teddy and Burns, Chris and Costa Sousa, Mario and Maurer, Frank
    and Tang, Anthony},
  Booktitle={ITS '12: Proceedings of the 2012 ACM international conference on Interactive
    tabletops and surfaces},
  Date-Modified={2014-01-11 20:42:55 +0000},
  Doi={http://doi.acm.org/10.1145/2396636.2396643},
  Isbn={978-1-4503-1209-7},
  Keywords={Tabletop; gestures; multi-display environments; multi- surface environments;
    multi-display interaction; cross-device interaction; touch; mobile devices},
  Location={Cambridge, Massachusetts, USA},
  Pages={41--50},
  Pdfurl={http://hcitang.org/papers/2012-its2012-eliciting-usable-gestures.pdf},
  Publisher={ACM},
  Title={Eliciting usable gestures for multi-display environments},
  Type={conference},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2396636.2396643}
}

@inproceedings{grevet2012eating,
  Abstract={Eating with others, or commensality, is an enjoyable activity that
    serves many important social functions; however, many individuals eat meals
    alone due to life circumstances, meaning that they miss out on these social
    benefits. We developed and deployed a simple technology probe providing social
    awareness around mealtimes to explore how social systems might help alleviate
    the loneliness of solitary dining. Our findings suggest that these systems
    can convey a sense of connectedness around a meal; further, our analysis revealed
    three themes relevant to systems of this type: that contextually-located peripheral
    awareness engenders connectedness; that such tools can foster a feeling of
    shared social presence, and that they can be a catalyst for other forms of
    communication around the meal. These findings suggest that ``remote commensality''
    is not only possible, but that it may take on forms entirely different to that
    which we are accustomed. },
  Acceptance={25.5{\%} - 24/94},
  Address={New York, NY, USA},
  Author={Grevet, Catherine and Tang, Anthony and Mynatt, Elizabeth},
  Booktitle={GROUP '12: Proceedings of the 17th ACM international conference on
    Supporting group work},
  Date-Modified={2014-01-11 20:45:46 +0000},
  Doi={http://doi.acm.org/10.1145/2389176.2389192},
  Isbn={978-1-4503-1486-2},
  Keywords={HCI, Social computing, Awareness, Contextual information, Design, Food,
    Mealtime},
  Location={Sanibel Island, Florida, USA},
  Pages={103--106},
  Pdfurl={http://hcitang.org/papers/2012-group2012-eating-alone-together.pdf},
  Publisher={ACM},
  Title={Eating alone, together: new forms of commensality},
  Type={conference},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2389176.2389192}
}

@inproceedings{chen2012extending,
  Abstract={Modern mobile devices rely on the screen as a primary input modality.
    Yet the small screen real-estate limits interaction possibilities, motivating
    researchers to explore alternate input techniques. Within this arena, our goal
    is to develop Body-Centric Interaction with Mobile Devices: a class of input
    techniques that allow a person to position and orient her mobile device to
    navigate and manipulate digital content anchored in the space on and around
    the body. To achieve this goal, we explore such interaction in a bottom-up
    path of prototypes and implementations. From our experiences, as well as by
    examining related work, we discuss and present three recurring themes that
    characterize how these interactions can be realized. We illustrate how these
    themes can inform the design of Body-Centric Interactions by applying them
    to the design of a novel mobile browser application. Overall, we contribute
    a class of mobile input techniques where interactions are extended beyond the
    small screen, and are instead driven by a person's movement of the device on
    and around the body.},
  Acceptance={25{\%} - 54/212},
  Address={New York, NY, USA},
  Author={Chen, Xiang 'Anthony' and Marquardt, Nicolai and Tang, Anthony and Boring,
    Sebastian and Greenberg, Saul},
  Booktitle={MobileHCI '12: Proceedings of the 14th international conference on
    Human-computer interaction with mobile devices and services},
  Date-Modified={2014-01-11 20:47:56 +0000},
  Doi={http://doi.acm.org/10.1145/2371574.2371599},
  Isbn={978-1-4503-1105-2},
  Keywords={Body-Centric Interaction, mobile device, mobile interaction},
  Location={San Francisco, California, USA},
  Pages={151--160},
  Pdfurl={http://hcitang.org/papers/2012-mobilehci2012-body-centric-interaction.pdf},
  Publisher={ACM},
  Title={Extending a mobile device's interaction space through body-centric interaction},
  Type={conference},
  Videourl={http://vimeo.com/31172179},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2371574.2371599}
}

@inproceedings{chen2012spalendar,
  Abstract={Portable paper calendars (i.e., day planners and organizers) have greatly
    influenced the design of group electronic calendars. Both use time units (hours/days/weeks/etc.)
    to organize visuals, with useful information (e.g., event types, locations,
    attendees) usually presented as - perhaps abbreviated or even hidden - text
    fields within those time units. The problem is that, for a group, this visual
    sorting of individual events into time buckets conveys only limited information
    about the social network of people. For example, people's whereabouts cannot
    be read `at a glance' but require examining the text. Our goal is to explore
    an alternate visualization that can reflect and illustrate group members' calendar
    events. Our main idea is to display the group's calendar events as spatiotemporal
    activities occurring over a geographic space animated over time, all presented
    on a highly interactive public display. In particular, our SPALENDAR (SPAtial
    CALENDAR) design animates people's past, present and forthcoming movements
    between event locations as well as their static locations. Detail of people's
    events, their movements and their locations is progressively revealed and controlled
    by the viewer's proximity to the display, their identity, and their gestural
    interactions with it, all of which are tracked by the public display. },
  Address={New York, NY, USA},
  Author={Chen, Xiang 'Anthony' and Boring, Sebastian and Carpendale, Sheelagh
    and Tang, Anthony and Greenberg, Saul},
  Booktitle={AVI '12: Proceedings of the International Working Conference on Advanced
    Visual Interfaces},
  Date-Modified={2014-01-12 17:10:29 +0000},
  Doi={http://doi.acm.org/10.1145/2254556.2254686},
  Isbn={978-1-4503-1287-5},
  Keywords={Calendar, group, visualization, location, situated interaction},
  Location={Capri Island, Italy},
  Pages={689--696},
  Publisher={ACM},
  Title={Spalendar: visualizing a group's calendar events over a geographic space
    on a public display},
  Type={conference},
  Videourl={http://vimeo.com/31823922},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2254556.2254686}
}

@inproceedings{tang2012epicplay,
  Abstract={During a live sports event, many sports fans use social media as a
    part of their viewing experience, reporting on their thoughts on the event
    as it unfolds. In this work, we use this information stream to semantically
    annotate live broadcast sports games, using these annotations to select video
    highlights from the game. We demonstrate that this approach can be used to
    select highlights specific for fans of each team, and that these clips reflect
    the emotions of a fan during a game. Further, we describe how these clips differ
    from those seen on nightly sportscasts.},
  Acceptance={23.4{\%} - 369/1577},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Boring, Sebastian},
  Booktitle={CHI '12: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-11 06:50:16 +0000},
  Doi={http://doi.acm.org/10.1145/2207676.2208622},
  Isbn={978-1-4503-1015-4},
  Keywords={Crowd-sourcing; Video summarization; Sports; Twitter; Microblogging;
    Broadcast sports; Video annotation.},
  Location={Austin, Texas, USA},
  Pages={1569--1572},
  Pdfurl={http://hcitang.org/papers/2012-chi2012-epicplay.pdf},
  Publisher={ACM},
  Title={\#EpicPlay: crowd-sourcing sports video highlights},
  Type={conference},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2207676.2208622}
}

@inproceedings{tang2012verbal,
  Abstract={We explore how expert First Person Shooter (FPS) players coordinate
    actions using a shared voice channel. Our findings emphasize the importance
    of the temporality and spatiality of these tactical verbal communications ("call-outs").
    From here, we outline potential designs to mitigate problems in the production/interpretation
    of call-outs to better support coordination.},
  Acceptance={29.6{\%} - 37/125 for notes},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Massey, Jonathan and Wong, Nelson and Reilly, Derek
    and Edwards, W. Keith},
  Booktitle={CSCW '12: Proceedings of the ACM 2012 conference on Computer Supported
    Cooperative Work},
  Date-Modified={2014-01-11 07:01:08 +0000},
  Doi={http://doi.acm.org/10.1145/2145204.2145292},
  Isbn={978-1-4503-1086-4},
  Keywords={First person shooter, FPS, CVE, coordination, First person shooter,
    FPS, CVE, coordination},
  Location={Seattle, Washington, USA},
  Pages={579--582},
  Pdfurl={http://hcitang.org/papers/2012-cscw2012-fps.pdf},
  Publisher={ACM},
  Title={Verbal coordination in first person shooter games},
  Type={conference},
  Year={2012},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2145204.2145292}
}

@inproceedings{reilly2011toward,
  Abstract={We reflect on our experiences using an experimental platform for rapidly
    prototyping physical control configurations for multiplayer games. We describe
    how the architecture permits novel forms of collaborative play through the
    combination and configuration of basic tangible/physical building blocks, the
    deep integration between physical and virtual objects, and flexibility in how
    physical and virtual spaces are mapped onto each other. We also identify three
    important limitations of the architecture that became apparent through our
    prototyping efforts.},
  Address={Berlin, Heidelberg},
  Author={Reilly, Derek and Tang, Anthony and Wu, Andy and Mathiasen, Niels and
    Echenique, Andy and Massey, Jonathan and Rouzati, Hafez and Chamoli, Shashank},
  Booktitle={ICEC'11: Proceedings of the 10th international conference on Entertainment
    Computing},
  Date-Modified={2014-01-17 04:39:20 +0000},
  Doi={http://dx.doi.org/10.1007/978-3-642-24500-8_58},
  Isbn={978-3-642-24499-5},
  Location={Vancouver, Canada},
  Pages={428--431},
  Pdfurl={http://hcitang.org/papers/2011-icec2011-twinspace-experiences.pdf},
  Publisher={Springer-Verlag},
  Title={Toward a framework for prototyping physical interfaces in multiplayer
    gaming: twinspace experiences},
  Type={conference},
  Year={2011},
  Bdsk-Url-1={http://dx.doi.org/10.1007/978-3-642-24500-8_58}
}

@inproceedings{sundaresan2011helping,
  Abstract={When purchasing home broadband access from Internet service providers
    (ISPs), users must decide which service plans are most appropriate for their
    needs. Today, ISPs advertise their available service plans using only generic
    upload and download speeds. Unfortunately, these metrics do not always accurately
    reflect the varying performance that home users will experience for a wide
    range of applications. In this paper, we propose that each ISP service plan
    carry a "nutrition label" that conveys more comprehensive information about
    network metrics along many dimensions, including various aspects of throughput,
    latency, loss rate, and jitter. We first justify why these metrics should form
    the basis of a network nutrition label. Then, we demonstrate that current plans
    that are superficially similar with respect to advertised download rates may
    have different performance according to the label metrics. We close with a
    discussion of the challenges involved in presenting a nutrition label to users
    in a way that is both accurate and easy to understand.},
  Address={New York, NY, USA},
  Author={Sundaresan, Srikanth and Feamster, Nick and Teixeira, Renata and Tang,
    Anthony and Edwards, W. Keith and Grinter, Rebecca E. and Chetty, Marshini
    and de Donato, Walter},
  Booktitle={HomeNets '11: Proceedings of the 2nd ACM SIGCOMM workshop on Home
    networks},
  Date-Modified={2014-01-17 04:40:29 +0000},
  Doi={http://doi.acm.org/10.1145/2018567.2018571},
  Isbn={978-1-4503-0798-7},
  Keywords={access networks, broadband networks, BISMark, benchmarking},
  Location={Toronto, Ontario, Canada},
  Pages={13--18},
  Pdfurl={http://hcitang.org/papers/2011-homenets2011-internetnutritionlabels.pdf},
  Publisher={ACM},
  Title={Helping users shop for ISPs with internet nutrition labels},
  Type={conference},
  Year={2011},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2018567.2018571}
}

@inproceedings{wu2011tangible,
  Abstract={In this paper, we introduce approaches to navigating and manipulating
    objects in a Collaborative Virtual Environment (CVE) that engage tangible objects
    and an interactive table interface. We also identify three design concerns
    that are common to physical-virtual connectivity for interaction with CVE systems.
    At last, we propose solutions to these issues within the context of CVEs.},
  Acceptance={32{\%} - 65/203},
  Address={New York, NY, USA},
  Author={Wu, Andy and Reilly, Derek and Tang, Anthony and Mazalek, Ali},
  Booktitle={TEI '11: Proceedings of the fifth international conference on Tangible,
    embedded, and embodied interaction},
  Date-Modified={2014-01-17 04:34:09 +0000},
  Doi={http://doi.acm.org/10.1145/1935701.1935710},
  Isbn={978-1-4503-0478-8},
  Keywords={Tangible User Interface, TUI, HCI, Tabletop, 3D interaction, virtual
    world},
  Location={Funchal, Portugal},
  Pages={37--44},
  Pdfurl={http://hcitang.org/papers/2011-tei2011-tangible-navigation.pdf},
  Publisher={ACM},
  Title={Tangible navigation and object manipulation in virtual environments},
  Type={conference},
  Year={2011},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1935701.1935710}
}

@inproceedings{tang2010vistaco,
  Abstract={As we design tabletop technologies, it is important to also understand
    how they are being used. Many prior researchers have developed visualizations
    of interaction data from their studies to illustrate ideas and concepts. In
    this work, we develop an interactional model of tabletop collaboration, which
    informs the design of VisTACO, an interactive visualization tool for tabletop
    collaboration. Using VisTACO, we can explore the interactions of collaborators
    with the tabletop to identify patterns or unusual spatial behaviours, supporting
    the analysis process. VisTACO helps bridge the gap between observing the use
    of a tabletop system, and understanding users' interactions with the system.},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Pahud, Michel and Carpendale, Sheelagh and Buxton,
    Bill},
  Booktitle={ITS '10: ACM International Conference on Interactive Tabletops and
    Surfaces},
  Date-Modified={2014-01-17 04:42:20 +0000},
  Doi={http://doi.acm.org/10.1145/1936652.1936659},
  Isbn={978-1-4503-0399-6},
  Keywords={tabletop, collaboration, information visualization},
  Location={Saarbr\"{u}cken, Germany},
  Pages={29--38},
  Pdfurl={http://hcitang.org/papers/2010-its2010-vistaco.pdf},
  Publisher={ACM},
  Title={VisTACO: visualizing tabletop collaboration},
  Type={conference},
  Year={2010},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1936652.1936659}
}

@inproceedings{neustaedter2010role,
  Abstract={Applications that provide location-based experiences are an increasingly
    viable design space given the proliferation of GPS-enabled mobile devices.
    However, these applications are in their infancy, and we do not yet know what
    design factors will contribute to their success. For this reason, we have studied
    the well-established location-based experience of geocaching. We report on
    the results of a survey of geocachers along with observations from our own
    in-depth geocaching activities. Our findings illustrate that geocaching permits
    users to create a range of experiences for others within a permeable yet restricted
    culture of norms. Once created, geocaches are maintained by the community of
    geocachers through a well-designed groupware system. Here maintenance acts
    can be performed ``in the small,'' given their lightweight and well-defined
    nature, and become less about maintenance and more about personal participation.
    These findings provide insight into how community and groupware can be leveraged
    to support applications for location-based experiences.},
  Address={New York, NY, USA},
  Author={Neustaedter, Carman and Tang, Anthony and Tejinder, Judge K.},
  Booktitle={CHI '10: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-17 04:47:25 +0000},
  Doi={http://doi.acm.org/10.1145/1753326.1753590},
  Isbn={978-1-60558-929-9},
  Keywords={Geocaching, location-based experiences, Global Positioning System (GPS)},
  Location={Atlanta, Georgia, USA},
  Pages={1757--1766},
  Pdfurl={http://hcitang.org/papers/2010-chi2010-geocaching.pdf},
  Publisher={ACM},
  Title={The role of community and groupware in geocache creation and maintenance},
  Type={conference},
  Year={2010},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1753326.1753590}
}

@inproceedings{tang2010threes,
  Abstract={We explore the design of a system for three-way collaboration over
    a shared visual workspace, specifically in how to support three channels of
    communication: person, reference, and task-space. In two studies, we explore
    the implications of extending designs intended for dyadic collaboration to
    three-person groups, and the role of each communication channel. Our studies
    illustrate the utility of multiple configurations of users around a distributed
    workspace, and explore the subtleties of traditional notions of identity, awareness,
    spatial metaphor, and corporeal embodiments as they relate to three-way collaboration.
    },
  Address={New York, NY, USA},
  Author={Tang, Anthony and Pahud, Michel and Inkpen, Kori and Benko, Hrvoje and
    Tang, John C. and Buxton, Bill},
  Booktitle={CSCW '10: Proceedings of the 2010 ACM conference on Computer supported
    cooperative work},
  Doi={http://doi.acm.org/10.1145/1718918.1718969},
  Isbn={978-1-60558-795-0},
  Keywords={Shared workspace, tabletop, media space, video-mediated communication},
  Location={Savannah, Georgia, USA},
  Notes={Best Paper Nominee (top 5{\%} of submissions)},
  Pages={271--280},
  Pdfurl={http://hcitang.org/papers/2010-cscw2010-three's-company.pdf},
  Publisher={ACM},
  Title={Three's company: understanding communication channels in three-way distributed
    collaboration},
  Type={conference},
  Year={2010},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1718918.1718969}
}

@inproceedings{tang2009supporting,
  Abstract={In this paper, we explore the practice of using a whiteboard for multiple
    tasks, and specifically how users employ whiteboards to smoothly transition
    between related sets of tasks. Our study underscores several basic, but important
    affordances of whiteboards that support this practice, including visual persistence,
    flexibility of interaction primitives, and their situated physicality. We discuss
    the implications of these findings for the design of large display applications.},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Lanir, Joel and Greenberg, Saul and Fels, Sidney},
  Booktitle={GROUP '09: Proceedings of the ACM 2009 international conference on
    Supporting group work},
  Date-Modified={2014-01-17 04:52:20 +0000},
  Doi={http://doi.acm.org/10.1145/1531674.1531697},
  Isbn={978-1-60558-500-0},
  Keywords={Whiteboard, large display groupware, reflexive CSCW},
  Location={Sanibel Island, Florida, USA},
  Pages={149--158},
  Pdfurl={http://hcitang.org/papers/2009-group2009-transitions.pdf},
  Publisher={ACM},
  Title={Supporting transitions in work: informing large display application design
    by understanding whiteboard use},
  Type={conference},
  Year={2009},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1531674.1531697}
}

@inproceedings{lanir2008multipresenter,
  Abstract={We introduce MultiPresenter, a novel presentation system designed to
    work on very large display spaces (multiple displays or physically large high-resolution
    displays). MultiPresenter allows presenters to organize and present pre-made
    and dynamic presentations that take advantage of a very large display space
    accessed from a personal laptop. Presenters can use the extra space to provide
    long-term persistency of information to the audience. Our design deliberately
    separates content generation (authoring) from the presentation of content.
    We focus on supporting presentation flow and a variety of presentation styles,
    ranging from automated, scripted sequences of pre-made slides to highly dynamic
    ad-hoc, and non-linear content. By providing smooth transition between these
    styles, presenters can easily alter the flow of content during a presentation
    to adapt to an audience or to change emphasis in response to emerging interests.
    We describe our goals, rationale and the design process, providing a detailed
    description of the current version of the system, and discuss our experience
    using it throughout a one-semester first year computer science course},
  Address={New York, NY, USA},
  Author={Lanir, Joel and Booth, Kellogg S. and Tang, Anthony},
  Booktitle={MM '08: Proceedings of the 16th ACM international conference on Multimedia},
  Date-Modified={2014-01-17 05:00:32 +0000},
  Doi={http://doi.acm.org/10.1145/1459359.1459428},
  Isbn={978-1-60558-303-7},
  Keywords={High-resolution displays, Human-Centered Design, Multiple displays,
    Presentations},
  Location={Vancouver, British Columbia, Canada},
  Pages={519--528},
  Pdfurl={http://hcitang.org/papers/2008-mm2008-multipresenter-preprint.pdf},
  Publisher={ACM},
  Title={MultiPresenter: a presentation system for (very) large display surfaces},
  Type={conference},
  Year={2008},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1459359.1459428}
}

@inproceedings{finke2008lessons,
  Abstract={This paper presents the design and deployment of Polar Defence, an
    interactive game for a large public display. We designed this display based
    on a model of ``users'' and their interactions with large public displays in
    public spaces, which we derived from prior work. We conducted a four-day user
    study of this system in a public space to evaluate the game and its impact
    on the surrounding environment. Our analysis showed that the installation successfully
    encouraged participation among strangers, and that its design and deployment
    addressed many of the challenges described by prior research literature. Finally,
    we reflect on this deployment to provide design guidance to other researchers
    building large interactive public displays for public spaces. },
  Address={New York, NY, USA},
  Author={Finke, Matthias and Tang, Anthony and Leung, Rock and Blackstock, Michael},
  Booktitle={DIMEA '08: Proceedings of the 3rd international conference on Digital
    Interactive Media in Entertainment and Arts},
  Date-Modified={2014-01-17 05:02:26 +0000},
  Doi={http://doi.acm.org/10.1145/1413634.1413644},
  Isbn={978-1-60558-248-1},
  Keywords={Interactive large public displays, personal devices, cell phones, short
    messages services (SMS), shared entertainment, gaming, user study},
  Location={Athens, Greece},
  Pages={26--33},
  Pdfurl={http://hcitang.org/papers/2008-dimea2008-lessons-learned.pdf},
  Publisher={ACM},
  Title={Lessons learned: game design for large public displays},
  Type={conference},
  Year={2008},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1413634.1413644}
}

@inproceedings{tang2008exploring,
  Abstract={Video slicing---a variant of slit scanning in photography---extracts
    a scan line from a video frame and successively adds that line to a composite
    image over time. The composite image becomes a time line, where its visual
    patterns reflect changes in a particular area of the video stream. We extend
    this idea of video slicing by allowing users to draw marks anywhere on the
    source video to capture areas of interest. These marks, which we call slit-tears,
    are used in place of a scan line, and the resulting composite timeline image
    provides a much richer visualization of the video data. Depending on how tears
    are placed, they can accentuate motion, small changes, directional movement,
    and relational patterns.},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Greenberg, Saul and Fels, Sidney},
  Booktitle={AVI '08: Proceedings of the working conference on Advanced visual
    interfaces},
  Date-Modified={2014-01-17 04:55:58 +0000},
  Doi={http://doi.acm.org/10.1145/1385569.1385601},
  Isbn={978-1-60558-141-5},
  Keywords={Information visualization, video analysis, video history, timelines},
  Location={Napoli, Italy},
  Pages={191--198},
  Pdfurl={http://hcitang.org/papers/2008-avi2008-slit-tear.pdf},
  Publisher={ACM},
  Title={Exploring video streams using slit-tear visualizations},
  Type={conference},
  Videourl={http://www.youtube.com/watch?v=-kvMth6IpNw},
  Year={2008},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1385569.1385601}
}

@inproceedings{isenberg2008exploratory,
  Abstract={To design information visualization tools for collaborative use, we
    need to understand how teams engage with visualizations during their information
    analysis process. We report on an exploratory study of individuals, pairs,
    and triples engaged in information analysis tasks using paper-based visualizations.
    From our study results, we derive a framework that captures the analysis activities
    of co-located teams and individuals. Comparing this framework with existing
    models of the information analysis process suggests that information visualization
    tools may benefit from providing a flexible temporal flow of analysis actions.},
  Address={New York, NY, USA},
  Author={Isenberg, Petra and Tang, Anthony and Carpendale, Sheelagh},
  Booktitle={CHI '08: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-17 05:01:42 +0000},
  Doi={http://doi.acm.org/10.1145/1357054.1357245},
  Isbn={978-1-60558-011-1},
  Keywords={Information Visualization, analysis process, collaboration},
  Location={Florence, Italy},
  Pages={1217--1226},
  Pdfurl={http://hcitang.org/papers/2008-chi2008-exploratory-study-of-visual-information-analysis.pdf},
  Publisher={ACM},
  Title={An exploratory study of visual information analysis},
  Type={conference},
  Year={2008},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1357054.1357245}
}

@inproceedings{tang2008designing,
  Abstract={In this paper, we reflect on the design and deployment process of MAGICBoard,
    a public display deployed in a university setting that solicits the electronic
    votes and opinions of bystanders on trivial but amusing topics. We focus on
    the consequences of our design choices with respect to encouraging bystanders
    to interact with the public display. Bystanders are individuals around the
    large display who may never fully engage with the application itself, but are
    potential contributors to the system. Drawing on our recent experiences with
    MAGICBoard, we present a classification of bystanders, and then discuss three
    design themes relevant to the design of systems for bystander use: graduated
    proximal engagement, lowering barriers for interaction and supporting covert
    engagement. },
  Address={New York, NY, USA},
  Author={Tang, Anthony and Finke, Mattias and Blackstock, Michael and Leung, Rock
    and Deutscher, Meghan and Lea, Rodger},
  Booktitle={CHI '08: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-17 04:57:29 +0000},
  Doi={http://doi.acm.org/10.1145/1357054.1357193},
  Isbn={978-1-60558-011-1},
  Keywords={bystanders, large display groupware, large displays, mobile computing,
    sms},
  Location={Florence, Italy},
  Pages={879--882},
  Pdfurl={http://hcitang.org/papers/2008-chi2008-designing-for-bystanders.pdf},
  Publisher={ACM},
  Title={Designing for bystanders: reflections on building a public digital forum},
  Type={conference},
  Year={2008},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1357054.1357193}
}

@inproceedings{wong2009character,
  Abstract={Many online games are played through characters that act out players'
    intentions in the game world. The practice of character sharing -- allowing
    others to use one's characters, or using others' -- is prohibited in many RPGs,
    but anecdotal evidence suggests that the practice is common, and that it may
    play an important role in the game. To shed light on this little-known form
    of collaboration, we carried out a large-scale survey study to investigate
    character sharing in one RPG, World of Warcraft. We analyze and report on 1348
    responses, providing a detailed picture of sharing practices and attitudes.
    We found that character sharing is common (57{\%} of respondents reported sharing)
    and that sharers have a wide variety of motivations and concerns. In addition
    to showing how character sharing works, the study also provides new perspectives
    on several themes in CSCW, including conceptions of sharing, online identity,
    and mediating artifacts.},
  Author={Wong, Nelson and Tang, Anthony and Livingston, Ian and Gutwin, Carl and
    Mandryk, Regan},
  Booktitle={ECSCW 2009: Proceedings of the European Conference on Computer Supported
    Cooperative Work (ECSCW)},
  Date-Modified={2014-01-17 04:51:21 +0000},
  Pages={343--362},
  Pdfurl={http://hcitang.org/papers/2009-ecscw2009-character-sharing.pdf},
  Publisher={Springer London},
  Title={Character sharing in World of Warcraft},
  Type={conference},
  Year={2009}
}

@inproceedings{shoemaker2007shadow,
  Abstract={We introduce Shadow Reaching, an interaction technique that makes use
    of a perspective projection applied to a shadow representation of a user. The
    technique was designed to facilitate manipulation over large distances and
    enhance understanding in collaborative settings. We describe three prototype
    implementations that illustrate the technique, examining the advantages of
    using shadows as an interaction metaphor to support single users and groups
    of collaborating users. Using these prototypes as a design probe, we discuss
    how the three components of the technique (sensing, modeling, and rendering)
    can be accomplished with real (physical) or computed (virtual) shadows, and
    the benefits and drawbacks of each approach.},
  Address={New York, NY, USA},
  Author={Shoemaker, Garth and Tang, Anthony and Booth, Kellogg S.},
  Booktitle={UIST '07: Proceedings of the 20th annual ACM symposium on User interface
    software and technology},
  Date-Modified={2014-01-17 05:04:59 +0000},
  Doi={http://doi.acm.org/10.1145/1294211.1294221},
  Isbn={978-1-59593-679-0},
  Keywords={Large displays, interaction techniques},
  Location={Newport, Rhode Island, USA},
  Pages={53--56},
  Pdfurl={http://hcitang.org/papers/2007-uist2007-shadow-reaching.pdf},
  Publisher={ACM},
  Title={Shadow reaching: a new perspective on interaction for large displays},
  Type={conference},
  Videourl={http://www.youtube.com/watch?v=Su4ZIqxaObo},
  Year={2007},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1294211.1294221}
}

@inproceedings{miyaoku2007cband,
  Abstract={This paper proposed a new visual tag system for enhancing real world
    media interaction using handheld camera devices. This paper also described
    performance evaluations of the prototype, and its applications. C-Band is based
    on a ring with a color pattern code. A C-Band tag can provide a self-contained
    URL, and is flexible enough to allow various aesthetic designs for the tag's
    surface. Furthermore, the tag's structure is useful for building interactive
    techniques. Taken together, these features suggest that C-Band is an effective
    method to build various attractive camera-based media interactions.},
  Address={Berlin, Heidelberg},
  Author={Miyaoku, Kento and Tang, Anthony and Fels, Sidney},
  Booktitle={ICVR'07: Proceedings of the 2nd international conference on Virtual
    reality},
  Date-Modified={2014-01-17 05:07:10 +0000},
  Doi={http://dx.doi.org/10.1007/978-3-540-73335-5_35},
  Isbn={978-3-540-73334-8},
  Keywords={Visual Tag, Color-Difference Signal, Camera, Mobile Terminal},
  Location={Beijing, China},
  Pages={320--328},
  Pdfurl={http://hcitang.org/papers/2007-hcii-c-band.pdf},
  Publisher={Springer-Verlag},
  Title={C-band: a flexible ring tag system for camera-based user interface},
  Type={conference},
  Year={2007},
  Bdsk-Url-1={http://dx.doi.org/10.1007/978-3-540-73335-5_35}
}

article{2006c,
  Author={{\aa}{\textregistered}{\textregistered}{\aa}{\textyen}{\textyen}{\aa}?{\textyen}{\"a}??
    and Tang, A and Fels, S},
  Journal={{\~a}?{\textcurrency}{\~a}?{\textthreesuperior}{\~a}?{\textquestiondown}{\~a}?{\copyright}{\~a}??{\~a}?{\textperiodcentered}{\~a}?{\S}{\~a}?{\textthreesuperior}},
  Pages={3--10},
  Title={C-Band: {\ae}??{\`e}{\guillemotright}?{\~a}??{\~a}?{\guillemotleft}{\~a}?{\copyright}{\~a}?{\textonequarter}{\c{c}}?{\textdegree}{\c{c}}??{\~a}?{\textquestiondown}{\~a}?{\textdegree}{\~a}?{\textperiodcentered}{\~a}?{\textonesuperior}{\~a}??{\~a}??},
  Type={conference},
  Year={2006}
}

@inproceedings{siu2006going,
  Abstract={Email use in the context of everyday work practices, or email flow,
    has not been heavily studied. We present the results of a pair of studies examining
    how users interlace email with their day-to-day, ongoing work processes. We
    demonstrate that our subjects use email as a tool for managing moment-to-moment
    attention and task focus. We also provide a model of this workflow that builds
    upon an existing model by Venolia et al. Finally, we provide specific design
    recommendations to enhance the usability of email clients in support of these
    modes of interaction. },
  Address={New York, NY, USA},
  Author={Siu, Nelson and Iverson, Lee and Tang, Anthony},
  Booktitle={CSCW '06: Proceedings of the 2006 20th anniversary conference on Computer
    supported cooperative work},
  Date-Modified={2014-01-17 05:11:49 +0000},
  Doi={http://doi.acm.org/10.1145/1180875.1180942},
  Isbn={1-59593-249-6},
  Keywords={Email deferral, email flow, task management},
  Location={Banff, Alberta, Canada},
  Pages={441--450},
  Pdfurl={http://hcitang.org/papers/2006-cscw2006-going-with-the-flow.pdf},
  Publisher={ACM},
  Title={Going with the flow: email awareness and task management},
  Type={conference},
  Year={2006},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1180875.1180942}
}

@inproceedings{tang2006collaborative,
  Abstract={Designing collaborative interfaces for tabletops remains difficult
    because we do not fully understand how groups coordinate their actions when
    working collaboratively over tables. We present two observational studies of
    pairs completing independent and shared tasks that investigate collaborative
    coupling, or the manner in which collaborators are involved and occupied with
    each other's work. Our results indicate that individuals frequently and fluidly
    engage and disengage with group activity through several distinct, recognizable
    states with unique characteristics. We describe these states and explore the
    consequences of these states for tabletop interface design.},
  Address={New York, NY, USA},
  Author={Tang, Anthony and Tory, Melanie and Po, Barry and Neumann, Petra and
    Carpendale, Sheelagh},
  Booktitle={CHI '06: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-17 05:11:23 +0000},
  Doi={http://doi.acm.org/10.1145/1124772.1124950},
  Isbn={1-59593-372-7},
  Keywords={Collaborative tabletop displays, single display groupware, mixed focus
    collaboration, coordination, coupling},
  Location={Montr{\'e}al, Qu{\'e}bec, Canada},
  Pages={1181--1190},
  Pdfurl={http://hcitang.org/papers/2006-chi2006-collaborative-coupling.pdf},
  Publisher={ACM},
  Title={Collaborative coupling over tabletop displays},
  Type={conference},
  Year={2006},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1124772.1124950}
}

@inproceedings{tang2007videoarms,
  Abstract={Mixed presence groupware (MPG) allows collocated and distributed teams
    to work together on a shared visual workspace. Presence disparity arises in
    MPG because it is harder to maintain awareness of remote collaborators compared
    to collocated collaborators. We examine the role of one's body in collaborative
    work and how it affects presence disparity, articulating four design implications
    for embodiments in mixed presence groupware to mitigate the effects of presence
    disparity: embodiments should provide local feedback; they should visually
    portray people's interaction with the work surface using direct input mechanisms;
    they should display fine-grain movement and postures of hand gestures, and
    they should be positioned within the workspace. We realize and evaluate these
    implications with VideoArms, an embodiment technique that captures and reproduces
    people's arms as they work over large displays. },
  Author={Tang, Anthony and Neustaedter, Carman and Greenberg, Saul},
  Booktitle={People and Computers XX---Engage},
  Date-Modified={2014-01-17 05:03:42 +0000},
  Keywords={consequential communication, embodiments, distributed groupware, gestures,
    mixed presence groupware, single display groupware},
  Pages={85--102},
  Pdfurl={http://hcitang.org/papers/2006-hci2006-videoarms-embodiments-in-mpg.pdf},
  Publisher={Springer London},
  Title={Videoarms: embodiments for mixed presence groupware},
  Type={conference},
  Year={2007}
}

@inproceedings{fels2006investigation,
  Abstract={We explore the impact of tilting the driver's seat according to the
    relative distance and velocity to objects outside the car using a haptic feedback
    chair in a driving simulator. We found that drivers perform best when (1) the
    seat tilts according to relative distance (vs. velocity) to objects outside
    the car and (2) the seat tilts forward (vs. backward) when the driver gets
    closer to a car in front of them. We also found that when visually and cognitively
    distracted, drivers perform better using haptic feedback than without. Our
    results suggest that adding haptic feedback to the car seat may improve driving
    safety and enjoyment by enhancing the driving experience},
  Author={Fels, Sidney and Hausch, Robert and Tang, Anthony},
  Booktitle={Intelligent Transportation Systems Conference, 2006. ITSC'06. IEEE},
  Date-Modified={2014-01-17 05:14:19 +0000},
  Doi={http://dx.doi.org/10.1109/ITSC.2006.1706804},
  Keywords={driver seat; driving safety; driving simulator; haptic feedback chair},
  Organization={IEEE},
  Pages={584--589},
  Title={Investigation of haptic feedback in the driver seat},
  Type={conference},
  Year={2006},
  PdfUrl={http://hcitang.org/papers/2006-itsc2006-haptic-seat.pdf},
  Bdsk-Url-1={http://dx.doi.org/10.1109/ITSC.2006.1706804}
}

@inproceedings{tang2005perceiving,
  Abstract={Visual information overload is a threat to the interpretation of displays
    presenting large data sets or complex application environments. To combat this
    problem, researchers have begun to explore how haptic feedback can be used
    as another means for information transmission. In this paper, we show that
    people can perceive and accurately process haptically rendered ordinal data
    while under cognitive workload. We evaluate three haptic models for rendering
    ordinal data with participants who were performing a taxing visual tracking
    task. The evaluation demonstrates that information rendered by these models
    is perceptually available even when users are visually busy. This preliminary
    research has promising implications for haptic augmentation of visual displays
    for information visualization. },
  Address={New York, NY, USA},
  Author={Tang, Anthony and McLachlan, Peter and Lowe, Karen and Saka, Chalapati
    Rao and MacLean, Karon},
  Booktitle={ICMI '05: Proceedings of the 7th international conference on Multimodal
    interfaces},
  Date-Modified={2014-01-17 05:26:20 +0000},
  Doi={http://doi.acm.org/10.1145/1088463.1088517},
  Isbn={1-59593-028-0},
  Keywords={Haptics, 1-DOF, tangible user interface, graspable user interface,
    haptic perception, multimodal displays, information visualization},
  Location={Torento, Italy},
  Pages={317--324},
  Pdfurl={http://hcitang.org/papers/2005-icmi-haptic-perception.pdf},
  Publisher={ACM},
  Title={Perceiving ordinal data haptically under workload},
  Type={conference},
  Year={2005},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1088463.1088517},
  Notes={Best paper award}
}

@inproceedings{kruger2005fluid,
  Abstract={Previous research has shown that rotation and orientation of items
    plays three major roles during collaboration: comprehension, coordination and
    communication. Based on these roles of orientation and advice from kinesiology
    research, we have designed the Rotate'N Translate (RNT) interaction mechanism,
    which provides integrated control of rotation and translation using only a
    single touch-point for input. We present an empirical evaluation comparing
    RNT to a common rotation mechanism that separates control of rotation and translation.
    Results of this study indicate RNT is more efficient than the separate mechanism
    and better supports the comprehension, coordination and communication roles
    of orientation.},
  Address={New York, NY, USA},
  Author={Kruger, Russell and Carpendale, Sheelagh and Scott, Stacey D. and Tang,
    Anthony},
  Booktitle={CHI '05: Proceedings of the SIGCHI Conference on Human Factors in
    Computing Systems},
  Date-Modified={2014-01-17 05:28:25 +0000},
  Doi={http://doi.acm.org/10.1145/1054972.1055055},
  Isbn={1-58113-998-5},
  Keywords={Fluid interactions, rotation, translation, orientation, roles of orientation,
    tabletop collaboration, communicative gestures},
  Location={Portland, Oregon, USA},
  Pages={601--610},
  Pdfurl={http://hcitang.org/papers/2005-chi2005-rnt.pdf},
  Publisher={ACM},
  Title={Fluid integration of rotation and translation},
  Type={conference},
  Year={2005},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1054972.1055055}
}

@inproceedings{tang2004display,
  Abstract={Mixed Presence Groupware (MPG) supports both co-located and distributed
    participants working over a shared visual workspace. It does this by connecting
    multiple single-display groupware workspaces together through a shared data
    structure. Our implementation and observations of MPG systems exposes two problems.
    The first is display disparity, where connecting heterogeneous tabletop and
    vertical displays introduces issues in how one seats people around the virtual
    table and how one orients work artifacts. The second is presence disparity,
    where a participant's perception of the presence of others is markedly different
    depending on whether a collaborator is co-located or remote. This is likely
    caused by inadequate consequential communication between remote participants,
    which in turn disrupts group collaborative and communication dynamics. To mitigate
    display and presence disparity problems, we determine virtual seating positions
    and replace conventional telepointers with digital arm shadows that extend
    from a person's side of the table to their pointer location.},
  Address={Darlinghurst, Australia, Australia},
  Author={Tang, Anthony and Boyle, Michael and Greenberg, Saul},
  Booktitle={AUIC '04: Proceedings of the fifth conference on Australasian user
    interface},
  Date-Modified={2014-01-17 05:30:58 +0000},
  Location={Dunedin, New Zealand},
  Notes={Superceded by JRPIT article of same title},
  Pages={73--82},
  Publisher={Australian Computer Society, Inc.},
  Title={Display and presence disparity in Mixed Presence Groupware},
  Type={conference},
  Url={http://dl.acm.org/citation.cfm?id=976310.976320},
  PdfUrl={http://hcitang.org/papers/2004-auic2004-display-and-presence-disparity.pdf},
  Year={2004},
  Bdsk-Url-1={http://dl.acm.org/citation.cfm?id=976310.976320}
}

@inproceedings{tang2006surface,
  Abstract={Although we can now augment meeting rooms with large-format digital
    displays (e.g. digital whiteboards or tabletops), successful deployment of
    groupware tools for such environments has been limited. I believe this problem
    stems from a poor understanding of how teams make use of traditional meeting
    room surfaces (e.g. whiteboards, walls, tables) in collaboration; as a consequence,
    our large display groupware applications do not always reflect the general
    expectations users have of large displays, which replace traditional, non-digital
    meeting room surfaces. My research develops a framework for understanding how
    meeting room surfaces are used collaboratively, thereby providing insight into
    application design for digital display surfaces in meeting rooms. },
  Author={Tang, Anthony},
  Booktitle={Conference Companion of the 2006 20th Anniversary Conference on Computer
    Supported Cooperative Work},
  Date-Modified={2014-01-17 05:09:10 +0000},
  Keywords={Large display groupware, meeting room collaboration},
  Pages={43-44},
  Pdfurl={http://hcitang.org/papers/2006-cscw2006-surface-use-in-meeting-room-collaboration.pdf},
  Title={Surface use in meeting room collaboration},
  Type={poster},
  Url={http://hcitang.org/papers/2006-cscw2006-surface-use-in-meeting-room-collaboration-poster.pdf},
  Year={2006},
  Bdsk-Url-1={http://hcitang.org/papers/2006-cscw2006-surface-use-in-meeting-room-collaboration-poster.pdf}
}

@inproceedings{ledo2013onespace,
  Abstract={Video conferencing commonly employs a video portal metaphor to connect
    individuals from remote spaces. In this work, we explore an alternate metaphor,
    a shared depth-mirror, where video images of two spaces are fused into a single
    shared, depth-corrected video space. We realize this metaphor in OneSpace,
    where the space respects virtual spatial relationships between people and objects
    as if all parties were looking at a mirror together. We report preliminary
    observations of OneSpace's use, noting that it encourages cross-site, full-body
    interactions, and that participants employed the depth cues in their interactions.
    Based on these observations, we argue that the depth mirror offers new opportunities
    for shared video interaction.},
  Address={New York, NY, USA},
  Author={Ledo, David and Aseniero, Bon Adriel and Greenberg, Saul and Boring,
    Sebastian and Tang, Anthony},
  Booktitle={CHI EA '13: CHI '13 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Modified={2014-01-11 05:45:23 +0000},
  Doi={http://doi.acm.org/10.1145/2468356.2468534},
  Isbn={978-1-4503-1952-2},
  Keywords={Video communication; media spaces},
  Location={Paris, France},
  Pages={997--1002},
  Pdfurl={http://hcitang.org/papers/2013-chi2013-wip-onespace.pdf},
  Publisher={ACM},
  Title={OneSpace: shared depth-corrected video interaction},
  Type={poster},
  Year={2013},
  Bdsk-Url-1={http://doi.acm.org/10.1145/2468356.2468534}
}

@inproceedings{tang2006studying,
  Abstract={In this poster, we focus on the use of large vertical surfaces (e.g.walls,
    flipcharts, whiteboards), articulating four unique roles they play in collaboration:
    presentation, ideation, reference, and notice . By understanding these roles,
    we can design interactiontechniques that exploit people's expectations and
    uses of thesesurfaces. As an example, we realize one design idea in Pick-and-
    Point ---a fluid interaction technique that moves content from personal surfaces
    onto large surfaces that recognizes thecollaborative role of large vertical
    surfaces.},
  Author={Tang, Anthony and Parker, J Karen and Lanir, Joel and Booth, KS and Fels,
    Sidney},
  Booktitle={Conference Companion of the 2006 20th Anniversary Conference on Computer
    Supported Cooperative Work},
  Date-Modified={2014-01-17 05:10:53 +0000},
  Keywords={display ecology, large screen display, tabletpc},
  Pages={219-220},
  Pdfurl={http://hcitang.org/papers/2006-cscw2006-studying-surface-use-to-guide-large-display-interaction-design.pdf},
  Title={Studying collaborative surface use to guide large display interaction
    design},
  Type={poster},
  Url={http://hcitang.org/papers/2006-cscw2006-surface-use-in-meeting-room-collaboration-poster.pdf},
  Year={2006},
  Bdsk-Url-1={http://hcitang.org/papers/2006-cscw2006-surface-use-in-meeting-room-collaboration-poster.pdf}
}

@inproceedings{tang2009exploring,
  Abstract={Slit-tear visualizations allow users to selectively visualize pixel
    paths in a video scene. The slit-tear visualization technique is a generalization
    of the traditional photographic slit-scanning and more recent video slicing
    techniques: after a user specifies a pixel path of interest, the system generates
    a timeline that replicates those pixels for each frame in the video. These
    rich visualizations of the video data help users to discover and explore spatio-temporal
    patterns of activity in a video. In this video, we illustrate the use of slit-tear
    visualizations to detect movement and incidence of activity in a video scene,
    accentuate directional motion and small changes in the video, and discover
    patterns of activity between spatially distinct areas of the scene. },
  Address={New York, NY, USA},
  Author={Tang, Anthony and Greenberg, Saul and Fels, Sidney},
  Booktitle={CHI EA '09: CHI '09 Extended Abstracts on Human Factors in Computing
    Systems},
  Date-Modified={2014-01-17 04:53:20 +0000},
  Doi={http://doi.acm.org/10.1145/1520340.1520516},
  Isbn={978-1-60558-247-4},
  Keywords={video analysis, video visualization, video interaction, information
    visualization},
  Location={Boston, MA, USA},
  Notes={Best Research Video Nominee},
  Pages={3509--3510},
  Pdfurl={http://hcitang.org/papers/2009-chi2009-videoshowcase-slittears.pdf},
  Publisher={ACM},
  Title={Exploring video streams using slit-tear visualizations},
  Type={video},
  Videourl={http://hcitang.org/papers/2009-chi2009-videoshowcase-slittears.wmv},
  Year={2009},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1520340.1520516}
}

@inproceedings{tang2004videoarms,
  Abstract={Shared visual workspaces afford collaboration by providing a medium
    that grounds workspace activity, conversation, and gestures. If distributed
    groupware systems designed as shared visual workspaces are to replace or augment
    the physical workspaces of today, they need to naturally support these affordances.
    VideoArms is an embodiment technique for distributed groupware that captures
    body images of collaborators as they work, and transmits them to remote workspaces.
    These body images are then placed in the context of the workspace, thereby
    supporting the transmission of conversational gestures, collaborator identity,
    workspace activity, and complex workspace gestures. The technique uses a purely
    digital approach, allowing for the possibility of different presentation techniques
    (e.g. colour video, shadows, transparent video, outlines, etc.). },
  Author={Tang, Anthony and Neustaedter, Carman and Greenberg, Saul},
  Booktitle={Video Proceedings of CSCW 2004},
  Date-Modified={2014-01-17 05:29:26 +0000},
  Pdfurl={http://hcitang.org/papers/2004-cscw2004-videoarms-video.pdf},
  Title={VideoArms: supporting remote embodiment in groupware},
  Type={video},
  Videourl={http://hcitang.org/papers/2004-cscw2004-videoarms-video.wmv},
  Year={2004}
}

@inproceedings{tang2005dartmail,
  Author={Tang, Anthony and Pattison, Eric and Greenberg, Saul},
  Booktitle={ECSCW 2005: Video Proceedings of European Conference on Computer Supported
    Cooperative Work},
  Date-Modified={2014-01-17 05:27:37 +0000},
  Journal={Video Proceedings of European Conference on Computer Supported Cooperative
    Work},
  Notes={Video and 2-page summary},
  Pdfurl={http://hcitang.org/papers/2005-ecscw2005-dartmail-video.pdf},
  Title={DartMail: Digital information transfer through physical surrogates},
  Type={video},
  Videourl={http://hcitang.org/papers/2005-ecscw2005-dartmail-video.wmv},
  Year={2005}
}

@inproceedings{judge2010bridging,
  Abstract={A typical product development lifecycle for interactive systems starts
    with contextual analysis to guide system design. The challenge however is in
    transitioning from findings about users, their activities, and needs, into
    design requirements, constraints and implications that are directly applicable
    to design. In this workshop, we seek to bring together researchers, designers,
    and practitioners who regularly face the challenge of transitioning from contextual
    analysis to design implications and design practices. Our goal is to foster
    a community in this space, understand the techniques that are being employed
    to move from contextual analysis to design, the challenges that still exist,
    and solutions to overcome them. },
  Author={Judge, Tejinder K and Neustaedter, Carman and Tang, Anthony and Harrison,
    Steve},
  Booktitle={CHI'10 Extended Abstracts on Human Factors in Computing Systems},
  Date-Modified={2014-01-17 04:50:52 +0000},
  Doi={http://doi.acm.org/10.1145/1753846.1754183},
  Keywords={Contextual analysis, design, requirements analysis, gap},
  Organization={ACM},
  Pages={4497--4500},
  Pdfurl={http://hcitang.org/papers/2010-chi2010workshop-bridging-the-gap.pdf},
  Title={Bridging the gap: Moving from contextual analysis to design},
  Type={other},
  Year={2010},
  Bdsk-Url-1={http://doi.acm.org/10.1145/1753846.1754183}
}

@phdthesis{tang2010phd,
  Abstract={Interactive large displays offer exciting new opportunities for collaboration
    and work. Yet, their size will fundamentally change how users expect to use
    and engage with computer applications: a likely reality is that such displays
    will be used by multiple users for multiple simultaneous tasks. These expectations
    demand a new approach for application design beyond the conventional desktop
    application model, where applications are single-user, and intended to support
    a subset of user tasks. In this research, we develop such a framework based
    on the premise that large display applications should support transitions---users'
    desires to shift between multiple tasks and activities. We build this framework
    from models of how traditional large surfaces such as whiteboards are used
    to facilitate multiple tasks---often simultaneously. Based on studies of users'
    whiteboard use, we construct a classification scheme of users' activities with
    whiteboards, and the role of whiteboards in supporting the transitions between
    these activities. From a study of meeting room activity, we then develop a
    classification for collocated activity around traditional surfaces. We further
    develop models of how users' needs change during their use of large display
    applications, exploring two contexts: a digital tabletop application for focused
    collaboration, and a public large display. These studies reveal how users engage
    and disengage with one another during collaborative work, and the dynamic needs
    of bystanders. Next, we design and evaluate a prototype that supports transitions
    between tasks in a scheduling activity using viewing changes. The results demonstrate
    that users transition between related tasks during such activities, and that
    viewing changes can support these transitions. Finally, we describe a design
    space for supporting transitions in large display applications. Taken together,
    the findings of this research illustrate the fundamental need to develop a
    new framework for designing large display applications. This work provides
    a step in this direction by providing rationale and empirical evidence for
    supporting transitions in this framework. In so doing, it suggests that we
    realign designers' efforts from the predominant desktop-centric model of application
    development, and instead to a model that engenders smooth transitions between
    multiple, related activities.},
  Address={2332 Main Mall Vancouver, BC Canada V6T 1Z4},
  Author={Tang, Anthony},
  Date-Modified={2014-01-17 05:25:15 +0000},
  Month={January},
  Pdfurl={http://hcitang.org/papers/2010-phd-tang.pdf},
  School={University of British Columbia},
  Title={Understanding and supporting transitions with large display applications},
  Type={thesis},
  Url={http://circle.ubc.ca/handle/2429/19394},
  Year={2010},
  Bdsk-Url-1={http://circle.ubc.ca/handle/2429/19394}
}

@mastersthesis{tang2005mscthesis,
  Abstract={In this thesis, I define and explore Mixed Presence Groupware (MPG):
    software that connects distributed groups of collaborators together, allowing
    collocated individuals to work together on a shared display while simultaneously
    working with other, remote groups in the same digital workspace. In my explorations
    of this new class of groupware, I articulate a problem unique to MPG workspaces
    called presence disparity, where collaborators focus their collaborative energies
    toward collocated collaborators while ignoring their remote counterparts. I
    propose that the root cause of this problem is the poor representational properties
    of embodiments for remote collaborators, and develop a theory about embodiments
    for MPG workspaces. I present a video-based embodiment technique called VideoArms
    that addresses the presence disparity problem by following the design guidelines
    set out by the theory. Finally, I evaluate this embodiment technique, demonstrating
    and critiquing its effectiveness in mitigating presence disparity.},
  Address={2500 University Dr NW, Calgary AB, Canada T2N 1N4},
  Author={Tang, Anthony},
  Date-Modified={2014-01-17 05:24:45 +0000},
  Month={January},
  School={University of Calgary},
  Title={Embodiments in Mixed Presence Groupware},
  PdfUrl={http://hcitang.org/papers/2005-mscthesis-tang.pdf},
  Type={thesis},
  Year={2005}
}

@inproceedings{rahman2014headaches,
  author={Rahman, S. M. Waliur and Bhaskar, Rahul Kamal and Maurer, Frank and Tang,
    Anthony},
  year={2014},
  title={Supporting Chronic Headache Patientswith Visual Analytics},
  abstract={Usually chronic headache patients are advised by their care providers
    to track their headache episodes in apaper or electronic diary. These diaries
    are primarilyused for information extraction by clinicians to prepareproper
    treatment plan. Patients are generallydependent on clinician?s advices to improve
    theirconditions. However, we hypothesize that with help ofvisual trends and
    analyses of chronic conditions as aform of personal informatics, patients will
    beempowered to manage their own conditions. We propose innovative analytics
    based interactivevisualizations revealing hidden chronic patterns afteranalyzing
    limitations of previous work in this area anddemonstrate how these visualizations
    can help patientsby providing meaningful insights. We intend to carryout qualitative
    research involving real chronic patients to verify our hypothesis},
  booktitle={PVA 2014: A Personal Perspective on Visualization and Visual Analytics
    - Workshop at DIS 2014},
  editor={Sheelagh Carpendale and Melanie Tory and Anthony Tang},
  pdfurl={http://hcitang.org/papers/2014-dis2014workshop-chronic-headaches.pdf},
  type={workshop}
}

@inproceedings{payne2014physviz,
  author={Payne, Jennifer and Carpendale, Sheelagh and Tang, Anthony},
  year={2014},
  title={Physical Visualization and Personal Visual Analytics},
  abstract={Personal data is increasing in both volume and variety. Personal visual
    analytics (PVA) can help us to make use of such data. How might physical visualizations
    serve in this context? Such representations of data can be aesthetically pleasing
    and may feel less task-like; in some situations, physical visualizations also
    improve information retrieval over screen-based representations. },
  booktitle={PVA 2014: A Personal Perspective on Visualization and Visual Analytics
    - Workshop at DIS 2014},
  editor={Sheelagh Carpendale and Melanie Tory and Anthony Tang},
  type={workshop},
}

@inproceedings{aseniero2014river,
  author={Aseniero, Bon Adriel and Tang, Anthony and Carpendale, Sheelagh},
  title={River: Using Personalisation to Support Reflection on Personal Activities},
  abstract={Tools that help us track and log activities ? a class of tools broadly
    termed personal informatics, are gaining prevalence in both the marketplace
    (e.g. FitBit, Nike) and in the research space. These tools collect data about
    a person?s physical activity but leave out contextual data which could be valuable
    for reflection. To address this, we look at other common practices involving
    the logging of activities such as day-planners and diaries. We distil previous
    research to articulate four design considerations to support reflection on
    activities, and realised these in a work-in-progress web-based personal informatics
    tool ?River. },
  booktitle={PVA 2014: A Personal Perspective on Visualization and Visual Analytics
    - Workshop at DIS 2014},
  editor={Sheelagh Carpendale and Melanie Tory and Anthony Tang},
  pdfurl={http://hcitang.org/papers/2014-dis2014workshop-river.pdf},
  type={workshop},
  year={2014}
}

@inproceedings{carpendale2014pvaworkshop,
  author={Carpendale, Sheelagh and Tory, Melanie and Tang, Anthony},
  year={2014},
  title={A Personal Perspective on Visualization and Visual Analytics},
  abstract={Data surrounds each and every one of us in our daily lives, ranging
    from logs of exercise and diet, to information about our home energy use, to
    archives of our interactions with others on social media, to online resources
    pertaining to our hobbies and interests. There is enormous potential for us
    use this data to gain insight and knowledge about ourselves and our communities.
    However, designing and applying visualization and visual analytics in our personal
    lives brings a unique set of design challenges. If these tools belong in our
    personal lives, work type criteria such as efficiency may no longer apply.
    In this workshop we will identify and explore research directions and design
    criteria for personal visualization and personal visual analytics. Our goal
    is to call research attention to these areas, to engage the design community
    in this timely and growing field, and to establish a community and common vision
    for researchers and practitioners working in this space.},
  booktitle={DIS 2014 Companion: Proceedings of the 2014 Companion Publication
    on Designing Interactive Systems},
  publisher={ACM},
  pages={223-225},
  doi={http://doi.acm.org/10.1145/2598784.2598806},
  pdfurl={http://hcitang.org/papers/2014-dis2014workshop-pva-workshop.pdf}
}

