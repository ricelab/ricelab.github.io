<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP - html5up.net 
-->
<html>
	<head>
		<title>360 Video Interaction</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="/assets/css/ie/html5shiv.js"></script><![endif]-->
		<script src="/assets/js/jquery.min.js"></script>
		<script src="/assets/js/jquery.dropotron.min.js"></script>
		<script src="/assets/js/skel.min.js"></script>
		<script src="/assets/js/skel-layers.min.js"></script>
		<script src="/assets/js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="/assets/css/skel.css" />
			<link rel="stylesheet" href="/assets/css/style.css" />
			<link rel="stylesheet" href="/assets/css/style-wide.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie/v8.css" /><![endif]-->
		<link rel="shortcut icon" href="/assets/images/favicon.png">
		
	</head>
	<body>

		<!-- Header -->
			<div id="header">
						
				<!-- Logo -->
					<h1><a href="/index.html" id="logo">RICELab <em style="font-size: 9pt;color: #cbcbcb" class="only-large">Rethinking Interaction Collaboration and Engagement</em></a></h1>
				
				<!-- Nav -->
					<nav id="nav">
						<ul>
							
							<li ><a href="/index.html">Home</a></li>
							<li ><a href="/about-us">About Us</a></li>
							<li ><a href="/people">People</a></li>
							<li class="current"><a href="/projects">Projects</a></li>
							<li ><a href="/papers">Publications</a></li>
							<li ><a href="/blog">Blog</a></li>
							<li ><a href="/join-us">Join Us</a></li>
							<li ><a href="/internal">Internal</a></li>
						</ul>
					</nav>
			</div>
<style>
#citation a {
  color: black;
  border-bottom: none;
}
#citation {
  margin-bottom: 1em;
}
</style>

<!-- Main -->
	<section class="wrapper style1">
		<div class="container">
			<div class="row double">
				<div class="4u">
					<div id="sidebar">

	<!-- Sidebar -->

		<section>
			<h3>Current Projects</h3>
			<ul>
			  
			    
			      <li><a href="/projects/hai-collaboration/">Human-AI Collaboration</a></li>
			    
			  
			    
			      <li><a href="/projects/proxemics/">Proxemic Interaction Design</a></li>
			    
			  
			    
			      <li><a href="/projects/haptics/">Haptics in VR</a></li>
			    
			  
			    
			      <li><a href="/projects/games/">Games and Leisure</a></li>
			    
			  
			    
			      <li><a href="/projects/efashion/">Electronic Fashion</a></li>
			    
			  
			    
			      <li><a href="/projects/360video/">360 Video Interaction</a></li>
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			</ul>

			<h3>Past Projects</h3>
			<ul>
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			      <li><a href="/projects/teaching-technologies/">Technologies for Teaching</a></li>
			    
			  
			    
			      <li><a href="/projects/social-watching-of-sports/">Social Watching of Sports</a></li>
			    
			  
			    
			      <li><a href="/projects/slit-tear-video/">Slit-Tear Video Exploration</a></li>
			    
			  
			    
			      <li><a href="/projects/remote-embodiment/">Remote Embodiment for Shared Activity</a></li>
			    
			  
			    
			      <li><a href="/projects/personal-informatics/">Personal Informatics and Analytics</a></li>
			    
			  
			    
			      <li><a href="/projects/physiotherapy/">Physiotherapy for the Future</a></li>
			    
			  
			    
			      <li><a href="/projects/pervasive-games/">Pervasive Games and Citizen Science</a></li>
			    
			  
			    
			      <li><a href="/projects/mobile-video-conferencing/">Mobile Video Conferencing</a></li>
			    
			  
			    
			      <li><a href="/projects/mobile-device-interaction/">Interaction Techniques for Mobile Devices</a></li>
			    
			  
			    
			      <li><a href="/projects/mixed-reality-workrooms/">Mixed Reality Workrooms</a></li>
			    
			  
			    
			      <li><a href="/projects/collaborative-visual-analytics/">Collaborative Visual Analytics</a></li>
			    
			  
			    
			      <li><a href="/projects/collaborative-ar/">Mixed Reality for Collaboration</a></li>
			    
			  
			    
			      <li><a href="/projects/multi-surface-environments/">Digital Workrooms of the Future</a></li>
			    
			  
			</ul>
		</section>
</div>
				</div>
				<div class="8u skel-cell-important">
					<div id="content">

						<!-- Content -->
					
							<article>
								<header>
									<h2>360 Video Interaction</h2>
									<p>New interaction techniques for immersive video</p>
								</header>
								
								<!-- 160x144 -->

<p>360 video cameras provide an omnidirectional view on the world around us, and new opportunities to explore those views. Videos from these devices, in particular, afford us new ways to see and understand the world, as well as certain kinds of freedoms to explore these recorded experiences. Our work in this space has explored new ways to take advantage of the media – both in “single user” kinds of scenarios, where we are exploring spaces on our own, as well as in “multi user” kinds of experiences, where we are exploring the spaces with others.</p>

<p>One of the neat challenges and opportunities with 360 video is that they allow us to decide what <em>we</em> want to look at as viewers. Most videos have an “intended viewing angle” determined by the producer/camera person. In this sense, they restrict your view, and your ability to explore the space. What makes 360 videos even more interesting is that we have an additional dimension of flexibilit (time), which we must also navigate in addition to viewing direction. Our work has explored ways to smoothing this interaction to make it more straightforward for viewers.</p>

<h2 id="exploring-with-others--navigating-time-and-space-together">Exploring with Others – Navigating Time and Space Together</h2>

<p>Most interfaces for exploring 360 video are made for single user exploration. This presents real challenges when we are trying to explore 360 video scenes with other people. Some of our earliest work in this space showed that viewing 360 videos with others is somewhat cumbersome – mainly because our goals as viewers may differ <a class="citation" href="#tang2017watching360together">(Tang &amp; Fakourfar, 2017)</a>. We observe similar kinds of challenges when people try to communicate over live mobile video chat <a class="citation" href="#jones2015mobilecamerawork">(Jones, Witcraft, Tang, Bateman, &amp; Neustaedter, 2015)</a>. We designed two prototypes to explore how live video interaction over distance could be made richer through 360 video cameras: in one where studied how a simple prototype might introduce new complexities to the interaction <a class="citation" href="#tang2017360videochat">(Tang, Fakourfar, Neustaedter, &amp; Bateman, 2017)</a>, and in another where we attached a 360 camera to a telepresence robot to support play <a class="citation" href="#heshmat2018beam">(Heshmat, Jones, Xiong, Neustaedter, Tang, Riecke, &amp; Yang, 2018)</a>. These experiences revealed the complexities of interacting with remote collaborators, and paved the way for new kinds of interaction techniques to smooth those interactions.</p>

<p>Our most recent explorations of this research space have involved the design and evaluation of a prototype called Tourgether360 <a class="citation" href="#kumar2022tourgether">(Kumar, Poretski, Li, &amp; Tang, 2022; Kumar, Poretski, Li, &amp; Tang, 2022)</a>. This prototype allows people to experience virtual 360 tours together, and focuses on the design of communication and awareness tools to smooth the interactive dialogue between partners. We learned valuable lessons in this work – especially in how much communication is implicit and subtle.</p>

<h2 id="single-user-exploration-of-360-videos">Single-User Exploration of 360 Videos</h2>

<p>One challenge of exploring 360 videos is that one can really only manipulate one dimension at a time—either changing the view, or changing the time. Controlling both at the same time is hard. This means exploring 360 videos quickly is difficult. To this end, we created the Route Tapestries approach <a class="citation" href="#li2021routetapestries">(Li, Lyu, Sousa, Balakrishnan, Tang, &amp; Grossman, 2021)</a>, which borrows from the slit-tear approach <a class="citation" href="#tang2008exploring">(Tang, Greenberg, &amp; Fels, 2008)</a>. This technique creates continuous orthographic-perspective projection of scenes along camera routes, which allows users to quickly scan the entire video rapidly without having to watch the video in time.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/kumar2022tourgetherlbw"><span id="kumar2022tourgetherlbw">Kartikaeya Kumar, Lev Poretski, Jiannan Li, and Anthony Tang. (2022). <span style="text-decoration: underline">Tourgether360: Exploring 360° Tour Videos with Others</span>. In <i>EA CHI ’22: Extended Abstracts of the 2022 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(poster).</a><br />

	<small>Acceptance: 36.1% (261/722).</small>



	<br />


	<a href="http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2022-chi2022lbw-kumar-tourgether.mp4"><i class="fa fa-film"></i></a>


</section><a class="details" href="/papers/kumar2022tourgetherlbw/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/kumar2022tourgether"><span id="kumar2022tourgether">Kartikaeya Kumar, Lev Poretski, Jiannan Li, and Anthony Tang. (2022). <span style="text-decoration: underline"> Tourgether360: Collaborative Exploration of 360 Tour Videos using Pseudo-Spatial Navigation</span>. <i>Proceedings of the ACM on Human-Computer Interaction (PACMHCI)</i> 6, CSCW2.</span>

(conference).</a><br />




	<a href="https://hcitang.org/papers/2022-cscw2022-kumar-tourgether360.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://cscw.acm.org/2022/"><i class="fa fa-sticky-note-o"></i></a>


	<a href="https://hcitang.org/papers/2022-cscw2022-kumar-tourgether360.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3555604"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/kumar2022tourgether/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/li2021routetapestries"><span id="li2021routetapestries">Jiannan Li, Jiahe Lyu, Mauricio Sousa, Ravin Balakrishnan, Anthony Tang, and Tovi Grossman. (2021). <span style="text-decoration: underline">Route Tapestries: Navigating 360 Virtual Tour Videos Using Slit-Scan Visualizations</span>. In <i>UIST 2021: Proceedings of the 34th Annual ACM Symposium on User Interface Software and Technology</i>.</span>

(conference).</a><br />

	<small>Acceptance: 95/367 - 25.9%.</small>



	<br />


	<a href="http://hcitang.org/papers/2021-uist2021-li-routetapestries.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=0uAhUxJXgpc"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2021-uist2021-li-routetapestries.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3472749.3474746"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/li2021routetapestries/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/heshmat2018beam"><span id="heshmat2018beam">Yasamin Heshmat, Brennan Jones, Xiaoxuan Xiong, Carman Neustaedter, Anthony Tang, Bernhard E. Riecke, and Lillian Yang. (2018). <span style="text-decoration: underline">Geocaching with a Beam: Shared Outdoor Activities through a Telepresence Robot with 360 Degree Viewing</span>. In <i>CHI 2018: Proceedings of the 2018 SIGCHI Conference on Human Factors in Computing Systems</i>, Paper 359.</span>

(conference).</a><br />

	<small>Acceptance: 25.7% - 667/2595.</small>


	<small>Notes: 10 pages.</small>


	<br />


	<a href="http://hcitang.org/papers/2018-chi2018-geocaching-with-a-beam.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://chi2018.acm.org"><i class="fa fa-sticky-note-o"></i></a>



	<a href="http://doi.acm.org/10.1145/3173574.3173933"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/heshmat2018beam/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2017360videochat"><span id="tang2017360videochat">Anthony Tang, Omid Fakourfar, Carman Neustaedter, and Scott Bateman. (2017). <span style="text-decoration: underline">Collaboration in 360° Videochat: Challenges and Opportunities</span>. In <i>DIS 2017: Conference on Designing Interactive Systems 2017 </i>, 1327–1339.</span>

(conference).</a><br />

	<small>Acceptance: 24% - 110/458.</small>


	<small>Notes: Appendix material: http://dspace.ucalgary.ca/handle/1880/51950.</small>


	<br />


	<a href="http://hcitang.org/papers/2017-dis2017-360videochat.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://dis2017.org/"><i class="fa fa-sticky-note-o"></i></a>


	<a href=""><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/3064663.3064707"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2017360videochat/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2017watching360together"><span id="tang2017watching360together">Anthony Tang and Omid Fakourfar. (2017). <span style="text-decoration: underline">Watching 360° Videos Together</span>. In <i>CHI 2017: Proceedings of the 2017 SIGCHI Conference on Human Factors in Computing Systems </i>, 4501–4506.</span>

(conference).</a><br />

	<small>Acceptance: 25% - 600/2400.</small>


	<small>Notes: Presentation - https://www.youtube.com/watch?v=OPc4mBD7pgw.</small>


	<br />


	<a href="http://hcitang.org/papers/2017-chi2017-watching360video.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="http://chi2017.acm.org"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2017-chi2017-watching360video.mp4"><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/3025453.3025519"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2017watching360together/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/jones2015mobilecamerawork"><span id="jones2015mobilecamerawork">Brennan Jones, Anna Witcraft, Anthony Tang, Scott Bateman, and Carman Neustaedter. (2015). <span style="text-decoration: underline">Mechanics of Camera Work in Mobile Video Collaboration</span>. In <i>CHI 2015: Proceedings of the 2015 SIGCHI Conference on Human Factors in Computing Systems</i>, ACM, 957–966.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2015-chi2015-mobile-video-collaboration.mp4"><i class="fa fa-film"></i></a>


	<a href="http://dx.doi.org/10.1145/2702123.2702345"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/jones2015mobilecamerawork/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/tang2008exploring"><span id="tang2008exploring">Anthony Tang, Saul Greenberg, and Sidney Fels. (2008). <span style="text-decoration: underline">Exploring video streams using slit-tear visualizations</span>. In <i>AVI ’08: Proceedings of the working conference on Advanced visual
    interfaces</i>, ACM, 191–198.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2008-avi2008-slit-tear.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://www.youtube.com/watch?v=-kvMth6IpNw"><i class="fa fa-film"></i></a>


	<a href="http://doi.acm.org/10.1145/1385569.1385601"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/tang2008exploring/"><!-- --></a></li></ol>



							</article>
					</div>
				</div>
			</div>
		</div>
	</section>			

		<!-- Footer -->
 			<div id="footer">
				<!-- Copyright -->
					<div class="copyright">
					    <script type="text/javascript">
					    // from http://grouplab.cpsc.ucalgary.ca/
					    function hideshow(el){
						    if (!document.getElementById) return;
						    if (el.style.visibility=="visible") {
						    	el.style.visibility="hidden"
						    }
						    else {
						    	el.style.visibility="visible"
						    }
						}
					    </script>
						<ul class="menu">
							<!--
							<li><a href="https://www.google.ca/maps/place/Math+Science,+Calgary,+AB+T2N+4V8/@51.0799635,-114.1300982,17z/data=!3m1!4b1!4m2!3m1!1s0x53716f0c07993c17:0xb8f1352e9e5dfa06?hl=en"><i class="icon fa-map-marker"></i> 2500 University Drive NW, Calgary, AB T2N 1N4</a> (<a href="/directions">Directions</a>)</li><li><a href="tel:403-210-6912"><i class="icon fa-phone"></i> 403-210-6912</a></li>
							-->
							<li><a rel="nofollow" href="javascript:hideshow(document.getElementById('colophon'))">Colophon</a></li>
						</ul>
						<section id="colophon" visibility="hidden" style="visibility: hidden;">
							<ul class="menu">
								<li>Design: <a href="http://html5up.net">HTML5 UP</a></li><li>Images: <a href="http://unsplash.com/">unsplash.com</a></li><li>Icons: <a href="https://fortawesome.github.io/Font-Awesome/">font awesome</a></li><li>Bibliography: <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a></li>
							</ul>
						</section>
					</div>

			</div>

	</body>
</html>