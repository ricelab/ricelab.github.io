<!DOCTYPE HTML>
<!--
	Arcana by HTML5 UP - html5up.net 
-->
<html>
	<head>
		<title>Human-AI Collaboration</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<!--[if lte IE 8]><script src="/assets/css/ie/html5shiv.js"></script><![endif]-->
		<script src="/assets/js/jquery.min.js"></script>
		<script src="/assets/js/jquery.dropotron.min.js"></script>
		<script src="/assets/js/skel.min.js"></script>
		<script src="/assets/js/skel-layers.min.js"></script>
		<script src="/assets/js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="/assets/css/skel.css" />
			<link rel="stylesheet" href="/assets/css/style.css" />
			<link rel="stylesheet" href="/assets/css/style-wide.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie/v8.css" /><![endif]-->
		<link rel="shortcut icon" href="/assets/images/favicon.png">
		
	</head>
	<body>

		<!-- Header -->
			<div id="header">
						
				<!-- Logo -->
					<h1><a href="/index.html" id="logo">RICELab <em style="font-size: 9pt;color: #cbcbcb" class="only-large">Rethinking Interaction Collaboration and Engagement</em></a></h1>
				
				<!-- Nav -->
					<nav id="nav">
						<ul>
							
							<li ><a href="/index.html">Home</a></li>
							<li ><a href="/about-us">About Us</a></li>
							<li ><a href="/people">People</a></li>
							<li class="current"><a href="/projects">Projects</a></li>
							<li ><a href="/papers">Publications</a></li>
							<li ><a href="/blog">Blog</a></li>
							<li ><a href="/join-us">Join Us</a></li>
							<li ><a href="/internal">Internal</a></li>
						</ul>
					</nav>
			</div>
<style>
#citation a {
  color: black;
  border-bottom: none;
}
#citation {
  margin-bottom: 1em;
}
</style>

<!-- Main -->
	<section class="wrapper style1">
		<div class="container">
			<div class="row double">
				<div class="4u">
					<div id="sidebar">

	<!-- Sidebar -->

		<section>
			<h3>Current Projects</h3>
			<ul>
			  
			    
			      <li><a href="/projects/hai-collaboration/">Human-AI Collaboration</a></li>
			    
			  
			    
			      <li><a href="/projects/proxemics/">Proxemic Interaction Design</a></li>
			    
			  
			    
			      <li><a href="/projects/haptics/">Haptics in VR</a></li>
			    
			  
			    
			      <li><a href="/projects/games/">Games and Leisure</a></li>
			    
			  
			    
			      <li><a href="/projects/efashion/">Electronic Fashion</a></li>
			    
			  
			    
			      <li><a href="/projects/360video/">360 Video Interaction</a></li>
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			</ul>

			<h3>Past Projects</h3>
			<ul>
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			  
			    
			      <li><a href="/projects/teaching-technologies/">Technologies for Teaching</a></li>
			    
			  
			    
			      <li><a href="/projects/social-watching-of-sports/">Social Watching of Sports</a></li>
			    
			  
			    
			      <li><a href="/projects/slit-tear-video/">Slit-Tear Video Exploration</a></li>
			    
			  
			    
			      <li><a href="/projects/remote-embodiment/">Remote Embodiment for Shared Activity</a></li>
			    
			  
			    
			      <li><a href="/projects/personal-informatics/">Personal Informatics and Analytics</a></li>
			    
			  
			    
			      <li><a href="/projects/physiotherapy/">Physiotherapy for the Future</a></li>
			    
			  
			    
			      <li><a href="/projects/pervasive-games/">Pervasive Games and Citizen Science</a></li>
			    
			  
			    
			      <li><a href="/projects/mobile-video-conferencing/">Mobile Video Conferencing</a></li>
			    
			  
			    
			      <li><a href="/projects/mobile-device-interaction/">Interaction Techniques for Mobile Devices</a></li>
			    
			  
			    
			      <li><a href="/projects/mixed-reality-workrooms/">Mixed Reality Workrooms</a></li>
			    
			  
			    
			      <li><a href="/projects/collaborative-visual-analytics/">Collaborative Visual Analytics</a></li>
			    
			  
			    
			      <li><a href="/projects/collaborative-ar/">Mixed Reality for Collaboration</a></li>
			    
			  
			    
			      <li><a href="/projects/multi-surface-environments/">Digital Workrooms of the Future</a></li>
			    
			  
			</ul>
		</section>
</div>
				</div>
				<div class="8u skel-cell-important">
					<div id="content">

						<!-- Content -->
					
							<article>
								<header>
									<h2>Human-AI Collaboration</h2>
									<p>Enriching communication and collaboration with AI-based tools</p>
								</header>
								
								<!-- 160x144 -->

<p>A major focus of our current work is to explore how to design effective interfaces for human-AI collaboration. This framing of interaction draws focus away from the idea of computation (as in human-<em>computer</em> interaction), and rather on the idea of computation that can assist and amplify human activity through interactive collaboration. Collaboration, as we understand it, involves multiple turns for interacting, as well as a commitment to help and assist one another.</p>

<h2 id="coordination-and-communication-based-on-human-human-interaction">Coordination and Communication based on Human-Human Interaction</h2>

<p>Recently, we have been exploring how our understanding of human-human communication and coordination can be used to smooth human-AI interaction. One design resource for this is in studying and understanding how humans work with other humans, and using ideas here to apply to human-AI communication and coordination.</p>

<p>For instance, we have been interested in studying how we can use proxemics to mediate how we interact with robots <a class="citation" href="#mahadevan2021hri">(Mahadevan, Sousa, Tang, &amp; Grossman, 2021)</a>. We have found that when robots can understand how people are using space to coordinate their interactions with one another, they can more straightforwardly anticipate how human collaborators will work. This smooths collaborative activity between humans and robots.</p>

<p>We applied similar ideas in Stargazer <a class="citation" href="#li2023stargazer">(Li, Sousa, Mahadevan, Wang, Aoyagui, Yu, Yang, Balakrishnan, Tang, &amp; Grossman, 2023)</a>, where we designed a robotic camera that could understand both gestures and speech from a howto video tutorial producer. The robotic camera could, based on what the producer was saying, make decisions about how to reposition itself, or to zoom in/out.</p>

<p>Broadly, we expect that if we can design AI that understands proxemic behaviours and language, this will make it easier for humans to interact with AI that understands and has embodiment in our physical world. This is not the <em>only</em> way to design for human-AI collaboration, but one tact that is likely to be successful.</p>

<h2 id="generative-tools-for-authors">Generative Tools for Authors</h2>

<p>We have recently begun exploring how generative tools can be designed to support authors. An important aspect of this is understand what kinds of challenges authors have. In one project, we are exploring the kinds of challenges that fiction authors have in crafting their works <a class="citation" href="#stark2023fictionworkshop">(Stark, Tang, Kim, Park, &amp; Wigdor, 2023)</a>. After having identified some of these challenges, we have begun the process of designing and studying the impacts of tools that support the creative process.</p>

<h2 id="publications">Publications</h2>

<ol class="bibliography"><li><section id="citation">
    <a href="/papers/stark2023fictionworkshop"><span id="stark2023fictionworkshop">Jessi Stark, Anthony Tang, Young-Ho Kim, Joonsuk Park, and Daniel Wigdor. (2023). <span style="text-decoration: underline"> Can AI Support Fiction Writers Without Writing For Them? </span>. In <i>CHI23 Workshop: In2Writing - The Second Workshop on Intelligent and Interactive Writing Assistants</i>.</span>

	(Gero, Katy Ilonka and Chang, Minsuk and Huang, Ting-Hao ’Kenneth’ and Chung, John Joon Young and Raheja, Vipul and Wambsganss, Thiemo and Kang, Dongyeop and Lee, Mina, Eds.)

(workshop).</a><br />




	<a href=" http://hcitang.org/papers/2023-chi2023workshop-stark-fiction-writers.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://in2writing.glitch.me/"><i class="fa fa-sticky-note-o"></i></a>



</section><a class="details" href="/papers/stark2023fictionworkshop/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/li2023stargazer"><span id="li2023stargazer">Jiannan Li, Mauricio Sousa, Karthik Mahadevan, Bryan Wang, Paula Akemi Aoyagui, Nicole Yu, Angela Yang, Ravin Balakrishnan, Anthony Tang, and Tovi Grossman. (2023). <span style="text-decoration: underline">Stargazer: An Interactive Camera Robot for Capturing How-To Videos Based on Subtle Instructor Cues</span>. In <i>CHI 2023: Proceedings of the 2023 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />




	<a href="http://hcitang.org/papers/2023-chi2023-li-stargazer.pdf"><i class="fa fa-file-pdf-o"></i></a>



	<a href="http://hcitang.org/papers/2023-chi2023-li-stargazer.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3544548.3580896"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/li2023stargazer/"><!-- --></a></li>
<li><section id="citation">
    <a href="/papers/mahadevan2021hri"><span id="mahadevan2021hri">Karthik Mahadevan, Mauricio Sousa, Anthony Tang, and Tovi Grossman. (2021). <span style="text-decoration: underline">"Grip-that-there": An Investigation of Explicit and Implicit Task Allocation Techniques for Human-Robot Collaboration</span>. In <i>CHI 2021: Proceedings of the 2021 SIGCHI Conference on Human Factors in Computing Systems</i>.</span>

(conference).</a><br />

	<small>Acceptance: 26.3% - 749/2844.</small>


	<small>Notes: Best Paper Nominee (top 5% of submissions).</small>


	<br />


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.pdf"><i class="fa fa-file-pdf-o"></i></a>


	<a href="https://www.youtube.com/watch?v=Fvn_qjWJqwk"><i class="fa fa-sticky-note-o"></i></a>


	<a href="http://hcitang.org/papers/2021-chi2021-mahadevan-grip-that-there.mp4"><i class="fa fa-film"></i></a>


	<a href="https://doi.org/10.1145/3411764.3445355"><i class="fa fa-external-link"></i></a>

</section><a class="details" href="/papers/mahadevan2021hri/"><!-- --></a></li></ol>



							</article>
					</div>
				</div>
			</div>
		</div>
	</section>			

		<!-- Footer -->
 			<div id="footer">
				<!-- Copyright -->
					<div class="copyright">
					    <script type="text/javascript">
					    // from http://grouplab.cpsc.ucalgary.ca/
					    function hideshow(el){
						    if (!document.getElementById) return;
						    if (el.style.visibility=="visible") {
						    	el.style.visibility="hidden"
						    }
						    else {
						    	el.style.visibility="visible"
						    }
						}
					    </script>
						<ul class="menu">
							<!--
							<li><a href="https://www.google.ca/maps/place/Math+Science,+Calgary,+AB+T2N+4V8/@51.0799635,-114.1300982,17z/data=!3m1!4b1!4m2!3m1!1s0x53716f0c07993c17:0xb8f1352e9e5dfa06?hl=en"><i class="icon fa-map-marker"></i> 2500 University Drive NW, Calgary, AB T2N 1N4</a> (<a href="/directions">Directions</a>)</li><li><a href="tel:403-210-6912"><i class="icon fa-phone"></i> 403-210-6912</a></li>
							-->
							<li><a rel="nofollow" href="javascript:hideshow(document.getElementById('colophon'))">Colophon</a></li>
						</ul>
						<section id="colophon" visibility="hidden" style="visibility: hidden;">
							<ul class="menu">
								<li>Design: <a href="http://html5up.net">HTML5 UP</a></li><li>Images: <a href="http://unsplash.com/">unsplash.com</a></li><li>Icons: <a href="https://fortawesome.github.io/Font-Awesome/">font awesome</a></li><li>Bibliography: <a href="https://github.com/inukshuk/jekyll-scholar">jekyll-scholar</a></li>
							</ul>
						</section>
					</div>

			</div>

	</body>
</html>